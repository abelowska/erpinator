{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a73099-38c1-4bf0-afd3-5481df7c3d19",
   "metadata": {},
   "source": [
    "# Internal consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff9f56-5483-44c7-b9cc-486a31838279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e84d26-b32c-41ae-b3f4-81e0ee55156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import ast\n",
    "import os.path as op\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium.featurize\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from rumination_experiment_transformers_averaged_CDS import *\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c69899-6a32-4418-9079-8bd66aed497f",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading data\n",
    "\n",
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc483a7-410f-4b65-9955-9df42c783bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths TODO\n",
    "dir_path = os.path.dirname(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a25389-d030-415b-982d-b68df981b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.101562, 0.5937525  # Start and end of the segments\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1\n",
    "ALL = 2\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ef4f5-f711-4f8e-8ba5-163dc50bd2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_order_list = [\n",
    "    \"Fp1\",\n",
    "    \"AF7\",\n",
    "    \"AF3\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    \"F5\",\n",
    "    \"F7\",\n",
    "    \"FT7\",\n",
    "    \"FC5\",\n",
    "    \"FC3\",\n",
    "    \"FC1\",\n",
    "    \"C1\",\n",
    "    \"C3\",\n",
    "    \"C5\",\n",
    "    \"T7\",\n",
    "    \"TP7\",\n",
    "    \"CP5\",\n",
    "    \"CP3\",\n",
    "    \"CP1\",\n",
    "    \"P1\",\n",
    "    \"P3\",\n",
    "    \"P5\",\n",
    "    \"P7\",\n",
    "    \"P9\",\n",
    "    \"PO7\",\n",
    "    \"PO3\",\n",
    "    \"O1\",\n",
    "    \"Iz\",\n",
    "    \"Oz\",\n",
    "    \"POz\",\n",
    "    \"Pz\",\n",
    "    \"CPz\",\n",
    "    \"Fpz\",\n",
    "    \"Fp2\",\n",
    "    \"AF8\",\n",
    "    \"AF4\",\n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"F2\",\n",
    "    \"F4\",\n",
    "    \"F6\",\n",
    "    \"F8\",\n",
    "    \"FT8\",\n",
    "    \"FC6\",\n",
    "    \"FC4\",\n",
    "    \"FC2\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"C2\",\n",
    "    \"C4\",\n",
    "    \"C6\",\n",
    "    \"T8\",\n",
    "    \"TP8\",\n",
    "    \"CP6\",\n",
    "    \"CP4\",\n",
    "    \"CP2\",\n",
    "    \"P2\",\n",
    "    \"P4\",\n",
    "    \"P6\",\n",
    "    \"P8\",\n",
    "    \"P10\",\n",
    "    \"PO8\",\n",
    "    \"PO4\",\n",
    "    \"O2\",\n",
    "]\n",
    "\n",
    "channels_dict = dict(zip(channels_order_list, np.arange(1, 64, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13f57b8-2375-4faa-a00f-91294ffd7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_3 = [\"Fpz\",\n",
    "        \"F1\",\"Fz\", \"F2\",\n",
    "        \"FC1\", \"FCz\", \"FC2\",\n",
    "        \"C1\",\"Cz\",\"C2\",\n",
    "        \"CP1\", \"CPz\", \"CP2\",\n",
    "        \"P1\",\"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "box_4 = [\"Fpz\",\n",
    "         \"AFz\",\n",
    "        \"F1\",\"Fz\", \"F2\",\n",
    "        \"FC1\", \"FCz\", \"FC2\",\n",
    "        \"C1\",\"Cz\",\"C2\",\n",
    "        \"CP1\", \"CPz\", \"CP2\",\n",
    "        \"P1\",\"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "box_5 = [\"F1\",\"Fz\", \"F2\",\n",
    "        \"FC1\", \"FCz\", \"FC2\",\n",
    "        \"C1\",\"Cz\",\"C2\",\n",
    "        \"CP1\", \"CPz\", \"CP2\",\n",
    "        \"P1\",\"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu\n",
    "red_box7 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box7_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu i na środku\n",
    "red_box8 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\", \"Cz\",\"C2\", \"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\"\n",
    "]\n",
    "\n",
    "red_box8_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu i na środku i na poczatku\n",
    "red_box9 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F3\", \"F1\", \"Fz\", \"F2\", \"F4\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\", \"Cz\",\"C2\", \"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box9_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F1\", \"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "box_list = [red_box7_prim, red_box8_prim, red_box9_prim, box_5]\n",
    "# box_list = [red_box7_prim, red_box9_prim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c412f36-66a2-4fd8-bfa7-caa98c4a315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=\"all\",\n",
    "    personal=True,\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "CenteredSignalAfterBaseline3CenteredSignalAfterBaseline3CenteredSignalAfterBaseline3\n",
    "    On default, loads a train set: chooses only 80% of participants\n",
    "    and for each of them chooses 80% of epochs.\n",
    "    It will choose them deterministically.\n",
    "\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    If test_participants is set to True, it will load remaining 20% of participants.\n",
    "    If test_epochs is set to True, it will load remaining 20% of epochs.\n",
    "    Test epochs are chronologically after train epochs,\n",
    "    because it reflects real usage (first callibration and then classification).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "    personal: bool\n",
    "        whether a model will be both trained and tested on epochs from one person\n",
    "        if false, person's epochs aren't split into test and train\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    print(os.path.abspath(\"\"))\n",
    "    dir_path = os.path.dirname(os.path.abspath(\"\"))\n",
    "    print(dir_path)\n",
    "    header_files_glob = os.path.join(dir_path, \"data/responses_100_600/*.vhdr\")\n",
    "    header_files = glob.glob(header_files_glob)\n",
    "\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    # cut 20% of data for testing\n",
    "    h_train, h_test = train_test_split(header_files, test_size=0.3, random_state=0)\n",
    "    \n",
    "    print(f\"train size: {len(h_train)} ; test size: {len(h_test)}\")\n",
    "\n",
    "    if test_participants:\n",
    "        header_files = h_test\n",
    "    else:\n",
    "        header_files = h_train\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 5 or len(correct) < 5:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, participant_epochs, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991419b-7acd-42bc-b048-e5e9fa3cb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, participant_epochs, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        if info == \"all\":\n",
    "            rumination_df = pd.read_csv(info_filename)\n",
    "        else:\n",
    "            rumination_df = pd.read_csv(info_filename, usecols=[\"Demo_kod\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"Demo_kod\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "\n",
    "#     for epoch in correct:\n",
    "#         epoch_df = pd.DataFrame(\n",
    "#             {\"id\": [id], \"epoch\": [epoch], \"marker\": [CORRECT]}\n",
    "#         ).join(info_df)\n",
    "#         participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "#     for epoch in error:\n",
    "#         epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [epoch], \"marker\": [ERROR]}).join(\n",
    "#             info_df\n",
    "#         )\n",
    "#         participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "        \n",
    "#     print(participant_epochs)\n",
    "        \n",
    "    epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [participant_epochs], \"marker\": [ALL]}).join(\n",
    "            info_df\n",
    "        )\n",
    "    participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88517e-884a-4b5b-9652-1bf2e7787aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' means that bad segments are rejected automatically.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = True\n",
    "    \n",
    "    # maximum acceptable peak-to-peak amplitudes\n",
    "    reject_criteria = dict(eeg=150e-6)       # 200 µV\n",
    "    \n",
    "    # minimum acceptable peak-to-peak amplitudes\n",
    "    flat_criteria = dict(eeg=1e-6)           # 1 µV\n",
    "    \n",
    "    picks_eeg = mne.pick_types(raw.info, meg=False, eeg=True, eog=False,\n",
    "                           stim=False, exclude='bads', selection=red_box7_prim)\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        # reject_tmin=-0.101,\n",
    "        # reject_tmax=0.2,\n",
    "        # reject=reject_criteria, \n",
    "        # flat=flat_criteria\n",
    "        baseline=None,\n",
    "        # picks=picks_eeg,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "        # verbose='CRITICAL',\n",
    "    )\n",
    "    \n",
    "    # epochs.drop_bad()\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5e07d-9618-47de-a706-687a529fb2fe",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8c673-5477-4126-b09d-870a8fcd9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_100_600_df_3-5_all_scales\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "# info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "info_filename = \"../data/scales/all_scales.csv\"\n",
    "\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32394ef-8197-4010-a680-bb85379baedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_name = \"go_nogo_500_300_df_3-5_all_scales\"\n",
    "# pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "# info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# # Check if data is already loaded\n",
    "# if os.path.isfile(pickled_data_filename):\n",
    "#     print(\"Pickled file found. Loading pickled data...\")\n",
    "#     epochs_df_3 = pd.read_pickle(pickled_data_filename)\n",
    "#     print(\"Done\")\n",
    "# else:\n",
    "#     print(\"Pickled file not found. Loading data...\")\n",
    "#     epochs_df_3 = create_df_data(\n",
    "#         test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "#     )\n",
    "#     epochs_df_3.name = df_name\n",
    "#     # save loaded data into a pickle file\n",
    "#     epochs_df_3.to_pickle(\"../data/\" + epochs_df_3.name + \".pkl\")\n",
    "#     print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055815fa-0d47-4bdd-9caf-51d60ced5d91",
   "metadata": {},
   "source": [
    "#### Read data for external testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c990ac1-14fc-4d0c-bf5d-d96f076e1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_100_600_test_df_3-5_all_scales\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/scales/all_scales.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_df = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_df.to_pickle(\"../data/\" + epochs_test_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0137a-0f6b-49c9-b376-a5d60c201045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_500_300_test_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_df_3 = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_df_3 = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_df_3.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_df_3.to_pickle(\"../data/\" + epochs_test_df_3.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326f5ec-d352-4f43-a403-c4debfbec641",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100 = epochs_df\n",
    "# X_test_df_100 = epochs_test_df\n",
    "\n",
    "# X_train_df_500 = epochs_df_3\n",
    "# X_test_df_500 = epochs_test_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e2949e-ec48-4cc9-8ff0-9e79164037f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'ern'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e4384-3e06-40b8-8fb8-1b5e1bc38459",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "# Between subject variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b9705-01f0-400b-8731-74ae0d12c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_df = pd.DataFrame({'pipeline': [], 'values': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4be3c8-3f83-4350-b705-4e69f5ba4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = [\"Fpz\",\n",
    "        \"AFz\",\n",
    "        \"Fz\", \n",
    "        \"FCz\",\n",
    "        \"C1\",\"Cz\",\"C2\",\n",
    "        \"CPz\",\n",
    "        \"P1\",\"Pz\", \"P2\",\n",
    "        ] if condition == 'pe' else [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bf4f8-8a5b-409f-8f66-01d4106b5f3e",
   "metadata": {},
   "source": [
    "- lowpass filters; ref:M, baseline:100, SF:no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902f5bb-ecf5-4067-b8c4-a121a057c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40, 30, 20, 15]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "    \n",
    "    pipeline_name = str(cutoff) + ' Hz'\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "    \n",
    "    \n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "                # ('neg', ReverseSignal()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                # (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "    x_feature_100_between = np.sum(x_pre, axis=1)\n",
    "    x_100_std_between = np.std(x_feature_100_between, axis=0)\n",
    "\n",
    "    values = x_feature_100_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_100_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    ###########################################################################################\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b86c0-a661-4e68-be68-177d797f3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b4c4d-60e5-44ad-90e4-2602e2b18025",
   "metadata": {},
   "source": [
    "- reference : Mastoids vs Average ; lowpass:40, baseline:100, SF=no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa5649-f027-4996-8efc-87bb64ac0e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "pipeline_name = 'Mastoids'\n",
    "\n",
    "ern_features = Pipeline(steps=[\n",
    "                                (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                ])\n",
    "\n",
    "\n",
    "pe_features = Pipeline(steps = [\n",
    "                                (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                (\"pe_amplitude\", PeAmplitude2()),\n",
    "                ])\n",
    "\n",
    "ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"average\", Evoked()),\n",
    "            ('extract_data', ExtractData()),\n",
    "            (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "x_feature_100_between = np.sum(x_pre, axis=1)\n",
    "x_100_std_between = np.std(x_feature_100_between, axis=0)\n",
    "\n",
    "values = x_feature_100_between.flatten().tolist()\n",
    "names = [pipeline_name] * len(x_feature_100_between)\n",
    "\n",
    "temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "between_df = between_df.append(temp_df, ignore_index=True)\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "pipeline_name = 'Average'\n",
    "\n",
    "ern_features = Pipeline(steps=[\n",
    "                                (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                ])\n",
    "\n",
    "\n",
    "pe_features = Pipeline(steps = [\n",
    "                                (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                (\"pe_amplitude\", PeAmplitude2()),\n",
    "                ])\n",
    "\n",
    "ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"reference_to_avg\", ReferenceToAverage()),\n",
    "            (\"average\", Evoked()),\n",
    "            ('extract_data', ExtractData()),\n",
    "            (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "x_feature_100_between = np.sum(x_pre, axis=1)\n",
    "x_100_std_between = np.std(x_feature_100_between, axis=0)\n",
    "\n",
    "values = x_feature_100_between.flatten().tolist()\n",
    "names = [pipeline_name] * len(x_feature_100_between)\n",
    "\n",
    "temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "between_df = between_df.append(temp_df, ignore_index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba75f90-076c-4703-af93-5fed474ecd17",
   "metadata": {},
   "source": [
    "- Baseline: -100, -500 ; ref:M, lowpass:30, SF=no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f49399-af66-4367-923f-e8f19937bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "pipeline_name = '-100:0'\n",
    "\n",
    "ern_features = Pipeline(steps=[\n",
    "                                (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                ])\n",
    "\n",
    "\n",
    "pe_features = Pipeline(steps = [\n",
    "                                (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                (\"pe_amplitude\", PeAmplitude2()),\n",
    "                ])\n",
    "\n",
    "ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"average\", Evoked()),\n",
    "            ('extract_data', ExtractData()),\n",
    "            (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "x_feature_100_between = np.sum(x_pre, axis=1)\n",
    "x_100_std_between = np.std(x_feature_100_between, axis=0)\n",
    "\n",
    "values = x_feature_100_between.flatten().tolist()\n",
    "names = [pipeline_name] * len(x_feature_100_between)\n",
    "\n",
    "temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "between_df = between_df.append(temp_df, ignore_index=True) \n",
    "\n",
    "##########################################################################\n",
    "\n",
    "X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "\n",
    "pipeline_name = '-500:-300'\n",
    "\n",
    "ern_features = Pipeline(steps=[\n",
    "                                (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                ])\n",
    "\n",
    "\n",
    "pe_features = Pipeline(steps = [\n",
    "                                (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                (\"pe_amplitude\", PeAmplitude2()),\n",
    "                ])\n",
    "\n",
    "ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"average\", Evoked()),\n",
    "            ('extract_data', ExtractData()),\n",
    "            (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "x_feature_500_between = np.sum(x_pre, axis=1)\n",
    "x_500_std_between = np.std(x_feature_500_between, axis=0)\n",
    "\n",
    "values = x_feature_500_between.flatten().tolist()\n",
    "names = [pipeline_name] * len(x_feature_500_between)\n",
    "\n",
    "temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05a216-d4b7-4728-8801-2485a25b43ea",
   "metadata": {},
   "source": [
    "- spatial filter with different boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9713c78-f364-4195-a1ad-6e334f34ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_list = [red_box7_prim, red_box8_prim, red_box9_prim, box_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c843f4-438c-4a64-84d8-cf7812cae71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, box in enumerate(box_list): \n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = f'SF Box-{index}'\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=4, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "                (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "    x_feature_100_sf_between = np.sum(x_pre, axis=1)\n",
    "    x_100_std_sf_between = np.std(x_feature_100_sf_between, axis=0)\n",
    "\n",
    "    values = x_feature_100_sf_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_100_sf_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae870b08-7884-4175-a343-5a78fe467b7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "------\n",
    "## Internal and external between-subject variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4a193-1603-4ed8-aeb5-4fe5db557fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'ern'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f36836-0932-47d3-915c-ab20c6e753a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_internal_external_df = pd.DataFrame({'pipeline': [], 'values': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1956be-173f-45c3-a549-2fb219835b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = [\"Fpz\",\n",
    "        \"AFz\",\n",
    "        \"Fz\", \n",
    "        \"FCz\",\n",
    "        \"C1\",\"Cz\",\"C2\",\n",
    "        \"CPz\",\n",
    "        \"P1\",\"Pz\", \"P2\",\n",
    "        ] if condition == 'pe' else [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a342c-71fc-4fbc-abdd-56720722cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae68795-1bb6-4f18-a765-80226c3de706",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "pipeline_name = 'Internal'\n",
    "\n",
    "ern_features = Pipeline(steps=[\n",
    "                                (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                # (\"scaler\", StandardScaler()),\n",
    "                ])\n",
    "\n",
    "\n",
    "pe_features = Pipeline(steps = [\n",
    "                                (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=9)),\n",
    "                                (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                # (\"scaler\", StandardScaler()),\n",
    "                ])\n",
    "\n",
    "ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"average\", Evoked()),\n",
    "            ('extract_data', ExtractData()),\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\",PCA(n_components=4, random_state=random_state)),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "x_feature_100_sf_between = np.sum(x_pre, axis=1)\n",
    "x_100_std_sf_between = np.std(x_feature_100_sf_between, axis=0)\n",
    "\n",
    "values = x_feature_100_sf_between.flatten().tolist()\n",
    "names = [pipeline_name] * len(x_feature_100_sf_between)\n",
    "\n",
    "temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "between_internal_external_df = between_internal_external_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f16c62-523c-4dab-a053-39d86713c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df_100copy = pd.DataFrame(copy.deepcopy(X_test_df_100.to_dict()))\n",
    "\n",
    "pipeline_name = 'External'\n",
    "\n",
    "ern_features = Pipeline(steps=[\n",
    "                                (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                # (\"scaler\", StandardScaler()),\n",
    "                ])\n",
    "\n",
    "\n",
    "pe_features = Pipeline(steps = [\n",
    "                                (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=9)),\n",
    "                                (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                # (\"scaler\", StandardScaler()),\n",
    "                ])\n",
    "\n",
    "ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"average\", Evoked()),\n",
    "            ('extract_data', ExtractData()),\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\",PCA(n_components=4, random_state=random_state)),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "]).fit_transform(X_test_df_100copy)\n",
    "\n",
    "x_feature_100_sf_between = np.sum(x_pre, axis=1)\n",
    "x_100_std_sf_between = np.std(x_feature_100_sf_between, axis=0)\n",
    "\n",
    "values = x_feature_100_sf_between.flatten().tolist()\n",
    "names = [pipeline_name] * len(x_feature_100_sf_between)\n",
    "\n",
    "temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "between_internal_external_df = between_internal_external_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f540e-c711-4261-80ee-8d56bfdf40b5",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64b4d2-5413-46f8-aaba-96ba681baa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = 1/2.54\n",
    "dpi = 200\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = dpi\n",
    "plt.rcParams['figure.figsize'] = [9*cm,3.5*cm]\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams['ytick.labelsize'] = 5\n",
    "plt.rcParams['xtick.labelsize'] = 5\n",
    "plt.rcParams['axes.labelsize'] = 6\n",
    "plt.rcParams[\"axes.edgecolor\"] = \".15\"\n",
    "plt.rcParams[\"axes.linewidth\"]  = 0.3\n",
    "\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(-2,2))\n",
    "plt.tick_params(axis='both', which='major', bottom=True, left = True)\n",
    "\n",
    "pal = sns.color_palette('deep')\n",
    "colors = [pal.as_hex()[3], pal.as_hex()[2]]\n",
    "\n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "\n",
    "ax = sns.pointplot(x = 'values', y='pipeline', hue='pipeline', data = between_internal_external_df, \n",
    "                   orient='h', join=False, estimator=np.std, ci=95,capsize=0, errwidth=1, \n",
    "                   scale = 0.4, legend=False)\n",
    "\n",
    "plt.legend([],[], frameon=False)\n",
    "\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['top'].set_color('black') \n",
    "ax.spines['right'].set_color('black')\n",
    "ax.spines['left'].set_color('black')\n",
    "\n",
    "ax.set_xlabel(\"Standard Deviation\")\n",
    "ax.set_ylabel('')\n",
    "\n",
    "ax.figure.savefig(f\"paper_images/between_subject_internal_external_{condition}_dpi_{dpi}.png\",  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77382c-ef13-4c83-b141-95e559bead46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "----\n",
    "# WITHIN SUBJECT\n",
    "- without spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f518d7b-b1cc-4d06-86b2-a696718ada98",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_df = pd.DataFrame({'pipeline': [], 'values': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45130623-d7bf-4ee5-8614-a627a66e1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = [\"Fpz\",\n",
    "        \"AFz\",\n",
    "        \"Fz\", \n",
    "        \"FCz\",\n",
    "        \"C1\",\"Cz\",\"C2\",\n",
    "        \"CPz\",\n",
    "        \"P1\",\"Pz\", \"P2\",\n",
    "        ] if condition == 'pe' else [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34904f-053c-4054-b2c5-70dc8a1c244b",
   "metadata": {},
   "source": [
    "- lowpass filters; ref:M, baseline:100, SF:no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce24511-aaa5-4cf4-9266-2f1f806b6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40, 30, 20, 15]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "    \n",
    "    pipeline_name = str(cutoff) + ' Hz'\n",
    "    \n",
    "    for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "        X = X_train_df_100copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "\n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                        (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                        (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                        ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                        (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                        (\"pe_amplitude\", PeAmplitude2()),\n",
    "                        ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "        this_x = Pipeline([\n",
    "                    (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "                    (\"binning\", BinTransformer(step=12)),\n",
    "                    (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                    (\"features\", ern_pe_features),\n",
    "        ]).fit_transform(x_pre)\n",
    "        \n",
    "        f_vector = np.sum(this_x, axis=1)\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1404d0-4087-469b-8c1f-07936fe6d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da3216-782f-4fa8-9c69-e7f94324f48a",
   "metadata": {},
   "source": [
    "- reference : Mastoids vs Average ; lowpass:40, baseline:100, SF=no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b7d2f-d891-4b02-9749-9df45facdef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "pipeline_name = 'Mastoids'\n",
    "\n",
    "for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "    X = X_train_df_100copy[i:i+1]    \n",
    "    x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            ('extract_data', ExtractDataEpochs()),\n",
    "    ]).fit_transform(X)  \n",
    "\n",
    "    x_pre = x_pre[0] \n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "    this_x = Pipeline([\n",
    "                (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(x_pre)\n",
    "\n",
    "    f_vector = np.sum(this_x, axis=1)\n",
    "    f_variance = np.std(f_vector)\n",
    "\n",
    "    data = {'pipeline' : pipeline_name,\n",
    "            'values' : f_variance,\n",
    "           }\n",
    "\n",
    "    within_df = within_df.append(data, ignore_index = True)\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "pipeline_name = 'Average'\n",
    "\n",
    "for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "    X = X_train_df_100copy[i:i+1]    \n",
    "    x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"reference_to_avg\", ReferenceToAverage()),\n",
    "            ('extract_data', ExtractDataEpochs()),\n",
    "    ]).fit_transform(X)  \n",
    "\n",
    "    x_pre = x_pre[0] \n",
    "    \n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "    this_x = Pipeline([\n",
    "                (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(x_pre)\n",
    "\n",
    "    f_vector = np.sum(this_x, axis=1)\n",
    "    f_variance = np.std(f_vector)\n",
    "\n",
    "    data = {'pipeline' : pipeline_name,\n",
    "            'values' : f_variance,\n",
    "           }\n",
    "\n",
    "    within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa5b20-9186-4f74-8e40-34d894d5529c",
   "metadata": {},
   "source": [
    "- Baseline: -100, -500 ; ref:M, lowpass:30, SF=no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f0b82-e4b7-460b-a80b-b2668afe0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "pipeline_name = '-100:0'\n",
    "\n",
    "\n",
    "for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "    X = X_train_df_100copy[i:i+1]    \n",
    "    x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            ('extract_data', ExtractDataEpochs()),\n",
    "    ]).fit_transform(X) \n",
    "    x_pre = x_pre[0] \n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "    this_x = Pipeline([\n",
    "                (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(x_pre)\n",
    "\n",
    "    f_vector = np.sum(this_x, axis=1)\n",
    "    f_variance = np.std(f_vector)\n",
    "\n",
    "    data = {'pipeline' : pipeline_name,\n",
    "            'values' : f_variance,\n",
    "           }\n",
    "\n",
    "    within_df = within_df.append(data, ignore_index = True)\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "\n",
    "pipeline_name = '-500:-300'\n",
    "\n",
    "for i in range(0,len(X_train_df_500copy)):\n",
    "\n",
    "    X = X_train_df_500copy[i:i+1]    \n",
    "    x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            ('extract_data', ExtractDataEpochs()),\n",
    "    ]).fit_transform(X) \n",
    "    x_pre = x_pre[0] \n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "    this_x = Pipeline([\n",
    "                (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(x_pre)\n",
    "\n",
    "    f_vector = np.sum(this_x, axis=1)\n",
    "    f_variance = np.std(f_vector)\n",
    "\n",
    "    data = {'pipeline' : pipeline_name,\n",
    "            'values' : f_variance,\n",
    "           }\n",
    "\n",
    "    within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f1543-4e81-4f1d-ba85-b0ff5cf33708",
   "metadata": {},
   "source": [
    "- spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecfc2ad-19de-4ddc-9c65-56fbcf3ea5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_list = [red_box7_prim, red_box8_prim, red_box9_prim, box_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107ef69-0f50-466a-be00-c7e7b8ac0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, box in enumerate(box_list): \n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "    \n",
    "    pre_pip_for_spatial_filter = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list = box)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=4, random_state=random_state)),\n",
    "    ]).fit(X_train_df_100copy)\n",
    "\n",
    "    spatial_filter = pre_pip_for_spatial_filter['spatial_filter']\n",
    "    \n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = f'SF Box-{index}'\n",
    "\n",
    "    for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "        X = X_train_df_100copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X) \n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        x_pre_pre = Pipeline([\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\", spatial_filter),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "                (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                        (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                        (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                        # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                        # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                        # (\"scaler\", StandardScaler()),\n",
    "                        ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                        (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                        (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                        # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                        # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                        # (\"scaler\", StandardScaler()),\n",
    "                        ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "        this_x = Pipeline([\n",
    "                (\"features\", ern_pe_features),\n",
    "            ]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "        f_vector = np.sum(this_x, axis=1)\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91db68d-6288-4c3a-a0c9-b06e2de79fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "# pipeline_name = 'Spatial Filter'\n",
    "\n",
    "# for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "#     X = X_train_df_100copy[i:i+1]    \n",
    "#     x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             ('extract_data', ExtractDataEpochs()),\n",
    "#     ]).fit_transform(X) \n",
    "#     x_pre = x_pre[0] \n",
    "    \n",
    "#     x_pre_pre = Pipeline([\n",
    "#             (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#             (\"spatial_filter\", spatial_filter),\n",
    "#             (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#             (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "#             (\"binning\", BinTransformer(step=12)),\n",
    "#             (\"centering\", CenteredSignalAfterBaseline3_bis())]).transform(x_pre)\n",
    "\n",
    "\n",
    "#     ern_features = Pipeline(steps=[\n",
    "#                                     (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                     (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                                     # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                     # (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                     # (\"scaler\", StandardScaler()),\n",
    "#                     ])\n",
    "\n",
    "\n",
    "#     pe_features = Pipeline(steps = [\n",
    "#                                     (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "#                                     (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                                     # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                     # (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                     # (\"scaler\", StandardScaler()),\n",
    "#                     ])\n",
    "    \n",
    "#     ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "#     this_x = Pipeline([\n",
    "#             (\"features\", ern_pe_features),\n",
    "#         ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "    \n",
    "#     f_vector = np.sum(this_x, axis=1)\n",
    "#     f_variance = np.std(f_vector)\n",
    "\n",
    "#     data = {'pipeline' : pipeline_name,\n",
    "#             'values' : f_variance,\n",
    "#            }\n",
    "\n",
    "#     within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff830364-e721-40ad-8de3-040f3a583d0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "## Internal and external within-subject variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9093f2b-7e4a-42c8-95a2-cd0576b0c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_internal_external_df = pd.DataFrame({'pipeline': [], 'values': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e3cb2-7727-4ed6-ac25-a23eb2ee0d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = [\"Fpz\",\n",
    "        \"AFz\",\n",
    "        \"Fz\", \n",
    "        \"FCz\",\n",
    "        \"C1\",\"Cz\",\"C2\",\n",
    "        \"CPz\",\n",
    "        \"P1\",\"Pz\", \"P2\",\n",
    "        ] if condition == 'pe' else [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d686f-eef0-4e1a-887e-42d405392856",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335b6b8-0e22-434d-9c25-05f548bac54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pip_for_spatial_filter = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list = box)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=4, random_state=random_state)),\n",
    "]).fit(X_train_df_100copy)\n",
    "\n",
    "spatial_filter = pre_pip_for_spatial_filter['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2a047-0429-47e8-b5e9-d0804ba0e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "pipeline_name = 'Internal'\n",
    "\n",
    "for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "    X = X_train_df_100copy[i:i+1]    \n",
    "    x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            ('extract_data', ExtractDataEpochs()),\n",
    "    ]).fit_transform(X) \n",
    "    x_pre = x_pre[0] \n",
    "    \n",
    "    x_pre_pre = Pipeline([\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\", spatial_filter),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "    \n",
    "    ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "    this_x = Pipeline([\n",
    "            (\"features\", ern_pe_features),\n",
    "        ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "    \n",
    "    f_vector = np.sum(this_x, axis=1)\n",
    "    f_variance = np.std(f_vector)\n",
    "\n",
    "    data = {'pipeline' : pipeline_name,\n",
    "            'values' : f_variance,\n",
    "           }\n",
    "\n",
    "    within_internal_external_df = within_internal_external_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5718b-c3ef-4fbe-96df-9cc36d343a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df_100copy = pd.DataFrame(copy.deepcopy(X_test_df_100.to_dict()))\n",
    "\n",
    "pipeline_name = 'External'\n",
    "\n",
    "for i in range(0,len(X_test_df_100copy)):\n",
    "\n",
    "    X = X_test_df_100copy[i:i+1]    \n",
    "    x_pre = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            ('extract_data', ExtractDataEpochs()),\n",
    "    ]).fit_transform(X) \n",
    "    x_pre = x_pre[0] \n",
    "    \n",
    "    x_pre_pre = Pipeline([\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\", spatial_filter),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter2(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "    \n",
    "    ern_pe_features = FeatureUnion([(\"pe_features\", pe_features)]) if condition == 'pe' else FeatureUnion([(\"ern_features\", ern_features)])\n",
    "\n",
    "\n",
    "    this_x = Pipeline([\n",
    "            (\"features\", ern_pe_features),\n",
    "        ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "    \n",
    "    f_vector = np.sum(this_x, axis=1)\n",
    "    f_variance = np.std(f_vector)\n",
    "\n",
    "    data = {'pipeline' : pipeline_name,\n",
    "            'values' : f_variance,\n",
    "           }\n",
    "\n",
    "    within_internal_external_df = within_internal_external_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e22eb78-66cb-4a82-8f99-8390dd787ae6",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3737dbe-a664-4a90-ae8f-1291930c528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "cm = 1/2.54\n",
    "dpi = 200\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = dpi\n",
    "plt.rcParams['figure.figsize'] = [9*cm,3.5*cm]\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams['ytick.labelsize'] = 5\n",
    "plt.rcParams['xtick.labelsize'] = 5\n",
    "plt.rcParams['axes.labelsize'] = 6\n",
    "plt.rcParams[\"axes.edgecolor\"] = \".15\"\n",
    "plt.rcParams[\"axes.linewidth\"]  = 0.3\n",
    "\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(-2,2))\n",
    "plt.tick_params(axis='both', which='major', bottom=True, left = True)\n",
    "\n",
    "pal = sns.color_palette('deep')\n",
    "colors = [pal.as_hex()[3], pal.as_hex()[2]]\n",
    "\n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "ax = sns.violinplot(\n",
    "    x = 'values', \n",
    "    y = 'pipeline', \n",
    "    data = within_internal_external_df, \n",
    "    orient='h',  \n",
    "    inner=\"quartile\", \n",
    "    cut=2, \n",
    "    scale='count', \n",
    "    linewidth=0.4,\n",
    "    # palette='deep'\n",
    ")\n",
    "\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['top'].set_color('black') \n",
    "ax.spines['right'].set_color('black')\n",
    "ax.spines['left'].set_color('black')\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Standard Deviation\")\n",
    "ax.set_ylabel('') \n",
    "\n",
    "ax.figure.savefig(f\"paper_images/within_subject_internal_external_{condition}_dpi_{dpi}.png\",  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090c94b-a945-4e59-b577-af7c353c9f8c",
   "metadata": {},
   "source": [
    "## Internal consistency\n",
    "\n",
    "consistency = betweenPerson / between_person + within_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d1f02-808b-45b9-81b7-5f60ebd86894",
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_internal_external_df = pd.DataFrame({'pipeline': [], 'internal_variance': []})\n",
    "\n",
    "for pipeline in between_internal_external_df['pipeline'].unique().tolist():\n",
    "    \n",
    "    between_std = np.std(np.array(between_internal_external_df.loc[between_internal_external_df['pipeline'] == pipeline, 'values'].tolist()))                     \n",
    "    within_list = np.array(within_internal_external_df.loc[within_internal_external_df['pipeline'] == pipeline, 'values'].tolist())\n",
    "    \n",
    "    for person_variance in within_list:\n",
    "        \n",
    "        internal = between_std/(between_std + person_variance)    \n",
    "        data = {'pipeline' : pipeline,\n",
    "                'internal_variance' : internal,\n",
    "               }\n",
    "    \n",
    "        consistency_internal_external_df = consistency_internal_external_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d8f6e-4f62-4789-9344-cef99515889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_df = pd.DataFrame({'pipeline': [], 'internal_variance': []})\n",
    "\n",
    "for pipeline in between_df['pipeline'].unique().tolist():\n",
    "    \n",
    "    between_std = np.std(np.array(between_df.loc[between_df['pipeline'] == pipeline, 'values'].tolist()))                     \n",
    "    within_list = np.array(within_df.loc[within_df['pipeline'] == pipeline, 'values'].tolist())\n",
    "    \n",
    "    for person_variance in within_list:\n",
    "        \n",
    "        internal = between_std/(between_std + person_variance)    \n",
    "        data = {'pipeline' : pipeline,\n",
    "                'internal_variance' : internal,\n",
    "               }\n",
    "    \n",
    "        internal_df = internal_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4dc4f-dbf9-4b00-996f-2a8a10342b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = 1/2.54\n",
    "dpi = 200\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = dpi\n",
    "plt.rcParams['figure.figsize'] = [9*cm,3.5*cm]\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams['ytick.labelsize'] = 5\n",
    "plt.rcParams['xtick.labelsize'] = 5\n",
    "plt.rcParams['axes.labelsize'] = 6\n",
    "plt.rcParams[\"axes.edgecolor\"] = \".15\"\n",
    "plt.rcParams[\"axes.linewidth\"]  = 0.3\n",
    "\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(-2,2))\n",
    "plt.tick_params(axis='both', which='major', bottom=True, left = True)\n",
    "\n",
    "pal = sns.color_palette('deep')\n",
    "colors = [pal.as_hex()[3], pal.as_hex()[2]]\n",
    "\n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "ax = sns.violinplot(\n",
    "    x = 'internal_variance', \n",
    "    y = 'pipeline', \n",
    "    data = consistency_internal_external_df, \n",
    "    orient='h', \n",
    "    inner=\"quartile\", \n",
    "    scale='count', \n",
    "    linewidth=0.4,\n",
    "    # palette='deep'\n",
    ")\n",
    "\n",
    "ax.spines['bottom'].set_color('black')\n",
    "ax.spines['top'].set_color('black') \n",
    "ax.spines['right'].set_color('black')\n",
    "ax.spines['left'].set_color('black')\n",
    "\n",
    "ax.set_xlabel(\"Internal Consistency\")\n",
    "ax.set_ylabel('') \n",
    "\n",
    "ax.figure.savefig(f\"paper_images/internal_consistency_internal_external_{condition}_dpi_{dpi}.png\",  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f43282-a29d-49e2-84fc-965089f83f85",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Visualization of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcdfb3-a4bc-4c5b-8e99-b3ab5fce156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "# X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d81fa9-377c-4b0f-8a4b-6e0812b75900",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = ['Fpz', 'AFz', 'Fz', 'FCz', 'C1', 'Cz', 'C2', 'CPz', 'P1', 'Pz', 'P2']\n",
    "red_box8_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F1\", \"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "red_box7_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d8306-c6c5-43db-ad1c-72d24c8e7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "\n",
    "pre_pip = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list = red_box8_prim)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "        (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "        # (\"lowpass_filter\", LowpassFilter()),\n",
    "        # ('neg', ReverseComponent3()),\n",
    "        # (\"binning\", BinTransformer(step=12)),\n",
    "        # (\"baseline\", ErnBaselined()),\n",
    "        # (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "         # (\"ern_data_extraction\", ErnTransformer()),\n",
    "        # (\"ern_amplitude\", ErnAmplitude2_prim()),\n",
    "]).fit(X_train_df_copy)\n",
    "\n",
    "X = pre_pip.transform(X_train_df_copy)\n",
    "X_mean = np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910d763-3d55-4e28-8ff9-650f9291c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e2a9f-1477-4e57-8e84-cb89503828d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = pre_pip['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe856a13-86c7-487d-bdb4-78444029f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = sf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b15f7e-84e5-4eb8-b496-c3ef5e9b3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_copy = components.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c5665-a4b8-459d-b512-a8e657205af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a68b04-4bad-45ee-9375-829bec5a1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_0 = [components_copy[0]]\n",
    "components_1 = [components_copy[1]]\n",
    "components_2 = [components_copy[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b80e2-4571-4ca3-9649-56575d174244",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a18191-8243-4aad-bf8e-43dda4c3fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "\n",
    "pre_pip = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list = red_box8_prim)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "        # (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        # (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "        # (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=179)),\n",
    "        # (\"lowpass_filter\", LowpassFilter()),\n",
    "        # ('neg', ReverseComponent3()),\n",
    "        # (\"binning\", BinTransformer(step=12)),\n",
    "        # (\"baseline\", ErnBaselined()),\n",
    "        # (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "        # (\"ern_data_extraction\", ErnTransformer()),\n",
    "        # (\"ern_amplitude\", ErnAmplitude2_prim()),\n",
    "]).fit(X_train_df_copy)\n",
    "\n",
    "X = pre_pip.transform(X_train_df_copy)\n",
    "X_mean = np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90e2b2-404c-4d1c-94d4-d57bdd24d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in components_0:\n",
    "    print(component)\n",
    "\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for item in component:\n",
    "        if item > 0:\n",
    "            positive.append(item)\n",
    "            negative.append(0)\n",
    "        else:\n",
    "            positive.append(0)\n",
    "            negative.append(item)\n",
    "    print(positive)\n",
    "    print(negative)\n",
    "    \n",
    "positive = np.array(positive).reshape(13,-1)\n",
    "negative = np.array(negative).reshape(13,-1)\n",
    "\n",
    "positive_compo = X_mean * positive\n",
    "negative_compo = X_mean * negative\n",
    "\n",
    "positive_signal_0 = np.sum(positive_compo, axis=0)\n",
    "negative_signal_0 = np.sum(negative_compo, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ffd8e7-f451-4dc7-9ba4-a7bfda471957",
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in components_1:\n",
    "    print(component)\n",
    "\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for item in component:\n",
    "        if item > 0:\n",
    "            positive.append(item)\n",
    "            negative.append(0)\n",
    "        else:\n",
    "            positive.append(0)\n",
    "            negative.append(item)\n",
    "    print(positive)\n",
    "    print(negative)\n",
    "    \n",
    "positive = np.array(positive).reshape(13,-1)\n",
    "negative = np.array(negative).reshape(13,-1)\n",
    "\n",
    "positive_compo = X_mean * positive\n",
    "negative_compo = X_mean * negative\n",
    "\n",
    "positive_signal_1 = np.sum(positive_compo, axis=0)\n",
    "negative_signal_1 = np.sum(negative_compo, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13788d11-4c19-4bf9-8911-75f78d32c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in components_2:\n",
    "    print(component)\n",
    "\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for item in component:\n",
    "        if item > 0:\n",
    "            positive.append(item)\n",
    "            negative.append(0)\n",
    "        else:\n",
    "            positive.append(0)\n",
    "            negative.append(item)\n",
    "    print(positive)\n",
    "    print(negative)\n",
    "    \n",
    "positive = np.array(positive).reshape(13,-1)\n",
    "negative = np.array(negative).reshape(13,-1)\n",
    "\n",
    "positive_compo = X_mean * positive\n",
    "negative_compo = X_mean * negative\n",
    "\n",
    "positive_signal_2 = np.sum(positive_compo, axis=0)\n",
    "negative_signal_2 = np.sum(negative_compo, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a3850-3000-4a57-ae91-79e4e0935e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_0 = positive_signal_0 + negative_signal_0\n",
    "c_1 = positive_signal_1 + negative_signal_1\n",
    "c_2 = positive_signal_2 + negative_signal_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867ceae-007d-468c-bc05-b6374e609d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "pal = sns.color_palette('deep')\n",
    "blue, red, green =  pal.as_hex()[0], pal.as_hex()[3], pal.as_hex()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9305da0-bc57-47c8-855a-4dac4036d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "pal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dbe2f-b901-4cb3-bd00-534ed7dc0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = 1/2.54\n",
    "dpi = 200\n",
    "\n",
    "plt.rcParams['figure.dpi'] = dpi\n",
    "plt.rcParams['figure.figsize'] = [9*cm,5*cm]\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams['ytick.labelsize'] = 5\n",
    "plt.rcParams['xtick.labelsize'] = 5\n",
    "plt.rcParams['axes.labelsize'] = 6\n",
    "plt.rcParams[\"axes.edgecolor\"] = \".15\"\n",
    "plt.rcParams[\"axes.linewidth\"]  = 0.3\n",
    "plt.rc('legend',fontsize=5) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(positive_signal_0, lw=1, color=blue)\n",
    "# plt.plot(-negative_signal_0, lw=3, color=red)\n",
    "\n",
    "plt.plot(-c_0, lw= 3, color = green)\n",
    "\n",
    "ax.set_xticks([0, 26, 52, 78, 104, 130, 156, 181])\n",
    "ax.set_xticklabels(['-100', '0','100', '200', '300','400', '500', '600'])\n",
    "\n",
    "ax.axvspan(37-11, 37+15, alpha=0.2, color='grey', lw=0)\n",
    "ax.axvspan(37+30, 37+65, alpha=0.2, color='grey', lw=0)\n",
    "plt.axhline(y=0, color='black', linestyle='--', lw=1)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(\"Amplitude (μV)\")\n",
    "\n",
    "plt.legend([\"Negative part\", \"PCA component\"], loc=1)\n",
    "\n",
    "\n",
    "plt.savefig(f'first_components_dpi_{dpi}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c262bf5-28bc-4da3-96aa-9333535df877",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = 1/2.54\n",
    "dpi = 200\n",
    "\n",
    "plt.rcParams['figure.dpi'] = dpi\n",
    "plt.rcParams['figure.figsize'] = [9*cm,5*cm]\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams['ytick.labelsize'] = 5\n",
    "plt.rcParams['xtick.labelsize'] = 5\n",
    "plt.rcParams['axes.labelsize'] = 6\n",
    "plt.rcParams[\"axes.edgecolor\"] = \".15\"\n",
    "plt.rc('legend',fontsize=5) # using a size in points\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "plt.plot(positive_signal_1, color=red, lw=1)\n",
    "plt.plot(-negative_signal_1, color=blue, lw=1)\n",
    "\n",
    "ax.set_xticks([0, 26, 52, 78, 104, 130, 156, 181])\n",
    "ax.set_xticklabels(['-100', '0','100', '200', '300','400', '500', '600'])\n",
    "\n",
    "ax.axvspan(37-11, 37+15, alpha=0.2, color='grey', lw=0)\n",
    "ax.axvspan(37+55, 37+85, alpha=0.2, color='grey', lw=0)\n",
    "\n",
    "plt.plot(c_1, lw=3,  color = green)\n",
    "\n",
    "plt.axhline(y=0, color='black', linestyle='--', lw=1)\n",
    "\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(\"Amplitude (μV)\")\n",
    "plt.legend([\"Positive part\", \"Negative part\", \"PCA component\"], loc=1)\n",
    "\n",
    "plt.savefig(f'second_components_dpi_{dpi}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc29a4f-ca3b-4c65-9e73-0031a23d9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = 1/2.54\n",
    "dpi = 200\n",
    "\n",
    "plt.rcParams['figure.dpi'] = dpi\n",
    "plt.rcParams['figure.figsize'] = [9*cm,5*cm]\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams['ytick.labelsize'] = 5\n",
    "plt.rcParams['xtick.labelsize'] = 5\n",
    "plt.rcParams['axes.labelsize'] = 6\n",
    "plt.rcParams[\"axes.edgecolor\"] = \".15\"\n",
    "plt.rc('legend',fontsize=5) # using a size in points\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "plt.plot(positive_signal_2, color=red, lw=1)\n",
    "plt.plot(-negative_signal_2, color=blue, lw=1)\n",
    "\n",
    "ax.set_xticks([0, 26, 52, 78, 104, 130, 156, 181])\n",
    "ax.set_xticklabels(['-100', '0','100', '200', '300','400', '500', '600'])\n",
    "\n",
    "ax.axvspan(37-11, 37+15, alpha=0.2, color='grey', lw=0)\n",
    "ax.axvspan(37+23, 37+55, alpha=0.2, color='grey', lw=0)\n",
    "\n",
    "plt.plot(c_2, lw = 3,  color = green)\n",
    "\n",
    "plt.axhline(y=0, color='black', linestyle='--', lw=1)\n",
    "\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(\"Amplitude (μV)\")\n",
    "plt.legend([\"Positive part\", \"Negative part\", \"PCA component\"], loc=1)\n",
    "\n",
    "\n",
    "plt.savefig(f'third_components_dpi_{dpi}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91252e-da53-47ed-8d5c-502ada6ee2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(positive_signal_0)\n",
    "# plt.plot(-negative_signal_0)\n",
    "\n",
    "# plt.plot(X_mean[0], lw=4)\n",
    "plt.plot(X[1][0], lw = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee5e17-f111-4dd8-9ffd-bbe31e25ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(positive_signal_1)\n",
    "plt.plot(-negative_signal_1)\n",
    "\n",
    "# plt.plot(X_mean[1], lw=4)\n",
    "plt.plot(c_1, lw = 2)\n",
    "# plt.savefig(\"differences_component_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c17656-6e64-41cf-9189-85bc23c321cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(positive_signal_2)\n",
    "plt.plot(-negative_signal_2)\n",
    "\n",
    "# plt.plot(X_mean[1], lw=4)\n",
    "plt.plot(c_2, lw = 2)\n",
    "# plt.savefig(\"differences_component_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfabaf5-fb5d-4491-99ff-244ec7709404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fdf95-311b-49a3-8986-7672d57b0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(1,3):\n",
    "    plt.plot(X_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6b6a6-36d6-418d-8b87-8caa71d0feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,10):\n",
    "    plt.plot(X[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a11b97-6817-4a7d-8b08-3d587fa77d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,10):\n",
    "    plt.plot(X[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf4bab-f079-4c8d-9ec2-2890d085221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,10):\n",
    "    plt.plot(X[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d281b-b5fe-4d59-ad63-71aa6632ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/results/regression_union_100-600_cached_ern_amplitude_various_scales_with_external_p.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc63489-4b7a-4070-ac42-784b4136ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['scale', 'mean_cv_r2', 'external_score', 'p-value', 'external_p-value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2dc5d-535a-4a60-8864-d16efcf403d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['external_score'] >= 0) & (df['mean_cv_r2'] >= 0) ][columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da5cf1-c743-4ed4-8a05-d33b85e0e471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
