{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a73099-38c1-4bf0-afd3-5481df7c3d19",
   "metadata": {},
   "source": [
    "# Internal consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff9f56-5483-44c7-b9cc-486a31838279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e84d26-b32c-41ae-b3f4-81e0ee55156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import ast\n",
    "import os.path as op\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium.featurize\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from rumination_experiment_transformers_averaged_CDS import *\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c69899-6a32-4418-9079-8bd66aed497f",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading data\n",
    "\n",
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc483a7-410f-4b65-9955-9df42c783bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths TODO\n",
    "dir_path = os.path.dirname(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a25389-d030-415b-982d-b68df981b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6  # Start and end of the segments\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1\n",
    "ALL = 2\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ef4f5-f711-4f8e-8ba5-163dc50bd2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_order_list = [\n",
    "    \"Fp1\",\n",
    "    \"AF7\",\n",
    "    \"AF3\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    \"F5\",\n",
    "    \"F7\",\n",
    "    \"FT7\",\n",
    "    \"FC5\",\n",
    "    \"FC3\",\n",
    "    \"FC1\",\n",
    "    \"C1\",\n",
    "    \"C3\",\n",
    "    \"C5\",\n",
    "    \"T7\",\n",
    "    \"TP7\",\n",
    "    \"CP5\",\n",
    "    \"CP3\",\n",
    "    \"CP1\",\n",
    "    \"P1\",\n",
    "    \"P3\",\n",
    "    \"P5\",\n",
    "    \"P7\",\n",
    "    \"P9\",\n",
    "    \"PO7\",\n",
    "    \"PO3\",\n",
    "    \"O1\",\n",
    "    \"Iz\",\n",
    "    \"Oz\",\n",
    "    \"POz\",\n",
    "    \"Pz\",\n",
    "    \"CPz\",\n",
    "    \"Fpz\",\n",
    "    \"Fp2\",\n",
    "    \"AF8\",\n",
    "    \"AF4\",\n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"F2\",\n",
    "    \"F4\",\n",
    "    \"F6\",\n",
    "    \"F8\",\n",
    "    \"FT8\",\n",
    "    \"FC6\",\n",
    "    \"FC4\",\n",
    "    \"FC2\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"C2\",\n",
    "    \"C4\",\n",
    "    \"C6\",\n",
    "    \"T8\",\n",
    "    \"TP8\",\n",
    "    \"CP6\",\n",
    "    \"CP4\",\n",
    "    \"CP2\",\n",
    "    \"P2\",\n",
    "    \"P4\",\n",
    "    \"P6\",\n",
    "    \"P8\",\n",
    "    \"P10\",\n",
    "    \"PO8\",\n",
    "    \"PO4\",\n",
    "    \"O2\",\n",
    "]\n",
    "\n",
    "channels_dict = dict(zip(channels_order_list, np.arange(1, 64, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c412f36-66a2-4fd8-bfa7-caa98c4a315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=\"all\",\n",
    "    personal=True,\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "\n",
    "    On default, loads a train set: chooses only 80% of participants\n",
    "    and for each of them chooses 80% of epochs.\n",
    "    It will choose them deterministically.\n",
    "\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    If test_participants is set to True, it will load remaining 20% of participants.\n",
    "    If test_epochs is set to True, it will load remaining 20% of epochs.\n",
    "    Test epochs are chronologically after train epochs,\n",
    "    because it reflects real usage (first callibration and then classification).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "    personal: bool\n",
    "        whether a model will be both trained and tested on epochs from one person\n",
    "        if false, person's epochs aren't split into test and train\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    print(os.path.abspath(\"\"))\n",
    "    dir_path = os.path.dirname(os.path.abspath(\"\"))\n",
    "    print(dir_path)\n",
    "    header_files_glob = os.path.join(dir_path, \"data/responses_100_600/*.vhdr\")\n",
    "    header_files = glob.glob(header_files_glob)\n",
    "\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    # cut 20% of data for testing\n",
    "    h_train, h_test = train_test_split(header_files, test_size=0.3, random_state=0)\n",
    "\n",
    "    if test_participants:\n",
    "        header_files = h_test\n",
    "    else:\n",
    "        header_files = h_train\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 3 or len(correct) < 3:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, participant_epochs, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991419b-7acd-42bc-b048-e5e9fa3cb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, participant_epochs, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        if info == \"all\":\n",
    "            rumination_df = pd.read_csv(info_filename)\n",
    "        else:\n",
    "            rumination_df = pd.read_csv(info_filename, usecols=[\"File\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"File\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "\n",
    "#     for epoch in correct:\n",
    "#         epoch_df = pd.DataFrame(\n",
    "#             {\"id\": [id], \"epoch\": [epoch], \"marker\": [CORRECT]}\n",
    "#         ).join(info_df)\n",
    "#         participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "#     for epoch in error:\n",
    "#         epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [epoch], \"marker\": [ERROR]}).join(\n",
    "#             info_df\n",
    "#         )\n",
    "#         participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "        \n",
    "#     print(participant_epochs)\n",
    "        \n",
    "    epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [participant_epochs], \"marker\": [ALL]}).join(\n",
    "            info_df\n",
    "        )\n",
    "    participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88517e-884a-4b5b-9652-1bf2e7787aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' means that bad segments are rejected automatically.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = True\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5e07d-9618-47de-a706-687a529fb2fe",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8c673-5477-4126-b09d-870a8fcd9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_100_600_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32394ef-8197-4010-a680-bb85379baedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_500_300_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df_3 = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df_3 = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df_3.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df_3.to_pickle(\"../data/\" + epochs_df_3.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74938fe4-b8ed-4e5f-a5f8-90a24c88265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_400_600_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df2 = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df2 = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df2.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df2.to_pickle(\"../data/\" + epochs_df2.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055815fa-0d47-4bdd-9caf-51d60ced5d91",
   "metadata": {},
   "source": [
    "#### Read data for external testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c990ac1-14fc-4d0c-bf5d-d96f076e1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_100_600_test_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_df = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_df.to_pickle(\"../data/\" + epochs_test_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0137a-0f6b-49c9-b376-a5d60c201045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_500_300_test_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_df_3 = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_df_3 = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_df_3.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_df_3.to_pickle(\"../data/\" + epochs_test_df_3.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326f5ec-d352-4f43-a403-c4debfbec641",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100 = epochs_df\n",
    "# X_train_df_400 = epochs_df2\n",
    "# X_train_df_500 = epochs_df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e4384-3e06-40b8-8fb8-1b5e1bc38459",
   "metadata": {},
   "source": [
    "---\n",
    "# Between subject variation\n",
    "- without spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f933b8-7084-4689-b107-41278c91417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df_rum = pd.read_pickle(\"../data/split0.3/regression_union_100-600_baselined_centered-2_diff_boxes_diff_pe-ind_diff_models.pkl\")\n",
    "\n",
    "# ern_fex = results_df_rum[results_df_rum['model'] == 'en'].best_estimator[11]['features'].transformer_list[0][1]['feature_extraction']\n",
    "# pe_fex = results_df_rum[results_df_rum['model'] == 'en'].best_estimator[11]['features'].transformer_list[1][1]['feature_extraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b9705-01f0-400b-8731-74ae0d12c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_df = pd.DataFrame({'pipeline': [], 'values': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4be3c8-3f83-4350-b705-4e69f5ba4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "box= [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"Pz\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776dfd2c-8395-42fa-8d11-78372ce326f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d719f0-74c8-4677-b822-2e25411f8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-100:0'\n",
    "\n",
    "# ern_features = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                 ])\n",
    "\n",
    "\n",
    "# pe_features = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                 ])\n",
    "\n",
    "# ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "# x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             (\"average\", Evoked()),\n",
    "#             ('extract_data', ExtractData()),\n",
    "#             (\"lowpass_filter\", LowpassFilter()),\n",
    "#             (\"binning\", BinTransformer(step=12)),\n",
    "#             (\"baseline\", ErnBaselined()),\n",
    "#             (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "#             (\"features\", ern_pe_features),\n",
    "# ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "# x_feature_100_between = np.sum(x_pre, axis=1)\n",
    "# x_100_std_between = np.std(x_feature_100_between, axis=0)\n",
    "\n",
    "# values = x_feature_100_between.flatten().tolist()\n",
    "# names = [pipeline_name] * len(x_feature_100_between)\n",
    "\n",
    "# temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "# between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c600f-75c0-490b-94df-9f33b62182d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78512c92-97d5-4ca1-b46a-5249f148f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-500:-300'\n",
    "\n",
    "# ern_features = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                 ])\n",
    "\n",
    "\n",
    "# pe_features = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                 ])\n",
    "\n",
    "# ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "# x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             (\"average\", Evoked()),\n",
    "#             ('extract_data', ExtractData()),\n",
    "#             (\"lowpass_filter\", LowpassFilter()),\n",
    "#             (\"binning\", BinTransformer(step=12)),\n",
    "#             (\"baseline\", ErnBaselined()),\n",
    "#             (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "#             (\"features\", ern_pe_features),\n",
    "# ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "# x_feature_500_between = np.sum(x_pre, axis=1)\n",
    "# x_500_std_between = np.std(x_feature_500_between, axis=0)\n",
    "\n",
    "# values = x_feature_500_between.flatten().tolist()\n",
    "# names = [pipeline_name] * len(x_feature_500_between)\n",
    "\n",
    "# temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "# between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bf4f8-8a5b-409f-8f66-01d4106b5f3e",
   "metadata": {},
   "source": [
    "- different lowpass filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902f5bb-ecf5-4067-b8c4-a121a057c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-100:0 '+ str(cutoff)\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2_prim()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=7)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                ('neg', ReverseSignal()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "    x_feature_100_between = np.sum(x_pre, axis=1)\n",
    "    x_100_std_between = np.std(x_feature_100_between, axis=0)\n",
    "\n",
    "    values = x_feature_100_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_100_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    ###########################################################################################\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa5649-f027-4996-8efc-87bb64ac0e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [15,20,30,40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-100:0 '+ str(cutoff) + ' no BS'\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                # (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "    x_feature_100_between = np.sum(x_pre, axis=1)\n",
    "    x_100_std_between = np.std(x_feature_100_between, axis=0)\n",
    "\n",
    "    values = x_feature_100_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_100_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f49399-af66-4367-923f-e8f19937bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40]:\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-500:-300 '+ str(cutoff)\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "    x_feature_500_between = np.sum(x_pre, axis=1)\n",
    "    x_500_std_between = np.std(x_feature_500_between, axis=0)\n",
    "\n",
    "    values = x_feature_500_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_500_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    ##########################################################################\n",
    "    \n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-500:-300 '+ str(cutoff) + ' no BS'\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                # (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "    x_feature_500_between = np.sum(x_pre, axis=1)\n",
    "    x_500_std_between = np.std(x_feature_500_between, axis=0)\n",
    "\n",
    "    values = x_feature_500_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_500_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05a216-d4b7-4728-8801-2485a25b43ea",
   "metadata": {},
   "source": [
    "- with spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124bb984-d695-4ad0-84c6-73f220b7c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c843f4-438c-4a64-84d8-cf7812cae71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 SF '+ str(cutoff)\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2_prim()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                ('neg', ReverseComponent3()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "                # (\"ern_amplitude\", ErnAmplitude2())\n",
    "    ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "    x_feature_100_sf_between = np.sum(x_pre, axis=1)\n",
    "    x_100_std_sf_between = np.std(x_feature_100_sf_between, axis=0)\n",
    "\n",
    "    values = x_feature_100_sf_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_100_sf_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 SF '+ str(cutoff) + ' no BS'\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                ('neg', ReverseComponent3()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                # (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "                # (\"ern_amplitude\", ErnAmplitude2())\n",
    "    ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "    x_feature_100_sf_between = np.sum(x_pre, axis=1)\n",
    "    x_100_std_sf_between = np.std(x_feature_100_sf_between, axis=0)\n",
    "\n",
    "    values = x_feature_100_sf_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_100_sf_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80b568-34a4-4633-befa-036cf1d96adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d9773-4a88-4023-ba85-673f3baf7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [15,20,30,40]:\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-500:-300 SF ' + str(cutoff)\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                ('neg', ReverseComponent3()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "    x_feature_500_sf_between = np.sum(x_pre, axis=1)\n",
    "    x_500_std_sf_between = np.std(x_feature_500_sf_between, axis=0)\n",
    "\n",
    "    values = x_feature_500_sf_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_500_sf_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    ##################################################################\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-500:-300 SF ' + str(cutoff) + ' no BS'\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                ('neg', ReverseComponent3()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                # (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "    x_feature_500_sf_between = np.sum(x_pre, axis=1)\n",
    "    x_500_std_sf_between = np.std(x_feature_500_sf_between, axis=0)\n",
    "\n",
    "    values = x_feature_500_sf_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_500_sf_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f856b-8f27-48f3-9b3b-365121554e17",
   "metadata": {},
   "source": [
    "- with spatial filter and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03450039-0b24-41f7-82ff-857e1e45945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce9087-934b-4fd4-86bc-f39188cc7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-100:0 SF EX'\n",
    "\n",
    "# x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             (\"average\", Evoked()),\n",
    "#             ('extract_data', ExtractData()),\n",
    "#             (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#             (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "#             (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#             (\"lowpass_filter\", LowpassFilter()),\n",
    "#             ('neg', ReverseComponent3()),\n",
    "#             (\"binning\", BinTransformer(step=12)),\n",
    "#             (\"baseline\", ErnBaselined()),\n",
    "#             (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "#             # (\"features\", ern_pe_features),\n",
    "#             # (\"ern_amplitude\", ErnAmplitude2())\n",
    "# ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "\n",
    "# ern_features_pre = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                                 # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "#                 ]).fit_transform(x_pre)\n",
    "\n",
    "\n",
    "# ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "# pe_features_pre = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                                 # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "\n",
    "#                 ]).fit_transform(x_pre)\n",
    "\n",
    "# pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "# # ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10).fit_transform()\n",
    "\n",
    "# x_pre2 = zip(ern_features, pe_features)\n",
    "# x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "        \n",
    "# x_feature_100_sf_between = np.sum(x_pre2, axis=1)\n",
    "# x_100_std_sf_between = np.std(x_feature_100_sf_between, axis=0)\n",
    "\n",
    "# values = x_feature_100_sf_between.flatten().tolist()\n",
    "# names = [pipeline_name] * len(x_feature_100_sf_between)\n",
    "\n",
    "# temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "# between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072d44e-04b8-4f02-bd3d-9df29694e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe505b8-d960-4bca-a21f-dbe6881ed8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-500:-300 SF EX'\n",
    "\n",
    "# # ern_features = Pipeline(steps=[\n",
    "# #                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "# #                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "# #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                                 (\"scaler\", StandardScaler()),\n",
    "# #                                 (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "# #                 ])\n",
    "\n",
    "\n",
    "# # pe_features = Pipeline(steps = [\n",
    "# #                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "# #                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "# #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                                 (\"scaler\", StandardScaler()),\n",
    "# #                                 (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "# #                 ])\n",
    "\n",
    "# # ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "# # x_pre = Pipeline([\n",
    "# #             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "# #             (\"average\", Evoked()),\n",
    "# #             ('extract_data', ExtractData()),\n",
    "# #             (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "# #             (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "# #             (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "# #             (\"lowpass_filter\", LowpassFilter()),\n",
    "# #             ('neg', ReverseComponent3()),\n",
    "# #             (\"binning\", BinTransformer(step=12)),\n",
    "# #             (\"baseline\", ErnBaselined()),\n",
    "# #             (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "# #             (\"features\", ern_pe_features),\n",
    "# #             # (\"ern_amplitude\", ErnAmplitude2())\n",
    "# # ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "# x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             (\"average\", Evoked()),\n",
    "#             ('extract_data', ExtractData()),\n",
    "#             (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#             (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "#             (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#             (\"lowpass_filter\", LowpassFilter()),\n",
    "#             ('neg', ReverseComponent3()),\n",
    "#             (\"binning\", BinTransformer(step=12)),\n",
    "#             (\"baseline\", ErnBaselined()),\n",
    "#             (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "#             # (\"features\", ern_pe_features),\n",
    "#             # (\"ern_amplitude\", ErnAmplitude2())\n",
    "# ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "\n",
    "# ern_features_pre = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                                 # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "#                 ]).fit_transform(x_pre)\n",
    "\n",
    "\n",
    "# ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "# pe_features_pre = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                                 # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "\n",
    "#                 ]).fit_transform(x_pre)\n",
    "\n",
    "# pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "# # ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10).fit_transform()\n",
    "\n",
    "# x_pre2 = zip(ern_features, pe_features)\n",
    "# x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "\n",
    "# x_feature_500_sf_between = np.sum(x_pre2, axis=1)\n",
    "# x_500_std_sf_between = np.std(x_feature_500_sf_between, axis=0)\n",
    "\n",
    "# values = x_feature_500_sf_between.flatten().tolist()\n",
    "# names = [pipeline_name] * len(x_feature_500_sf_between)\n",
    "\n",
    "# temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "# between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f540e-c711-4261-80ee-8d56bfdf40b5",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64b4d2-5413-46f8-aaba-96ba681baa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,8)},font_scale = 1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "ax = sns.pointplot(x = 'values', y='pipeline', data = between_df, orient='h', join=False, estimator=np.std, ci=95,capsize=.05,)\n",
    "# ax.figure.savefig(\"between_subject_std_ern_pe_lowpass.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe15df-0ecc-43c1-9acf-6743a262b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,8)},font_scale = 1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "ax = sns.pointplot(x = 'values', y='pipeline', data = between_df, orient='h', join=False, estimator=np.std, ci=95,capsize=.05,)\n",
    "# ax.figure.savefig(\"between_subject_std_ern_pe_lowpass.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77382c-ef13-4c83-b141-95e559bead46",
   "metadata": {},
   "source": [
    "----\n",
    "# WITHIN SUBJECT\n",
    "- without spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f518d7b-b1cc-4d06-86b2-a696718ada98",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_df = pd.DataFrame({'pipeline': [], 'values': []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da388eb4-07fd-4a6d-881f-7f97e0b88acf",
   "metadata": {},
   "source": [
    "## -100 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d5f22-221c-42b0-ae54-08dcf8ec644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1911c9b-cfd7-46db-9038-e8f1bf7411e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 ' + str(cutoff)\n",
    "\n",
    "    for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "        X = X_train_df_100copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "\n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2_prim()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=7)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            ('neg', ReverseSignal()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "            # (\"ern_amplitude\", ErnAmplitude2()),\n",
    "        ]).fit_transform(x_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)\n",
    "        \n",
    "        ##########################################################################\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2018f07-bf40-43b1-8fc3-1d31bf56b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [15,20,30,40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 ' + str(cutoff) + ' no BS'\n",
    "\n",
    "    for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "        X = X_train_df_100copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "\n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "            # (\"ern_amplitude\", ErnAmplitude2()),\n",
    "        ]).fit_transform(x_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79f315-00f4-4d77-9ffa-96a1758c7eed",
   "metadata": {},
   "source": [
    "## -500 to -300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544359a6-7414-4f0a-9d8d-faa9765f6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2eab5d-1760-4f65-9d65-30d7dda19461",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [15,20,30,40]:\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "\n",
    "    pipeline_name = '-500:-300 ' + str(cutoff)\n",
    "\n",
    "    for i in range(0,len(X_train_df_500copy)):\n",
    "\n",
    "        X = X_train_df_500copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "            # (\"ern_amplitude\", ErnAmplitude2()),\n",
    "        ]).fit_transform(x_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)\n",
    "        \n",
    "        #############################################################################\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "\n",
    "    pipeline_name = '-500:-300 ' + str(cutoff)  + ' no BS'\n",
    "\n",
    "    for i in range(0,len(X_train_df_500copy)):\n",
    "\n",
    "        X = X_train_df_500copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "            # (\"ern_amplitude\", ErnAmplitude2()),\n",
    "        ]).fit_transform(x_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2fdb1b-8dcf-405e-8129-ba281ef4f620",
   "metadata": {},
   "source": [
    "- With spatial filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8f609-bc53-4483-8f05-bec95bcbe8f4",
   "metadata": {},
   "source": [
    "### -100 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c094367-344c-49b8-acdb-a42414308e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b008b0-79f3-4346-bc05-7ced5a69a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pip_for_spatial_filter = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list = box)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "]).fit(X_train_df_100copy)\n",
    "\n",
    "spatial_filter = pre_pip_for_spatial_filter['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e89bd-fa73-4221-9bcd-27eabfbdfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28971e9-bf99-4ad1-8492-44710c6ff6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 SF ' + str(cutoff)\n",
    "\n",
    "    for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "        X = X_train_df_100copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "\n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        x_pre_pre = Pipeline([\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\", spatial_filter),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "        ern_features_pre = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2_prim()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "        # ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "        pe_features_pre = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "        # pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "    #     x_pre2 = zip(ern_features, pe_features)\n",
    "    #     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "\n",
    "    #     print(x_pre2.shape)\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features_pre), (\"pe_features\", pe_features_pre)], n_jobs = 10)\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"features\", ern_pe_features),\n",
    "        ]).fit_transform(x_pre_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)\n",
    "        \n",
    "        ########################################################################\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 SF ' + str(cutoff)  + ' no BS'\n",
    "\n",
    "\n",
    "    for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "        X = X_train_df_100copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "\n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        x_pre_pre = Pipeline([\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\", spatial_filter),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "        ern_features_pre = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "        # ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "        pe_features_pre = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "        # pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "    #     x_pre2 = zip(ern_features, pe_features)\n",
    "    #     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "\n",
    "    #     print(x_pre2.shape)\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features_pre), (\"pe_features\", pe_features_pre)], n_jobs = 10)\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"features\", ern_pe_features),\n",
    "        ]).fit_transform(x_pre_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f2c68-d047-477e-a514-263030d594bf",
   "metadata": {},
   "source": [
    "### -500 to -300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea380d64-bd87-4f1c-92d8-9cf84560e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f731af0-3c01-45f7-8e72-3197c292a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pip_for_spatial_filter = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list = box)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "]).fit(X_train_df_500copy)\n",
    "\n",
    "spatial_filter = pre_pip_for_spatial_filter['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a8244-b6b8-428c-afc6-50976c0e40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc8de9-dab2-47cf-ac1b-7bf4b2766d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [15,20,30,40]:\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "\n",
    "    pipeline_name = '-500:-300 SF ' + str(cutoff)\n",
    "\n",
    "    for i in range(0,len(X_train_df_500copy)):\n",
    "\n",
    "        X = X_train_df_500copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "\n",
    "\n",
    "        x_pre_pre = Pipeline([\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\", spatial_filter),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "        ]).transform(x_pre)\n",
    "\n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "        x_pre2 = Pipeline([(\"features\", ern_pe_features)]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "    #     x_pre_pre = Pipeline([\n",
    "    #         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "    #         (\"spatial_filter\", spatial_filter),\n",
    "    #         (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "    #         (\"lowpass_filter\", LowpassFilter()),\n",
    "    #         ('neg', ReverseComponent3()),\n",
    "    #         (\"binning\", BinTransformer(step=12)),\n",
    "    #         (\"baseline\", ErnBaselined()),\n",
    "    #         (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "    #     ern_features_pre = Pipeline(steps=[\n",
    "    #                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "    #                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "    #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "    #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "    #                                 (\"scaler\", StandardScaler()),\n",
    "    #                 ]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "    #     ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "    #     pe_features_pre = Pipeline(steps = [\n",
    "    #                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "    #                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "    #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "    #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "    #                                 (\"scaler\", StandardScaler()),\n",
    "    #                 ]).fit_transform(x_pre_pre)\n",
    "\n",
    "    #     pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "\n",
    "    #     x_pre2 = zip(ern_features, pe_features)\n",
    "    #     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)     \n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)\n",
    "        \n",
    "        ##################################################################\n",
    "                \n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "\n",
    "    pipeline_name = '-500:-300 SF ' + str(cutoff) + ' no BS'\n",
    "\n",
    "\n",
    "    for i in range(0,len(X_train_df_500copy)):\n",
    "\n",
    "        X = X_train_df_500copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "\n",
    "\n",
    "        x_pre_pre = Pipeline([\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\", spatial_filter),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "        ]).transform(x_pre)\n",
    "\n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "        x_pre2 = Pipeline([(\"features\", ern_pe_features)]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "    #     x_pre_pre = Pipeline([\n",
    "    #         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "    #         (\"spatial_filter\", spatial_filter),\n",
    "    #         (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "    #         (\"lowpass_filter\", LowpassFilter()),\n",
    "    #         ('neg', ReverseComponent3()),\n",
    "    #         (\"binning\", BinTransformer(step=12)),\n",
    "    #         (\"baseline\", ErnBaselined()),\n",
    "    #         (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "    #     ern_features_pre = Pipeline(steps=[\n",
    "    #                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "    #                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "    #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "    #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "    #                                 (\"scaler\", StandardScaler()),\n",
    "    #                 ]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "    #     ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "    #     pe_features_pre = Pipeline(steps = [\n",
    "    #                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "    #                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "    #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "    #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "    #                                 (\"scaler\", StandardScaler()),\n",
    "    #                 ]).fit_transform(x_pre_pre)\n",
    "\n",
    "    #     pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "\n",
    "    #     x_pre2 = zip(ern_features, pe_features)\n",
    "    #     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)     \n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0178ed8-953a-43ac-b55f-439a9570834a",
   "metadata": {},
   "source": [
    "- with SF and FEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d944d58-6674-4a72-872f-286a84d1efd1",
   "metadata": {},
   "source": [
    "### -100 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963ad9e-c503-446c-aba1-4de09c3c8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736e65a-ad72-4cc2-b481-3a73ed06ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_pip_for_spatial_filter = Pipeline([\n",
    "#         (\"channels_extraction\",PickChannels(channels_list = box)),\n",
    "#         (\"average\", Evoked()),\n",
    "#         ('extract_averaged_data', ExtractData()),\n",
    "#         # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "#         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "# ]).fit(X_train_df_100copy)\n",
    "\n",
    "# spatial_filter = pre_pip_for_spatial_filter['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d55ccb-b177-43f9-ab8e-e2f21405ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802e617-f7f9-47cd-a5dd-9354b72d3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-100:0 SF EX'\n",
    "\n",
    "# for i in range(0,len(X_train_df_100copy)):\n",
    "    \n",
    "#     X = X_train_df_100copy[i:i+1]    \n",
    "#     x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             ('extract_data', ExtractDataEpochs()),\n",
    "#     ]).fit_transform(X)  \n",
    "    \n",
    "#     x_pre = x_pre[0] \n",
    "    \n",
    "#     x_pre_pre = Pipeline([\n",
    "#         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         (\"spatial_filter\", spatial_filter),\n",
    "#         (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#         (\"lowpass_filter\", LowpassFilter()),\n",
    "#         ('neg', ReverseComponent3()),\n",
    "#         (\"binning\", BinTransformer(step=12)),\n",
    "#         (\"baseline\", ErnBaselined()),\n",
    "#         (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "    \n",
    "#     ern_features_pre = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                 ]).fit_transform(x_pre_pre)\n",
    "        \n",
    "#     ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "#     pe_features_pre = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                 ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "#     pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "        \n",
    "#     x_pre2 = zip(ern_features, pe_features)\n",
    "#     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "    \n",
    "#     # print(x_pre2.shape)\n",
    "\n",
    "#     # ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "    \n",
    "#     # x_pre2 = Pipeline([\n",
    "#     #     (\"features\", ern_pe_features),\n",
    "#     # ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "#     # f_vector = np.mean(x_pre2, axis=1)\n",
    "#     f_vector = np.sum(x_pre2, axis=1)\n",
    "    \n",
    "#     f_variance = np.std(f_vector)\n",
    "    \n",
    "#     data = {'pipeline' : pipeline_name,\n",
    "#             'values' : f_variance,\n",
    "#            }\n",
    "    \n",
    "#     within_df = within_df.append(data, ignore_index = True)\n",
    "    \n",
    "    \n",
    "# #     variances_100_sf.append(f_variance)\n",
    "    \n",
    "# # variances_100_sf = np.array(variances_100_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c162ca-88be-4a78-a575-ccea414858a2",
   "metadata": {},
   "source": [
    "### -500 to -300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46ef21-b854-474d-ad1b-e58875ff28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6653ec9-2160-4892-870d-a13efb422bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_pip_for_spatial_filter = Pipeline([\n",
    "#         (\"channels_extraction\",PickChannels(channels_list = box)),\n",
    "#         (\"average\", Evoked()),\n",
    "#         ('extract_averaged_data', ExtractData()),\n",
    "#         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "# ]).fit(X_train_df_500copy)\n",
    "\n",
    "# spatial_filter = pre_pip_for_spatial_filter['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddabf8-a497-4f85-8e1b-4a54bc7697d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea3d9c-9d02-4c1f-99bc-b35f0f863558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-500:-300 SF EX'\n",
    "\n",
    "# for i in range(0,len(X_train_df_500copy)):\n",
    "    \n",
    "#     X = X_train_df_500copy[i:i+1]    \n",
    "#     x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             ('extract_data', ExtractDataEpochs()),\n",
    "#     ]).fit_transform(X)  \n",
    "#     x_pre = x_pre[0] \n",
    "\n",
    "    \n",
    "    \n",
    "# #     x_pre_pre = Pipeline([\n",
    "# #         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "# #         (\"spatial_filter\", spatial_filter),\n",
    "# #         (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "# #         (\"lowpass_filter\", LowpassFilter()),\n",
    "# #         ('neg', ReverseComponent3()),\n",
    "# #         (\"binning\", BinTransformer(step=12)),\n",
    "# #         (\"baseline\", ErnBaselined()),\n",
    "# #         (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "# #     ]).transform(x_pre)\n",
    "    \n",
    "    \n",
    "# #     ern_features = Pipeline(steps=[\n",
    "# #                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "# #                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "# #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                                 (\"scaler\", StandardScaler()),\n",
    "# #                                 (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "# #                 ])\n",
    "\n",
    "\n",
    "# #     pe_features = Pipeline(steps = [\n",
    "# #                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "# #                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "# #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                                 (\"scaler\", StandardScaler()),\n",
    "# #                                 (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "# #                 ])\n",
    "\n",
    "# #     ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "    \n",
    "# #     x_pre2 = Pipeline([(\"features\", ern_pe_features)]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "#     x_pre_pre = Pipeline([\n",
    "#         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         (\"spatial_filter\", spatial_filter),\n",
    "#         (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#         (\"lowpass_filter\", LowpassFilter()),\n",
    "#         ('neg', ReverseComponent3()),\n",
    "#         (\"binning\", BinTransformer(step=12)),\n",
    "#         (\"baseline\", ErnBaselined()),\n",
    "#         (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "    \n",
    "#     ern_features_pre = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                 ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "    \n",
    "#     ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "#     pe_features_pre = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                 ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "#     pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "    \n",
    "    \n",
    "#     x_pre2 = zip(ern_features, pe_features)\n",
    "#     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "    \n",
    "\n",
    "#     # f_vector = np.mean(x_pre2, axis=1)\n",
    "#     f_vector = np.sum(x_pre2, axis=1)     \n",
    "#     f_variance = np.std(f_vector)\n",
    "    \n",
    "#     data = {'pipeline' : pipeline_name,\n",
    "#             'values' : f_variance,\n",
    "#            }\n",
    "    \n",
    "#     within_df = within_df.append(data, ignore_index = True)\n",
    "# #     variances_500_sf.append(f_variance)\n",
    "    \n",
    "# # variances_500_sf = np.array(variances_500_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e22eb78-66cb-4a82-8f99-8390dd787ae6",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3737dbe-a664-4a90-ae8f-1291930c528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,12)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "ax = sns.violinplot(x = 'values', y = 'pipeline', data = within_df, orient='h', )\n",
    "# ax.figure.savefig(\"within_subject_std_ern_pe_with_lowpass_BS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e76de-d0cd-4631-9655-743cf0da16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,12)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "ax = sns.violinplot(x = 'values', y = 'pipeline', data = within_df, orient='h', )\n",
    "# ax.figure.savefig(\"within_subject_std_ern_pe_with_lowpass_BS.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090c94b-a945-4e59-b577-af7c353c9f8c",
   "metadata": {},
   "source": [
    "## Internal consistency\n",
    "\n",
    "consistency = betweenPerson / between_person + within_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d1f02-808b-45b9-81b7-5f60ebd86894",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_df = pd.DataFrame({'pipeline': [], 'internal_variance': []})\n",
    "\n",
    "for pipeline in between_df['pipeline'].unique().tolist():\n",
    "    \n",
    "    between_std = np.std(np.array(between_df.loc[between_df['pipeline'] == pipeline, 'values'].tolist()))                     \n",
    "    within_list = np.array(within_df.loc[within_df['pipeline'] == pipeline, 'values'].tolist())\n",
    "    \n",
    "    for person_variance in within_list:\n",
    "        \n",
    "        internal = between_std/(between_std + person_variance)    \n",
    "        data = {'pipeline' : pipeline,\n",
    "                'internal_variance' : internal,\n",
    "               }\n",
    "    \n",
    "        internal_df = internal_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4dc4f-dbf9-4b00-996f-2a8a10342b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "ax = sns.violinplot(x = 'internal_variance', y = 'pipeline', data = internal_df, orient='h', inner=\"quartile\")\n",
    "# ax.figure.savefig(\"internal_consistency_ern_pe_lowpass_BS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5f85b-da98-4a7c-a545-1de98d624830",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_df[(within_df['pipeline'] == '-100:0 SF') & (within_df['values'] > 0.00005)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e1a01-a5c8-4520-babd-ebc0abb9d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 182 == 0\n",
    "\n",
    "indexes = within_df[(within_df['pipeline'] == '-100:0 SF') & (within_df['values'] > 0.00005)].index\n",
    "indexes = np.array(indexes.tolist())\n",
    "\n",
    "indexes_new = [index - 196 for index in indexes]\n",
    "indexes_new\n",
    "# index 8, 19, 33, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f43282-a29d-49e2-84fc-965089f83f85",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualization of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcdfb3-a4bc-4c5b-8e99-b3ab5fce156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "# X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d81fa9-377c-4b0f-8a4b-6e0812b75900",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = ['Fpz', 'AFz', 'Fz', 'FCz', 'C1', 'Cz', 'C2', 'CPz', 'P1', 'Pz', 'P2']\n",
    "red_box8_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d8306-c6c5-43db-ad1c-72d24c8e7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pip = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list = red_box8_prim)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "        (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        ('neg', ReverseComponent3()),\n",
    "        (\"binning\", BinTransformer(step=12)),\n",
    "        # (\"baseline\", ErnBaselined()),\n",
    "        (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "]).fit(X_train_df_copy)\n",
    "\n",
    "X = pre_pip.transform(X_train_df_copy)\n",
    "X_mean = np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910d763-3d55-4e28-8ff9-650f9291c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e2a9f-1477-4e57-8e84-cb89503828d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = pre_pip['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe856a13-86c7-487d-bdb4-78444029f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = sf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a19723-dfc8-4b08-a8a6-3badfe3d4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_positive(number):\n",
    "    if number > 0:\n",
    "          return True  \n",
    "\n",
    "    return False\n",
    "\n",
    "def check_negative(number):\n",
    "    if number < 0:\n",
    "          return True  \n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b15f7e-84e5-4eb8-b496-c3ef5e9b3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_copy = components.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c5665-a4b8-459d-b512-a8e657205af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a68b04-4bad-45ee-9375-829bec5a1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_1 = [components_copy[1]]\n",
    "components_2 = [components_copy[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b80e2-4571-4ca3-9649-56575d174244",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ffd8e7-f451-4dc7-9ba4-a7bfda471957",
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in components_1:\n",
    "    print(component)\n",
    "\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for item in component:\n",
    "        if item > 0:\n",
    "            positive.append(item)\n",
    "            negative.append(0)\n",
    "        else:\n",
    "            positive.append(0)\n",
    "            negative.append(item)\n",
    "    print(positive)\n",
    "    print(negative)\n",
    "    \n",
    "positive = np.array(positive).reshape(11,-1)\n",
    "negative = np.array(negative).reshape(11,-1)\n",
    "\n",
    "positive_compo = X_mean * positive\n",
    "negative_compo = X_mean * negative\n",
    "\n",
    "positive_signal_1 = np.sum(positive_compo, axis=0)\n",
    "negative_signal_1 = np.sum(negative_compo, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13788d11-4c19-4bf9-8911-75f78d32c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in components_2:\n",
    "    print(component)\n",
    "\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for item in component:\n",
    "        if item > 0:\n",
    "            positive.append(item)\n",
    "            negative.append(0)\n",
    "        else:\n",
    "            positive.append(0)\n",
    "            negative.append(item)\n",
    "    print(positive)\n",
    "    print(negative)\n",
    "    \n",
    "positive = np.array(positive).reshape(11,-1)\n",
    "negative = np.array(negative).reshape(11,-1)\n",
    "\n",
    "positive_compo = X_mean * positive\n",
    "negative_compo = X_mean * negative\n",
    "\n",
    "positive_signal_2 = np.sum(positive_compo, axis=0)\n",
    "negative_signal_2 = np.sum(negative_compo, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a3850-3000-4a57-ae91-79e4e0935e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_1 = positive_signal_1 + negative_signal_1\n",
    "c_2 = positive_signal_2 + negative_signal_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91252e-da53-47ed-8d5c-502ada6ee2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee5e17-f111-4dd8-9ffd-bbe31e25ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(positive_signal_2)\n",
    "plt.plot(-negative_signal_2)\n",
    "\n",
    "# plt.plot(X_mean[1], lw=4)\n",
    "plt.plot(c_2, lw = 4)\n",
    "# plt.savefig(\"differences_component_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fdf95-311b-49a3-8986-7672d57b0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.plot(X_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6b6a6-36d6-418d-8b87-8caa71d0feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,10):\n",
    "    plt.plot(X[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a11b97-6817-4a7d-8b08-3d587fa77d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,10):\n",
    "    plt.plot(X[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf4bab-f079-4c8d-9ec2-2890d085221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,10):\n",
    "    plt.plot(X[i][2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
