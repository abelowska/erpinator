{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "import inspect\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "from cachier import cachier\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# ignore FastICA did not converge warnings\n",
    "# TODO investigate why doesn't it converge\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "#### Data read into dataframe structure. Each epoch is a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs = pd.read_pickle(pickled_data_filename)\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs = create_df_data(info_filename=info_filename)\n",
    "    epochs.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs.to_pickle(\"../data/\" + epochs.name + \".pkl\")\n",
    "\n",
    "epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "#### Sort participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped = epochs.groupby(\"id\")\n",
    "epochs[\"error_sum\"] = grouped[[\"marker\"]].transform(lambda x: (x.values == ERROR).sum())\n",
    "epochs[\"correct_sum\"] = grouped[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs = epochs.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")\n",
    "epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "#### Get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "_mne_epochs = load_epochs_from_file(\"../data/responses/GNG_AA0303-64 el.vhdr\")\n",
    "times = _mne_epochs.times\n",
    "\n",
    "_channel_info = _mne_epochs.info[\"chs\"]\n",
    "channel_locations = np.array([ch[\"loc\"][:3] for ch in _channel_info])\n",
    "channel_names = [ch[\"ch_name\"] for ch in _channel_info]\n",
    "\n",
    "channel_colors = channel_locations - channel_locations.min(axis=0)\n",
    "channel_colors /= channel_colors.max(axis=0)\n",
    "channel_colors = channel_colors * 255 // 1\n",
    "channel_colors = [f\"rgb({c[0]:.0f},{c[1]:.0f},{c[2]:.0f})\" for c in channel_colors]\n",
    "\n",
    "log_freq = np.log2(get_frequencies())  # for plotting CWT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def _numpy_hasher(args, kwargs):\n",
    "    def make_hashable(value):\n",
    "        if type(value) == np.ndarray:\n",
    "            # largest hash to minimize collisions\n",
    "            return xxhash.xxh128_digest(value.tobytes())\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "    bound = inspect.signature(vectorize).bind(*args, **kwargs)\n",
    "    bound.apply_defaults()\n",
    "    key = tuple(\n",
    "        (k, make_hashable(value)) for k, value in sorted(bound.arguments.items())\n",
    "    )\n",
    "    return key\n",
    "\n",
    "\n",
    "@cachier(pickle_reload=False, hash_params=_numpy_hasher)\n",
    "def vectorize(\n",
    "    X,\n",
    "    y,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"PCA\",\n",
    "    wv_weighting_n_components=3,\n",
    "):\n",
    "    # it returns features of the shape EPOCH x ICA_COMP x WAVELET_COMP\n",
    "    #          and params of the shape ICA_COMP x (SPATIAL_FILTER, WV_WEIGHTS)\n",
    "    #          wv_weights has a shape  WAVELET_COMPONENT x FREQUENCY x TIMEPOINT\n",
    "\n",
    "    # X has a shape EPOCHS x CHANNELS x TIMEPOINTS\n",
    "\n",
    "    # compute ICA\n",
    "    concat = np.concatenate(X, axis=1)\n",
    "    # concat.shape == (num_of_channels, timepoints)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    ica.fit(concat.T)\n",
    "    # ica.components_.shape == (n_components, num_of_channels)\n",
    "\n",
    "    params = []\n",
    "    features = []\n",
    "    for spatial_filter in ica.components_:\n",
    "        # apply ICA\n",
    "        X_filtered = np.tensordot(X, spatial_filter, axes=([1], [0]))\n",
    "        # they have shape EPOCHS x TIMEPOINTS\n",
    "\n",
    "        # apply cwt\n",
    "        X_cwts = np.array([cwt(epoch, mwt, cwt_density) for epoch in X_filtered])\n",
    "        # it has a shape EPOCH x FREQUENCY x TIMEPOINT\n",
    "\n",
    "        X_flattened = X_cwts.reshape(X_cwts.shape[0], -1)\n",
    "        if wv_weighting == \"single\":\n",
    "            # find bets separating wavelet\n",
    "            separations = get_separations(X_cwts[y == ERROR], X_cwts[y == CORRECT])\n",
    "            # separations are shaped FREQUENCY x TIMEPOINT\n",
    "            index = np.unravel_index(separations.argmax(), separations.shape)\n",
    "            wv_weights = np.zeros((1, *separations.shape))\n",
    "            wv_weights[0][index[0]][index[1]] = 1\n",
    "            # 'single' means only one wv_component is found\n",
    "            wv_weighting_n_components = 1\n",
    "        elif wv_weighting == \"PCA\":\n",
    "            pca = PCA(n_components=wv_weighting_n_components)\n",
    "            pca.fit(X_flattened)\n",
    "            wv_weights = pca.components_\n",
    "        elif wv_weighting == \"ICA\":\n",
    "            ica = FastICA(n_components=wv_weighting_n_components, tol=0.001)\n",
    "            ica.fit(X_flattened)\n",
    "            wv_weights = ica.components_\n",
    "        elif wv_weighting == \"LDA\":\n",
    "            lda = LinearDiscriminantAnalysis(n_components=wv_weighting_n_components)\n",
    "            lda.fit(X_flattened, y)\n",
    "            wv_weights = lda.scalings_\n",
    "        else:\n",
    "            raise ValueError(\"wrong wv_choice argument\")\n",
    "\n",
    "        # unflatten wv_weights\n",
    "        cwt_shape = X_cwts.shape[1:]  # FREQUENCY x TIMEPOINT shape\n",
    "        wv_weights = wv_weights.reshape(wv_weighting_n_components, *cwt_shape)\n",
    "        # X_cwts has a shape EPOCH x FREQUENCY x TIMEPOINT\n",
    "        # wv_weights has a shape  WAVELET_COMPONENT x FREQUENCY x TIMEPOINT\n",
    "        one_channel_features = np.tensordot(X_cwts, wv_weights, axes=([1, 2], [1, 2]))\n",
    "        # one_channel_features has a shape EPOCH x WAVELET_COMPONENT\n",
    "\n",
    "        params.append((spatial_filter, wv_weights))\n",
    "        features.append(one_channel_features)\n",
    "\n",
    "    features = np.array(features)\n",
    "    # transform it from shape ICA_COMP x EPOCH x WAVELET_COMP\n",
    "    #                      to EPOCH x ICA_COMP x WAVELET_COMP\n",
    "    features = features.transpose((1, 0, 2))\n",
    "    return features, params\n",
    "\n",
    "\n",
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"PCA\",\n",
    "    wv_weighting_n_components=3,\n",
    "):\n",
    "    # X has a shape EPOCHS x CHANNELS x TIMEPOINTS\n",
    "    # y has a shape EPOCHS\n",
    "\n",
    "    features, params = vectorize(\n",
    "        X,\n",
    "        y,\n",
    "        mwt,\n",
    "        cwt_density,\n",
    "        ica_n_components,\n",
    "        wv_weighting,\n",
    "        wv_weighting_n_components,\n",
    "    )\n",
    "\n",
    "    # flatten features into shape EPOCH x (ICA_COMP*WAVELET_COMP)\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    # create a classifier from end feature values\n",
    "    # TODO maybe balance class sizes or priors somehow?\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(features, y)\n",
    "\n",
    "    return params, clf\n",
    "\n",
    "\n",
    "def predict(epochs, params, clf, mwt=\"mexh\", cwt_density=2):\n",
    "    features = []\n",
    "    for spatial_filter, wv_weights in params:\n",
    "        # apply spatial filter\n",
    "        filtered = np.tensordot(epochs, spatial_filter, axes=([1], [0]))\n",
    "\n",
    "        cwts = np.array([cwt(epoch, mwt, cwt_density) for epoch in filtered])\n",
    "        # EPOCH x FREQUENCY x TIMEPOINT\n",
    "\n",
    "        one_channel_features = np.tensordot(cwts, wv_weights, axes=([1, 2], [1, 2]))\n",
    "        #  features has a shape EPOCH x WAVELET_COMP\n",
    "        features.append(one_channel_features)\n",
    "\n",
    "    features = np.array(features)\n",
    "    # transform it from shape ICA_COMP x EPOCH x WAVELET_COMP\n",
    "    #                      to EPOCH x ICA_COMP x WAVELET_COMP\n",
    "    features = features.transpose((1, 0, 2))\n",
    "    # flatten feature_values into shape EPOCH x (ICA_COMP*WAVELET_COMP)\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    probs = clf.predict_proba(features)\n",
    "    return probs[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate model for each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def benchmark(epochs, test_on_train_set=False, verbose=False, **hyperparams):\n",
    "    start = time()\n",
    "    if verbose:\n",
    "        print(\"participant            AUROC   err/corr\")\n",
    "    aurocs = []\n",
    "    auroc_sems = []\n",
    "\n",
    "    # group data by participants' ids\n",
    "    grouped = epochs.groupby([\"id\"])\n",
    "    for participant_id in epochs[\"id\"].unique():\n",
    "        participant_df = grouped.get_group(participant_id)\n",
    "\n",
    "        X = np.array(participant_df[\"epoch\"].to_list())\n",
    "\n",
    "        # you can change y set in a easy way ---> y=np.array(participant_df[\"column_name\"].to_list())\n",
    "        y = np.array(participant_df[\"marker\"].to_list())\n",
    "\n",
    "        aurocs_personal = []\n",
    "        # KFold cross-validation\n",
    "        skf = StratifiedKFold(n_splits=4)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            if test_on_train_set:\n",
    "                X_test = X_train\n",
    "                y_test = y_train\n",
    "\n",
    "            # train\n",
    "            params, clf = train(X_train, y_train, **hyperparams)\n",
    "\n",
    "            # test\n",
    "            y_pred = predict(X_test, params, clf, hyperparams[\"mwt\"])\n",
    "\n",
    "            auroc = roc_auc_score(y_test, y_pred)\n",
    "            aurocs_personal.append(auroc)\n",
    "\n",
    "        aurocs.append(np.mean(aurocs_personal))\n",
    "        auroc_sems.append(scipy.stats.sem(aurocs_personal))\n",
    "\n",
    "        if verbose:\n",
    "            error_size = participant_df[\"error_sum\"].iloc[0]\n",
    "            correct_size = participant_df[\"correct_sum\"].iloc[0]\n",
    "\n",
    "            print(\n",
    "                f\"{participant_id:11}    \"\n",
    "                f\"{aurocs[-1]:.3f} ± {auroc_sems[-1]:.3f}    \"\n",
    "                f\"{error_size:3}/{correct_size:3}\"\n",
    "            )\n",
    "\n",
    "    total_sem = sum(np.array(auroc_sems) ** 2) ** (1 / 2) / len(auroc_sems)\n",
    "    mean_auroc = f\"{np.mean(aurocs):.3f} ± {total_sem:.3f}\"\n",
    "    if verbose:\n",
    "        print(f\"\\ntraining time: {(time() - start) / 60:.0f} min\")\n",
    "        print(\"mean AUROC: \" + mean_auroc)\n",
    "    return mean_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"single wavelet choice\")\n",
    "auroc = benchmark(epochs, mwt=\"mexh\", wv_weighting=\"PCA\", verbose=True)\n",
    "print(auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"finding the best number of PCA wavelet components\")\n",
    "for wv_comps in [1, 2, 3, 4, 5, 6]:\n",
    "    auroc = benchmark(\n",
    "        epochs,\n",
    "        mwt=\"mexh\",\n",
    "        wv_weighting=\"PCA\",\n",
    "        wv_weighting_n_components=wv_comps,\n",
    "        ica_n_components=3,\n",
    "    )\n",
    "    print(f\"{wv_comps}   {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"finding the best number of ICA components\")\n",
    "for ica_comps in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "    auroc = benchmark(\n",
    "        epochs,\n",
    "        mwt=\"mexh\",\n",
    "        wv_weighting=\"PCA\",\n",
    "        wv_weighting_n_components=3,\n",
    "        ica_n_components=ica_comps,\n",
    "    )\n",
    "    print(f\"{ica_comps}   {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# px.scatter(y=aurocs)\n",
    "# px.scatter(y=sorted(aurocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One model for all people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def benchmark_common_model(epochs, test_on_train_set=False, **hyperparams):\n",
    "    start = time()\n",
    "    aurocs = []\n",
    "\n",
    "    X = np.array(epochs[\"epoch\"].to_list())\n",
    "    y = np.array(epochs[\"marker\"].to_list())\n",
    "\n",
    "    # KFold cross-validation\n",
    "    # skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        if test_on_train_set:\n",
    "            X_test = X_train\n",
    "            y_test = y_train\n",
    "\n",
    "        # train\n",
    "        params, clf = train(X_train, y_train, **hyperparams)\n",
    "\n",
    "        # test\n",
    "        y_pred = predict(X_test, params, clf, hyperparams[\"mwt\"])\n",
    "\n",
    "        auroc = roc_auc_score(y_test, y_pred)\n",
    "        print(auroc)\n",
    "        aurocs.append(auroc)\n",
    "\n",
    "    sem = scipy.stats.sem(aurocs)\n",
    "    mean_auroc = f\"{np.mean(aurocs):.3f} ± {sem:.3f}\"\n",
    "    print(f\"\\ntraining time: {(time() - start) / 60:.0f} min\")\n",
    "    print(\"mean AUROC: \" + mean_auroc)\n",
    "    return mean_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_common_model(epochs, mwt=\"mexh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ICA stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def correlations(a0, a1):\n",
    "    \"\"\"Find correlation matrix between 2 matrices.\n",
    "    It's similar to np.corrcoef, but it doesn't subtract the mean,\n",
    "    when calculating the sum of squares.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a0, a1 : array_like\n",
    "        2-D arrays containing multiple variables and observations.\n",
    "        Each row represents a variable, and each column a single\n",
    "        observation of all those variables.\n",
    "        Their number of columns must be equal.\n",
    "    \"\"\"\n",
    "    cov = a0 @ a1.T\n",
    "    sum_of_squares0 = np.sum(a0 * a0, axis=1).reshape(-1, 1)\n",
    "    sum_of_squares1 = np.sum(a1 * a1, axis=1).reshape(1, -1)\n",
    "    return cov / (sum_of_squares0 @ sum_of_squares1) ** (1 / 2)\n",
    "\n",
    "\n",
    "def factor_similarity(a0, a1):\n",
    "    \"\"\"Measure how similar are the factors.\n",
    "    Reordering and rescaling them doesn't change the similarity.\n",
    "    \"\"\"\n",
    "    corr = correlations(a0, a1)\n",
    "    sim = abs(corr)  # don't care if factors' sign is flipped\n",
    "    sim = sim.max(axis=0)  # don't care if factors are reordered\n",
    "    return sim.mean()\n",
    "\n",
    "\n",
    "def show_spatial_filters(filters, coefs):\n",
    "    # all interpolation methods in mne.viz.plot_topomap\n",
    "    # give strange artifacts for some reason, so use this instead\n",
    "    x, y, z = channel_locations.T\n",
    "    titles = [f\"{coef:.2f}\" for coef in coefs]\n",
    "\n",
    "    scalp = go.FigureWidget(make_subplots(cols=len(filters), subplot_titles=titles))\n",
    "    scalp.update_layout(**base_layout)\n",
    "    scalp.update_layout(width=200 * len(filters), height=200)\n",
    "    scalp.update_xaxes(showgrid=False)\n",
    "    scalp.update_yaxes(showgrid=False)\n",
    "\n",
    "    for i, filter_ in enumerate(filters):\n",
    "        scalp.add_scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "            mode=\"markers\",\n",
    "            #         mode=\"markers+text\",\n",
    "            text=channel_names,\n",
    "            marker_size=15,\n",
    "            marker_color=-filter_,  # negate, so that red is positive\n",
    "            marker_colorscale=\"RdBu\",\n",
    "        )\n",
    "    return scalp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will ICA find the same factors for one person every time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "id_ = epochs[\"id\"].unique()[7]\n",
    "grouped = epochs.groupby([\"id\"])\n",
    "participant_df = grouped.get_group(id_)\n",
    "X = np.array(participant_df[\"epoch\"].to_list())\n",
    "y = np.array(participant_df[\"marker\"].to_list())\n",
    "\n",
    "clfs = []\n",
    "spatial_filters = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "# skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=0)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # use test sets, because they don't overlap\n",
    "    # so are better to test stability\n",
    "    params, clf = train(X_test, y_test, wv_weighting=\"single\")\n",
    "\n",
    "    single_split_spatial_filters = np.array([filt for filt, _ in params])\n",
    "    spatial_filters.append(single_split_spatial_filters)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"correlations between factors found in the first, and the second split\")\n",
    "correlations(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"similarity measures between factors found in each pair of splits, for a single participant\"\n",
    ")\n",
    "similarities = np.array(\n",
    "    [\n",
    "        [factor_similarity(sf_i, sf_j) for sf_i in spatial_filters]\n",
    "        for sf_j in spatial_filters\n",
    "    ]\n",
    ")\n",
    "print(similarities)\n",
    "print(\"mean\", similarities.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# for clf in clfs:\n",
    "#     print(clf.coef_, clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in range(4):\n",
    "    display(show_spatial_filters(spatial_filters[split], clfs[split].coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will ICA find similar factors for different people?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "clfs = []\n",
    "spatial_filters = []\n",
    "\n",
    "grouped = epochs.groupby([\"id\"])\n",
    "for participant_id in epochs[\"id\"].unique()[:8]:\n",
    "    participant_df = grouped.get_group(participant_id)\n",
    "\n",
    "    X = np.array(participant_df[\"epoch\"].to_list())\n",
    "    y = np.array(participant_df[\"marker\"].to_list())\n",
    "\n",
    "    # train\n",
    "    params, clf = train(X, y, wv_weighting=\"single\")\n",
    "\n",
    "    one_participant_spatial_filters = np.array([filt for filt, _ in params])\n",
    "    spatial_filters.append(one_participant_spatial_filters)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"correlations between factors found for the first, and the second participant\")\n",
    "correlations(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"similarity measures between factors found for each pair of participants\")\n",
    "np.array(\n",
    "    [\n",
    "        [factor_similarity(sf_i, sf_j) for sf_i in spatial_filters]\n",
    "        for sf_j in spatial_filters\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for participant in range(4):\n",
    "    display(\n",
    "        show_spatial_filters(spatial_filters[participant], clfs[participant].coef_[0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mne plotting for comparison\n",
    "# x, y, z = channel_locations.T\n",
    "# mne.viz.plot_topomap(\n",
    "#     spatial_filters[participant][2], np.stack((x, y), axis=-1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = correlations(spatial_filters[1], spatial_filters[7])\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find corresponding components\n",
    "best_similarity = 0\n",
    "for perm in itertools.permutations(range(3)):\n",
    "    perm = list(perm)\n",
    "    diag = corr[perm].diagonal()\n",
    "    similarity = abs(diag).mean()\n",
    "    if similarity > best_similarity:\n",
    "        best_similarity = similarity\n",
    "        best_perm = perm\n",
    "\n",
    "print(best_similarity)\n",
    "print(best_perm)\n",
    "corr[best_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
