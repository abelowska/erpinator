{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "import inspect\n",
    "from time import time\n",
    "\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "from cachier import cachier\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# ignore FastICA did not converge warnings\n",
    "# TODO investigate why doesn't it converge\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Data read into dataframe structure. Each epoch is a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs = pd.read_pickle(pickled_data_filename)\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs = create_df_data(info_filename=info_filename)\n",
    "    epochs.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs.to_pickle(\"../data/\" + epochs.name + \".pkl\")\n",
    "\n",
    "display(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Sort participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped = epochs.groupby(\"id\")\n",
    "epochs[\"error_sum\"] = grouped[[\"marker\"]].transform(lambda x: (x.values == ERROR).sum())\n",
    "epochs[\"correct_sum\"] = grouped[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs = epochs.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")\n",
    "\n",
    "display(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "_mne_epochs = load_epochs_from_file(\"../data/responses/GNG_AA0303-64 el.vhdr\")\n",
    "times = _mne_epochs.times\n",
    "\n",
    "_channel_info = _mne_epochs.info[\"chs\"]\n",
    "channel_locations = np.array([ch[\"loc\"][:3] for ch in _channel_info])\n",
    "channel_names = [ch[\"ch_name\"] for ch in _channel_info]\n",
    "\n",
    "channel_colors = channel_locations - channel_locations.min(axis=0)\n",
    "channel_colors /= channel_colors.max(axis=0)\n",
    "channel_colors = channel_colors * 255 // 1\n",
    "channel_colors = [f\"rgb({c[0]:.0f},{c[1]:.0f},{c[2]:.0f})\" for c in channel_colors]\n",
    "\n",
    "log_freq = np.log2(get_frequencies())  # for plotting CWT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def _numpy_hasher(args, kwargs):\n",
    "    def make_hashable(value):\n",
    "        if type(value) == np.ndarray:\n",
    "            # largest hash to minimize collisions\n",
    "            return xxhash.xxh128_digest(value.tobytes())\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "    bound = inspect.signature(vectorize).bind(*args, **kwargs)\n",
    "    bound.apply_defaults()\n",
    "    key = tuple(\n",
    "        (k, make_hashable(value)) for k, value in sorted(bound.arguments.items())\n",
    "    )\n",
    "    return key\n",
    "\n",
    "\n",
    "@cachier(pickle_reload=False, hash_params=_numpy_hasher)\n",
    "def vectorize(\n",
    "    X,\n",
    "    y,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"single\",\n",
    "    wv_weighting_n_components=3,\n",
    "):\n",
    "    # it returns features of the shape EPOCH x ICA_COMP x WAVELET_COMP\n",
    "    #          and params of the shape ICA_COMP x (SPATIAL_FILTER, WV_WEIGHTS)\n",
    "\n",
    "    # X has a shape EPOCHS x CHANNELS x TIMEPOINTS\n",
    "\n",
    "    # compute ICA\n",
    "    concat = np.concatenate(X, axis=1)\n",
    "    # concat.shape == (num_of_channels, timepoints)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    ica.fit(concat.T)\n",
    "    # ica.components_.shape == (n_components, num_of_channels)\n",
    "\n",
    "    params = []\n",
    "    features = []\n",
    "    for spatial_filter in ica.components_:\n",
    "        # apply ICA\n",
    "        X_filtered = np.tensordot(X, spatial_filter, axes=([1], [0]))\n",
    "        # they have shape EPOCHS x TIMEPOINTS\n",
    "\n",
    "        # apply cwt\n",
    "        X_cwts = np.array([cwt(epoch, mwt, cwt_density) for epoch in X_filtered])\n",
    "        # it has a shape EPOCH x FREQUENCY x TIMEPOINT\n",
    "\n",
    "        X_flattened = X_cwts.reshape(X_cwts.shape[0], -1)\n",
    "        if wv_weighting == \"single\":\n",
    "            # find bets separating wavelet\n",
    "            separations = get_separations(X_cwts[y == ERROR], X_cwts[y == CORRECT])\n",
    "            # separations are shaped FREQUENCY x TIMEPOINT\n",
    "            index = np.unravel_index(separations.argmax(), separations.shape)\n",
    "            wv_weights = np.zeros((1, *separations.shape))\n",
    "            wv_weights[0][index[0]][index[1]] = 1\n",
    "            # 'single' means only one wv_component is found\n",
    "            wv_weighting_n_components = 1\n",
    "        elif wv_weighting == \"PCA\":\n",
    "            pca = PCA(n_components=wv_weighting_n_components)\n",
    "            pca.fit(X_flattened)\n",
    "            wv_weights = pca.components_\n",
    "        elif wv_weighting == \"ICA\":\n",
    "            ica = FastICA(n_components=wv_weighting_n_components, tol=0.001)\n",
    "            ica.fit(X_flattened)\n",
    "            wv_weights = ica.components_\n",
    "        elif wv_weighting == \"LDA\":\n",
    "            lda = LinearDiscriminantAnalysis(n_components=wv_weighting_n_components)\n",
    "            lda.fit(X_flattened, y)\n",
    "            wv_weights = lda.scalings_\n",
    "        else:\n",
    "            raise ValueError(\"wrong wv_choice argument\")\n",
    "\n",
    "        # unflatten wv_weights\n",
    "        cwt_shape = X_cwts.shape[1:]  # FREQUENCY x TIMEPOINT shape\n",
    "        wv_weights = wv_weights.reshape(wv_weighting_n_components, *cwt_shape)\n",
    "        # X_cwts has a shape EPOCH x FREQUENCY x TIMEPOINT\n",
    "        # wv_weights has a shape  WAVELET_COMPONENT x FREQUENCY x TIMEPOINT\n",
    "        one_channel_features = np.tensordot(X_cwts, wv_weights, axes=([1, 2], [1, 2]))\n",
    "        # one_channel_features has a shape EPOCH x WAVELET_COMPONENT\n",
    "\n",
    "        params.append((spatial_filter, wv_weights))\n",
    "        features.append(one_channel_features)\n",
    "\n",
    "    features = np.array(features)\n",
    "    # transform it from shape ICA_COMP x EPOCH x WAVELET_COMP\n",
    "    #                      to EPOCH x ICA_COMP x WAVELET_COMP\n",
    "    features = features.transpose((1, 0, 2))\n",
    "    return features, params\n",
    "\n",
    "\n",
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"single\",\n",
    "    wv_weighting_n_components=3,\n",
    "):\n",
    "    # X has a shape EPOCHS x CHANNELS x TIMEPOINTS\n",
    "    # y has a shape EPOCHS\n",
    "\n",
    "    features, params = vectorize(\n",
    "        X,\n",
    "        y,\n",
    "        mwt,\n",
    "        cwt_density,\n",
    "        ica_n_components,\n",
    "        wv_weighting,\n",
    "        wv_weighting_n_components,\n",
    "    )\n",
    "\n",
    "    # flatten features into shape EPOCH x (ICA_COMP*WAVELET_COMP)\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    # create a classifier from end feature values\n",
    "    # TODO maybe balance class sizes or priors somehow?\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(features, y)\n",
    "\n",
    "    return params, clf\n",
    "\n",
    "\n",
    "def predict(epochs, params, clf, mwt=\"mexh\", cwt_density=2):\n",
    "    features = []\n",
    "    for spatial_filter, wv_weights in params:\n",
    "        # apply spatial filter\n",
    "        filtered = np.tensordot(epochs, spatial_filter, axes=([1], [0]))\n",
    "\n",
    "        cwts = np.array([cwt(epoch, mwt, cwt_density) for epoch in filtered])\n",
    "        # EPOCH x FREQUENCY x TIMEPOINT\n",
    "\n",
    "        one_channel_features = np.tensordot(cwts, wv_weights, axes=([1, 2], [1, 2]))\n",
    "        #  features has a shape EPOCH x WAVELET_COMP\n",
    "        features.append(one_channel_features)\n",
    "\n",
    "    features = np.array(features)\n",
    "    # transform it from shape ICA_COMP x EPOCH x WAVELET_COMP\n",
    "    #                      to EPOCH x ICA_COMP x WAVELET_COMP\n",
    "    features = features.transpose((1, 0, 2))\n",
    "    # flatten feature_values into shape EPOCH x (ICA_COMP*WAVELET_COMP)\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    probs = clf.predict_proba(features)\n",
    "    return probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def benchmark(epochs, test_on_train_set=False, verbose=False, **hyperparams):\n",
    "    start = time()\n",
    "    if verbose:\n",
    "        print(\"participant            AUROC   err/corr\")\n",
    "    aurocs = []\n",
    "    auroc_sems = []\n",
    "\n",
    "    # group data by participants' ids\n",
    "    grouped = epochs.groupby([\"id\"])\n",
    "    for participant_id in epochs[\"id\"].unique():\n",
    "        participant_df = grouped.get_group(participant_id)\n",
    "\n",
    "        X = np.array(participant_df[\"epoch\"].to_list())\n",
    "\n",
    "        # you can change y set in a easy way ---> y=np.array(participant_df[\"column_name\"].to_list())\n",
    "        y = np.array(participant_df[\"marker\"].to_list())\n",
    "\n",
    "        aurocs_personal = []\n",
    "        # KFold cross-validation\n",
    "        skf = StratifiedKFold(n_splits=4)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            if test_on_train_set:\n",
    "                X_test = X_train\n",
    "                y_test = y_train\n",
    "\n",
    "            # train\n",
    "            params, clf = train(X_train, y_train, **hyperparams)\n",
    "\n",
    "            # test\n",
    "            y_pred = predict(X_test, params, clf, hyperparams[\"mwt\"])\n",
    "\n",
    "            auroc = roc_auc_score(y_test, y_pred)\n",
    "            aurocs_personal.append(auroc)\n",
    "\n",
    "        aurocs.append(np.mean(aurocs_personal))\n",
    "        auroc_sems.append(scipy.stats.sem(aurocs_personal))\n",
    "\n",
    "        if verbose:\n",
    "            error_size = participant_df[\"error_sum\"].iloc[0]\n",
    "            correct_size = participant_df[\"correct_sum\"].iloc[0]\n",
    "\n",
    "            print(\n",
    "                f\"{participant_id:11}    \"\n",
    "                f\"{aurocs[-1]:.3f} ± {auroc_sems[-1]:.3f}    \"\n",
    "                f\"{error_size:3}/{correct_size:3}\"\n",
    "            )\n",
    "\n",
    "    total_sem = sum(np.array(auroc_sems) ** 2) ** (1 / 2) / len(auroc_sems)\n",
    "    mean_auroc = f\"{np.mean(aurocs):.3f} ± {total_sem:.3f}\"\n",
    "    if verbose:\n",
    "        print(f\"\\ntraining time: {(time() - start) / 60:.0f} min\")\n",
    "        print(\"mean AUROC: \" + mean_auroc)\n",
    "    return mean_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"single wavelet choice\")\n",
    "auroc = benchmark(epochs, mwt=\"mexh\", wv_weighting=\"PCA\", verbose=True)\n",
    "print(auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"finding the best number of PCA wavelet components\")\n",
    "for wv_comps in [1, 2, 3, 4, 5, 6]:\n",
    "    auroc = benchmark(\n",
    "        epochs,\n",
    "        mwt=\"mexh\",\n",
    "        wv_weighting=\"PCA\",\n",
    "        wv_weighting_n_components=wv_comps,\n",
    "        ica_n_components=3,\n",
    "    )\n",
    "    print(f\"{wv_comps}   {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"finding the best number of ICA components\")\n",
    "for ica_comps in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "    auroc = benchmark(\n",
    "        epochs,\n",
    "        mwt=\"mexh\",\n",
    "        wv_weighting=\"PCA\",\n",
    "        wv_weighting_n_components=3,\n",
    "        ica_n_components=ica_comps,\n",
    "    )\n",
    "    print(f\"{ica_comps}   {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# px.scatter(y=aurocs)\n",
    "# px.scatter(y=sorted(aurocs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
