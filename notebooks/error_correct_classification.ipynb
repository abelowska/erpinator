{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "import inspect\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "from cachier import cachier\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "from utils import *\n",
    "from architecture import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# ignore FastICA did not converge warnings\n",
    "# TODO investigate why doesn't it converge\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "#### Data read into dataframe structure. Each epoch is a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs = pd.read_pickle(pickled_data_filename)\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs = create_df_data(info_filename=info_filename)\n",
    "    epochs.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs.to_pickle(\"../data/\" + epochs.name + \".pkl\")\n",
    "\n",
    "# epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "#### Sort participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped = epochs.groupby(\"id\")\n",
    "epochs[\"error_sum\"] = grouped[[\"marker\"]].transform(lambda x: (x.values == ERROR).sum())\n",
    "epochs[\"correct_sum\"] = grouped[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs = epochs.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")\n",
    "# epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "#### Get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "_mne_epochs = load_epochs_from_file(\"../data/responses/GNG_AA0303-64 el.vhdr\")\n",
    "times = _mne_epochs.times\n",
    "\n",
    "_channel_info = _mne_epochs.info[\"chs\"]\n",
    "channel_locations = np.array([ch[\"loc\"][:3] for ch in _channel_info])\n",
    "channel_names = [ch[\"ch_name\"] for ch in _channel_info]\n",
    "\n",
    "channel_colors = channel_locations - channel_locations.min(axis=0)\n",
    "channel_colors /= channel_colors.max(axis=0)\n",
    "channel_colors = channel_colors * 255 // 1\n",
    "channel_colors = [f\"rgb({c[0]:.0f},{c[1]:.0f},{c[2]:.0f})\" for c in channel_colors]\n",
    "\n",
    "log_freq = np.log2(get_frequencies())  # for plotting CWT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = \"/home/filip/.erpinator_cache\"\n",
    "\n",
    "steps = steps_parallel_pca\n",
    "# steps = steps[:-1] + [(\"lda\", LinearDiscriminantAnalysis())]\n",
    "# steps = steps[:-1] + [(\"knr\", KNeighborsRegressor())]\n",
    "steps = steps[:-1] + [(\"lasso\", Lasso())]\n",
    "\n",
    "regressor_params = dict(\n",
    "    ica__n_components=[3],\n",
    "    cwt__mwt=[\"mexh\"],\n",
    "    pca__n_components=[3],\n",
    "    # featurize__power__cwt__mwt=[\"cmor0.5-1\"],\n",
    "    # featurize__power__pca__n_components=[3],\n",
    "    # featurize__shape__cwt__mwt=[\"mexh\"],\n",
    "    # featurize__shape__pca__n_components=[3],\n",
    "    #     svr__C=[0.1],\n",
    "    #     knr__n_neighbors=[11],\n",
    "    lasso__alpha=[0.2, 0.5, 1],\n",
    ")\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate model for each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipeline = Pipeline(steps, memory=cachedir)\n",
    "pipeline.set_params(**ParameterGrid(regressor_params)[0])\n",
    "\n",
    "print(\"participant            AUROC   err/corr\")\n",
    "aurocs = []\n",
    "auroc_sems = []\n",
    "\n",
    "# group data by participants' ids\n",
    "grouped = epochs.groupby([\"id\"])\n",
    "for participant_id in epochs[\"id\"].unique():\n",
    "    participant_df = grouped.get_group(participant_id)\n",
    "\n",
    "    X = np.array(participant_df[\"epoch\"].to_list())\n",
    "    y = np.array(participant_df[\"marker\"].to_list())\n",
    "\n",
    "    aurocs_personal = []\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        if type(steps[-1][1]) == LinearDiscriminantAnalysis:\n",
    "            y_pred = pipeline.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "        # r2 = r2_score(y_test, y_pred)\n",
    "        auroc = roc_auc_score(y_test, y_pred)\n",
    "        aurocs_personal.append(auroc)\n",
    "\n",
    "    aurocs.append(np.mean(aurocs_personal))\n",
    "    auroc_sems.append(scipy.stats.sem(aurocs_personal))\n",
    "\n",
    "    error_size = participant_df[\"error_sum\"].iloc[0]\n",
    "    correct_size = participant_df[\"correct_sum\"].iloc[0]\n",
    "    print(\n",
    "        f\"{participant_id:11}    \"\n",
    "        f\"{aurocs[-1]:.3f} ± {auroc_sems[-1]:.3f}    \"\n",
    "        f\"{error_size:3}/{correct_size:3}\"\n",
    "    )\n",
    "\n",
    "total_sem = sum(np.array(auroc_sems) ** 2) ** (1 / 2) / len(auroc_sems)\n",
    "mean_auroc = f\"{np.mean(aurocs):.3f} ± {total_sem:.3f}\"\n",
    "print(\"mean AUROC: \" + mean_auroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One model for all people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_gridsearch(steps, cv, regressor_params, memory):\n",
    "    pipeline = Pipeline(steps, memory=memory)\n",
    "    print(\" \" * 133 + \"corr           r2\")\n",
    "\n",
    "    # get params randomly\n",
    "    all_params = list(ParameterGrid(regressor_params))\n",
    "    # shuffle(all_params)\n",
    "\n",
    "    for params in all_params:\n",
    "        pipeline.set_params(**params)\n",
    "\n",
    "        scores = []\n",
    "        kf = KFold(n_splits=cv)\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            if type(steps[-1][1]) == LinearDiscriminantAnalysis:\n",
    "                y_pred = pipeline.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_pred = pipeline.predict(X_test)\n",
    "            corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            auroc = roc_auc_score(y_test, y_pred)  # it's different in classification!\n",
    "\n",
    "            scores.append([corr, r2, auroc])\n",
    "            print(corr, r2, auroc)\n",
    "\n",
    "        # print scores\n",
    "        print(f\"{str(params):126}\", end=\" \")\n",
    "        means = np.mean(scores, axis=0)\n",
    "        sems = scipy.stats.sem(scores, axis=0)\n",
    "        for mean, sem in zip(means, sems):\n",
    "            print(f\"{mean:5.2f}±{sem:4.2f}\", end=\"   \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(epochs[\"epoch\"].to_list())\n",
    "y = np.array(epochs[\"marker\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "custom_gridsearch(steps, cv=5, regressor_params=regressor_params, memory=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Testing ICA stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def correlations(a0, a1):\n",
    "    \"\"\"Find correlation matrix between 2 matrices.\n",
    "    It's similar to np.corrcoef, but it doesn't subtract the mean,\n",
    "    when calculating the sum of squares.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a0, a1 : array_like\n",
    "        2-D arrays containing multiple variables and observations.\n",
    "        Each row represents a variable, and each column a single\n",
    "        observation of all those variables.\n",
    "        Their number of columns must be equal.\n",
    "    \"\"\"\n",
    "    cov = a0 @ a1.T\n",
    "    sum_of_squares0 = np.sum(a0 * a0, axis=1).reshape(-1, 1)\n",
    "    sum_of_squares1 = np.sum(a1 * a1, axis=1).reshape(1, -1)\n",
    "    return cov / (sum_of_squares0 @ sum_of_squares1) ** (1 / 2)\n",
    "\n",
    "\n",
    "def factor_similarity(a0, a1):\n",
    "    \"\"\"Measure how similar are the factors.\n",
    "    Reordering and rescaling them doesn't change the similarity.\n",
    "    \"\"\"\n",
    "    corr = correlations(a0, a1)\n",
    "    sim = abs(corr)  # don't care if factors' sign is flipped\n",
    "    sim = sim.max(axis=0)  # don't care if factors are reordered\n",
    "    return sim.mean()\n",
    "\n",
    "\n",
    "def show_spatial_filters(filters, coefs):\n",
    "    # all interpolation methods in mne.viz.plot_topomap\n",
    "    # give strange artifacts for some reason, so use this instead\n",
    "    x, y, z = channel_locations.T\n",
    "    titles = [f\"{coef:.2f}\" for coef in coefs]\n",
    "\n",
    "    scalp = go.FigureWidget(make_subplots(cols=len(filters), subplot_titles=titles))\n",
    "    scalp.update_layout(**base_layout)\n",
    "    scalp.update_layout(width=200 * len(filters), height=200)\n",
    "    scalp.update_xaxes(showgrid=False)\n",
    "    scalp.update_yaxes(showgrid=False)\n",
    "\n",
    "    for i, filter_ in enumerate(filters):\n",
    "        scalp.add_scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "            mode=\"markers\",\n",
    "            #         mode=\"markers+text\",\n",
    "            text=channel_names,\n",
    "            marker_size=15,\n",
    "            marker_color=-filter_,  # negate, so that red is positive\n",
    "            marker_colorscale=\"RdBu\",\n",
    "        )\n",
    "    return scalp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will ICA find the same factors for one person every time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "id_ = epochs[\"id\"].unique()[7]\n",
    "grouped = epochs.groupby([\"id\"])\n",
    "participant_df = grouped.get_group(id_)\n",
    "X = np.array(participant_df[\"epoch\"].to_list())\n",
    "y = np.array(participant_df[\"marker\"].to_list())\n",
    "\n",
    "clfs = []\n",
    "spatial_filters = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "# skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=0)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # use test sets, because they don't overlap\n",
    "    # so are better to test stability\n",
    "    params, clf = train(X_test, y_test, wv_weighting=\"single\")\n",
    "\n",
    "    single_split_spatial_filters = np.array([filt for filt, _ in params])\n",
    "    spatial_filters.append(single_split_spatial_filters)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"correlations between factors found in the first, and the second split\")\n",
    "correlations(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"similarity measures between factors found in each pair of splits, for a single participant\"\n",
    ")\n",
    "similarities = np.array(\n",
    "    [\n",
    "        [factor_similarity(sf_i, sf_j) for sf_i in spatial_filters]\n",
    "        for sf_j in spatial_filters\n",
    "    ]\n",
    ")\n",
    "print(similarities)\n",
    "print(\"mean\", similarities.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# for clf in clfs:\n",
    "#     print(clf.coef_, clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in range(4):\n",
    "    display(show_spatial_filters(spatial_filters[split], clfs[split].coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will ICA find similar factors for different people?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "clfs = []\n",
    "spatial_filters = []\n",
    "\n",
    "grouped = epochs.groupby([\"id\"])\n",
    "for participant_id in epochs[\"id\"].unique()[:8]:\n",
    "    participant_df = grouped.get_group(participant_id)\n",
    "\n",
    "    X = np.array(participant_df[\"epoch\"].to_list())\n",
    "    y = np.array(participant_df[\"marker\"].to_list())\n",
    "\n",
    "    # train\n",
    "    params, clf = train(X, y, wv_weighting=\"single\")\n",
    "\n",
    "    one_participant_spatial_filters = np.array([filt for filt, _ in params])\n",
    "    spatial_filters.append(one_participant_spatial_filters)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"correlations between factors found for the first, and the second participant\")\n",
    "correlations(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"similarity measures between factors found for each pair of participants\")\n",
    "np.array(\n",
    "    [\n",
    "        [factor_similarity(sf_i, sf_j) for sf_i in spatial_filters]\n",
    "        for sf_j in spatial_filters\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for participant in range(4):\n",
    "    display(\n",
    "        show_spatial_filters(spatial_filters[participant], clfs[participant].coef_[0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mne plotting for comparison\n",
    "# x, y, z = channel_locations.T\n",
    "# mne.viz.plot_topomap(\n",
    "#     spatial_filters[participant][2], np.stack((x, y), axis=-1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = correlations(spatial_filters[1], spatial_filters[7])\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find corresponding components\n",
    "best_similarity = 0\n",
    "for perm in itertools.permutations(range(3)):\n",
    "    perm = list(perm)\n",
    "    diag = corr[perm].diagonal()\n",
    "    similarity = abs(diag).mean()\n",
    "    if similarity > best_similarity:\n",
    "        best_similarity = similarity\n",
    "        best_perm = perm\n",
    "\n",
    "print(best_similarity)\n",
    "print(best_perm)\n",
    "corr[best_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
