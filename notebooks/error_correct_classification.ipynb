{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_loc = channel_locations.T[0]\n",
    "# y_loc = channel_locations.T[1]\n",
    "# mne.viz.plot_topomap(ica.components_[0], np.stack((x_loc, y_loc), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "import inspect\n",
    "import itertools\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "from cachier import cachier\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from ipywidgets import HBox, VBox\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "\n",
    "from utils import *\n",
    "from architecture import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# ignore FastICA did not converge warnings\n",
    "# TODO investigate why doesn't it converge\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "#### Data read into dataframe structure. Each epoch is a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs = pd.read_pickle(pickled_data_filename)\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs = create_df_data(info_filename=info_filename)\n",
    "    epochs.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs.to_pickle(\"../data/\" + epochs.name + \".pkl\")\n",
    "\n",
    "# epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "#### Sort participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped = epochs.groupby(\"id\")\n",
    "epochs[\"error_sum\"] = grouped[[\"marker\"]].transform(lambda x: (x.values == ERROR).sum())\n",
    "epochs[\"correct_sum\"] = grouped[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs = epochs.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")\n",
    "# epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "#### Get metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "_mne_epochs = load_epochs_from_file(\"../data/responses/GNG_AA0303-64 el.vhdr\")\n",
    "times = _mne_epochs.times\n",
    "\n",
    "_channel_info = _mne_epochs.info[\"chs\"]\n",
    "channel_locations = np.array([ch[\"loc\"][:3] for ch in _channel_info])\n",
    "channel_names = [ch[\"ch_name\"] for ch in _channel_info]\n",
    "\n",
    "channel_colors = channel_locations - channel_locations.min(axis=0)\n",
    "channel_colors /= channel_colors.max(axis=0)\n",
    "channel_colors = channel_colors * 255 // 1\n",
    "channel_colors = [f\"rgb({c[0]:.0f},{c[1]:.0f},{c[2]:.0f})\" for c in channel_colors]\n",
    "\n",
    "log_freq = np.log2(get_frequencies())  # for plotting CWT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = \"/home/filip/.erpinator_cache\"\n",
    "\n",
    "# steps = steps_simple\n",
    "steps = steps_parallel_pca\n",
    "# StandardScaler doesn't seem to change anything for LDA\n",
    "steps = steps[:-2] + [(\"lda\", LinearDiscriminantAnalysis())]\n",
    "# steps = steps[:-1] + [(\"knr\", KNeighborsRegressor())]\n",
    "# steps = steps[:-1] + [(\"lasso\", Lasso())]\n",
    "\n",
    "steps.pop(3)  # remove CWT\n",
    "# steps.pop(3)  # remove PCA\n",
    "\n",
    "steps[1] = (\"ica\", PCA(random_state=0))  # replace ICA with PCA\n",
    "\n",
    "regressor_params = dict(\n",
    "    ica__n_components=[2],\n",
    "    #     cwt__mwt=[\"morl\"],\n",
    "    #     cwt__octaves=[4],\n",
    "    pca__n_components=[3],\n",
    "    # featurize__power__cwt__mwt=[\"cmor0.5-1\"],\n",
    "    # featurize__power__pca__n_components=[3],\n",
    "    # featurize__shape__cwt__mwt=[\"mexh\"],\n",
    "    # featurize__shape__pca__n_components=[3],\n",
    "    #     svr__C=[0.1],\n",
    "    #     knr__n_neighbors=[11],\n",
    "    #     lasso__alpha=[0.2, 0.5, 1],\n",
    "    lda__solver=[\"lsqr\"],  # to turn off scaling, to simplify visualizing\n",
    ")\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate model for each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "print(\"participant            AUROC   err/corr\")\n",
    "aurocs = []\n",
    "auroc_sems = []\n",
    "pipelines = []\n",
    "\n",
    "# group data by participants' ids\n",
    "grouped = epochs.groupby([\"id\"])\n",
    "for participant_id in epochs[\"id\"].unique():\n",
    "    participant_df = grouped.get_group(participant_id)\n",
    "\n",
    "    X = np.array(participant_df[\"epoch\"].to_list())\n",
    "    y = np.array(participant_df[\"marker\"].to_list())\n",
    "\n",
    "    pipeline = Pipeline(deepcopy(steps), memory=cachedir)\n",
    "    pipeline.set_params(**ParameterGrid(regressor_params)[0])\n",
    "    \n",
    "    aurocs_personal = []\n",
    "    pipelines_personal = []\n",
    "    skf = StratifiedKFold(n_splits=2)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        if type(steps[-1][1]) == LinearDiscriminantAnalysis:\n",
    "            y_pred = pipeline.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "        # r2 = r2_score(y_test, y_pred)\n",
    "        auroc = roc_auc_score(y_test, y_pred)\n",
    "        aurocs_personal.append(auroc)\n",
    "        pipelines_personal.append(pipeline)\n",
    "\n",
    "    aurocs.append(np.mean(aurocs_personal))\n",
    "    auroc_sems.append(scipy.stats.sem(aurocs_personal))\n",
    "    pipelines.append(pipelines_personal)\n",
    "\n",
    "    error_size = participant_df[\"error_sum\"].iloc[0]\n",
    "    correct_size = participant_df[\"correct_sum\"].iloc[0]\n",
    "    print(\n",
    "        f\"{participant_id:11}    \"\n",
    "        f\"{aurocs[-1]:.3f} ± {auroc_sems[-1]:.3f}    \"\n",
    "        f\"{error_size:3}/{correct_size:3}\"\n",
    "    )\n",
    "\n",
    "total_sem = sum(np.array(auroc_sems) ** 2) ** (1 / 2) / len(auroc_sems)\n",
    "mean_auroc = f\"{np.mean(aurocs):.3f} ± {total_sem:.3f}\"\n",
    "print(\"mean AUROC: \" + mean_auroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One model for all people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_gridsearch(steps, cv, regressor_params, memory):\n",
    "    print(\"AUROC   corr     r2\")\n",
    "\n",
    "    # get params randomly\n",
    "    all_params = list(ParameterGrid(regressor_params))\n",
    "    # shuffle(all_params)\n",
    "\n",
    "    for params in all_params:\n",
    "        pipelines = []\n",
    "        scores = []\n",
    "        kf = KFold(n_splits=cv)\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            pipeline = Pipeline(deepcopy(steps), memory=memory)\n",
    "            pipeline.set_params(**params)\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            if type(steps[-1][1]) == LinearDiscriminantAnalysis:\n",
    "                y_pred = pipeline.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_pred = pipeline.predict(X_test)\n",
    "            corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            auroc = roc_auc_score(y_test, y_pred)  # it's different in classification!\n",
    "\n",
    "            scores.append([auroc, corr, r2])\n",
    "            print(f\"{auroc:.3f}  {corr:.3f}  {r2:.3f}\")\n",
    "\n",
    "            pipelines.append(pipeline)\n",
    "\n",
    "        # print scores\n",
    "        print(f\"{str(params):126}\")\n",
    "        means = np.mean(scores, axis=0)\n",
    "        sems = scipy.stats.sem(scores, axis=0)\n",
    "        for mean, sem in zip(means, sems):\n",
    "            print(f\"{mean:5.2f}±{sem:4.2f}\", end=\"   \")\n",
    "        print()\n",
    "\n",
    "    # note that it returns pipelines only for last parameters in the grid\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(epochs[\"epoch\"].to_list())  # [::20]\n",
    "y = np.array(epochs[\"marker\"].to_list())  # [::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipelines = custom_gridsearch(steps, cv=2, regressor_params=regressor_params, memory=cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "scale = 0.6\n",
    "\n",
    "\n",
    "def plot_ica_comp(ica_comp):\n",
    "    x_loc, y_loc, z_loc = channel_locations.T\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(**base_layout)\n",
    "    fig.update_layout(width=350 * scale, height=350 * scale)\n",
    "\n",
    "    # sort by z_loc for prettier printing\n",
    "    info = list(zip(z_loc, x_loc, y_loc, channel_names, ica_comp))\n",
    "    info.sort()\n",
    "    _, _x_loc, _y_loc, _channel_names, _component = zip(*info)\n",
    "\n",
    "    fig.add_scatter(\n",
    "        x=_x_loc,\n",
    "        y=_y_loc,\n",
    "        text=_channel_names,\n",
    "        marker_color=_component,\n",
    "        mode=\"markers\",\n",
    "        marker_size=42 * scale,\n",
    "        marker_colorscale=blue_black_red,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "# def plot_pca_comps_on_cwt(pca_comps):\n",
    "#     amplitude = 0.1\n",
    "#     fig = go.FigureWidget(make_subplots(rows=len(pca_comps)))\n",
    "#     fig.update_layout(**base_layout)\n",
    "#     fig.update_layout(height=350, width=600)\n",
    "#     fig.update_xaxes(visible=False)\n",
    "#     for i, comp in enumerate(pca_comps):\n",
    "#         comp = comp.reshape(-1, timepoints_count)\n",
    "#         fig.add_heatmap(\n",
    "#             z=comp,\n",
    "#             x=times,\n",
    "#             row=i + 1,\n",
    "#             col=1,\n",
    "#             zmin=-amplitude,\n",
    "#             zmax=amplitude,\n",
    "#             y=log_freq,\n",
    "#             colorscale=blue_black_red,\n",
    "#         )\n",
    "#     return fig\n",
    "\n",
    "\n",
    "def plot_pca_shape(pca_comps, mwt, clf_coefs):\n",
    "    # CWT+PCA in practice, multiplies by this shape\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(**base_layout)\n",
    "    fig.update_layout(height=350 * scale, width=600 * scale)\n",
    "    for i, comp in enumerate(pca_comps):\n",
    "        if mwt is not None:\n",
    "            # TODO test this block, if this is really the shape\n",
    "            comp = comp.reshape(-1, timepoints_count)\n",
    "            acc = np.zeros_like(times)\n",
    "            for amps_for_freq, freq in zip(comp, get_frequencies()):\n",
    "                for amp, latency in zip(amps_for_freq, times):\n",
    "                    wv = get_wavelet(latency, freq, times, mwt)\n",
    "                    acc += wv * amp\n",
    "        else:\n",
    "            acc = np.copy(comp)\n",
    "\n",
    "        # weight by the component importance from LDA\n",
    "        acc *= clf_coefs[i]\n",
    "        #         print(clf_coefs[i])\n",
    "        fig.add_scatter(x=times, y=acc)\n",
    "\n",
    "    # show also the sum of all pca comps weighted by importance\n",
    "    acc = np.zeros_like(times)\n",
    "    for comp, coef in zip(pca_comps, clf_coefs):\n",
    "        acc += comp * coef\n",
    "    fig.add_scatter(x=times, y=acc, line_width=5, line_color=\"yellow\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing for steps with separate PCA for each ICA component\n",
    "\n",
    "split_index = 0\n",
    "fitted_steps = dict(pipelines[split_index].steps)\n",
    "ica = fitted_steps[\"ica\"]\n",
    "pcas = fitted_steps[\"pca\"].PCAs\n",
    "lda = fitted_steps[\"lda\"]\n",
    "clf_coefs_for_each_ica_comp = lda.coef_[0].reshape(len(ica.components_), -1)\n",
    "# clf_coefs_for_each_ica_comp = lda.coef_[0].reshape(-1, len(ica.components_)).T   # this was tested visually to be the wrong unflattening\n",
    "\n",
    "\n",
    "for ica_comp_num, ica_comp in enumerate(ica.components_):\n",
    "    pca_comps = pcas[ica_comp_num].components_\n",
    "    clf_coefs = clf_coefs_for_each_ica_comp[ica_comp_num]\n",
    "    if \"cwt\" in fitted_steps:\n",
    "        mwt = fitted_steps[\"cwt\"].mwt\n",
    "    else:\n",
    "        mwt = None\n",
    "\n",
    "    display(HBox([plot_ica_comp(ica_comp), plot_pca_shape(pca_comps, mwt, clf_coefs)]))\n",
    "    # display(HBox([plot_ica_comp(ica_comp), plot_pca_comps_on_cwt(pca_comps)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# visualizing for steps with only one PCA\n",
    "\n",
    "split_index = 0\n",
    "fitted_steps = dict(pipelines[split_index].steps)\n",
    "ica = fitted_steps[\"ica\"]\n",
    "pca = fitted_steps[\"pca\"]\n",
    "lda = fitted_steps[\"lda\"]\n",
    "\n",
    "pca_comps_separated = pca.components_.reshape(\n",
    "    len(pca.components_), len(ica.components_), -1\n",
    ")\n",
    "\n",
    "for ica_comp_num, ica_comp in enumerate(ica.components_):\n",
    "    pca_comps = pca_comps_separated[:, ica_comp_num, :]\n",
    "    clf_coefs = lda.coef_[0]\n",
    "    if \"cwt\" in fitted_steps:\n",
    "        mwt = fitted_steps[\"cwt\"].mwt\n",
    "    else:\n",
    "        mwt = None\n",
    "\n",
    "    display(HBox([plot_ica_comp(ica_comp), plot_pca_shape(pca_comps, mwt, clf_coefs)]))\n",
    "    # display(HBox([plot_ica_comp(ica_comp), plot_pca_comps_on_cwt(pca_comps)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that PCA+LDA can be replaced by just a dot product with a shape computed from weighted PCA comps\n",
    "\n",
    "split_index = 0\n",
    "fitted_steps = dict(pipelines[split_index].steps)\n",
    "pcas = fitted_steps[\"pca\"].PCAs\n",
    "ica_comp_num = 0\n",
    "pca_comps = pcas[ica_comp_num].components_\n",
    "lda = fitted_steps[\"lda\"]\n",
    "\n",
    "# do ICA steps\n",
    "X_ = pipelines[0].steps[0][1].transform(X)\n",
    "X_ = pipelines[0].steps[1][1].transform(X_)\n",
    "X_ = pipelines[0].steps[2][1].transform(X_)\n",
    "\n",
    "# compute a shape from the weighted PCA comps\n",
    "acc = np.zeros_like(times)\n",
    "for comp, coef in zip(pca_comps, lda.coef_[0]):\n",
    "    acc += comp * coef\n",
    "\n",
    "features = []\n",
    "for epoch in X_[0]:\n",
    "    feature = np.sum(acc * epoch)\n",
    "    features.append(feature)\n",
    "features = np.array(features).reshape(-1)\n",
    "\n",
    "real_features = pipelines[split_index].decision_function(X)\n",
    "\n",
    "corr = np.corrcoef(features, real_features)[0][1]\n",
    "np.isclose(corr, 1)\n",
    "# if True, the dot product gives the same as the normal pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ICA stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv should be 2 !\n",
    "# otherwise the components will be stable, but trivially\n",
    "#  - they are trained on overlapping data, so no wonder they are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def correlations(a0, a1):\n",
    "    \"\"\"Find correlation matrix between 2 matrices.\n",
    "    It's similar to np.corrcoef, but it doesn't subtract the mean,\n",
    "    when calculating the sum of squares.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a0, a1 : array_like\n",
    "        2-D arrays containing multiple variables and observations.\n",
    "        Each row represents a variable, and each column a single\n",
    "        observation of all those variables.\n",
    "        Their number of columns must be equal.\n",
    "    \"\"\"\n",
    "    cov = a0 @ a1.T\n",
    "    sum_of_squares0 = np.sum(a0 * a0, axis=1).reshape(-1, 1)\n",
    "    sum_of_squares1 = np.sum(a1 * a1, axis=1).reshape(1, -1)\n",
    "    return cov / (sum_of_squares0 @ sum_of_squares1) ** (1 / 2)\n",
    "\n",
    "\n",
    "def factor_similarity(a0, a1):\n",
    "    \"\"\"Measure how similar are the factors.\n",
    "    Reordering and rescaling them doesn't change the similarity.\n",
    "    \"\"\"\n",
    "    corr = correlations(a0, a1)\n",
    "    sim = abs(corr)  # don't care if factors' sign is flipped\n",
    "    sim_hor = sim.max(axis=0)  # don't care if factors are reordered\n",
    "    sim_ver = sim.max(axis=1)\n",
    "    # in case some row or comuln have two candidates, choose the more pessimistic axis\n",
    "    mean_sim = min(sim_hor.mean(), sim_ver.mean())\n",
    "    # TODO? a more robust way would be to generate permutations and chack them\n",
    "    return mean_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filters = [pipeline.steps[1][1].components_ for pipeline in pipelines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"correlations between factors found in the first, and the second split\")\n",
    "correlations(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_similarity(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"similarity measures between factors found in each pair of splits, for a single participant\"\n",
    "# )\n",
    "# similarities = np.array(\n",
    "#     [\n",
    "#         [factor_similarity(sf_i, sf_j) for sf_i in spatial_filters]\n",
    "#         for sf_j in spatial_filters\n",
    "#     ]\n",
    "# )\n",
    "# print(similarities)\n",
    "# print(\"mean\", similarities.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mne plotting for comparison\n",
    "# x, y, z = channel_locations.T\n",
    "# mne.viz.plot_topomap(\n",
    "#     spatial_filters[participant][2], np.stack((x, y), axis=-1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try to find corresponding components\n",
    "# best_similarity = 0\n",
    "# for perm in itertools.permutations(range(3)):\n",
    "#     perm = list(perm)\n",
    "#     diag = corr[perm].diagonal()\n",
    "#     similarity = abs(diag).mean()\n",
    "#     if similarity > best_similarity:\n",
    "#         best_similarity = similarity\n",
    "#         best_perm = perm\n",
    "\n",
    "# print(best_similarity)\n",
    "# print(best_perm)\n",
    "# corr[best_perm]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
