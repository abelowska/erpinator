{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "from utils import load_gonogo_responses, tmax, tmin\n",
    "\n",
    "# plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing epoched data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = load_gonogo_responses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.plot(\n",
    "    n_epochs=1,\n",
    "    event_colors={0: \"g\", 1: \"m\"},\n",
    ")\n",
    "None  # prevents doubled output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_response_epochs = epochs[\"correct_response\"]\n",
    "error_response_epochs = epochs[\"error_response\"]\n",
    "\n",
    "\n",
    "# Calculate averages of events sets\n",
    "correct_response_evoked = correct_response_epochs.average()\n",
    "error_response_evoked = error_response_epochs.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages of two event sets\n",
    "\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    dict(\n",
    "        correct_response=correct_response_evoked, error_response=error_response_evoked\n",
    "    ),\n",
    "    legend=\"upper left\",\n",
    "    show_sensors=\"upper right\",\n",
    "    ylim=dict(eeg=[-10, 10]),\n",
    "    invert_y=True,\n",
    "    combine=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages of error response events per channel\n",
    "\n",
    "error_response_evoked.plot_joint(picks=\"eeg\")\n",
    "error_response_evoked.plot_topomap(times=[0.0, 0.08, 0.1, 0.12, 0.2], ch_type=\"eeg\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages of merged event sets (diff between error and correct) per channel\n",
    "\n",
    "evoked_diff = mne.combine_evoked(\n",
    "    [correct_response_evoked, error_response_evoked], weights=[1, -1]\n",
    ")\n",
    "evoked_diff.plot_joint()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_mean_dict = {}\n",
    "\n",
    "for key in epochs.event_id.keys():\n",
    "    mean_key = key + \"_mean\"\n",
    "    events_mean_dict[mean_key] = epochs[key]._data.mean(axis=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart with averages of correct and error responses per channel\n",
    "\n",
    "colors = [\"b\", \"r\", \"g\"]\n",
    "color_iterator = 0\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "for key in events_mean_dict:\n",
    "    epoch = events_mean_dict[key]\n",
    "    plt.plot(\n",
    "        epoch.T + np.arange(start=1e-6, step=10e-6, stop=301e-7),\n",
    "        label=key,\n",
    "        color=colors[color_iterator],\n",
    "    )\n",
    "    color_iterator = color_iterator + 1\n",
    "\n",
    "plt.yticks([])\n",
    "plt.xticks(np.arange(0, 181, 181 / 8), np.arange(0, 800, 100))\n",
    "plt.xlabel(\"milliseconds\", fontsize=15)\n",
    "plt.ylabel(\"channels\", fontsize=15)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing done with Brain Vision Software:**\n",
    "\n",
    "- Notch filter  0.05-25\n",
    "- Baseline Correction //what baseline?\n",
    "- OcularCorrection\n",
    "- Artifact Rejection\n",
    "\n",
    "**TOTHINK**\n",
    "\n",
    "- downsampling\n",
    "- additional bandpass filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction recommended for eeg data is **Wavelet Transform** (especially **Discrite Wavelet Transform**). Better that FFT for biomedical signals because of its localization characteristics of non-stationary signals in time and frequency domains. DWT decompositing signal into five frequency bands.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Discrete_wavelet_transform\n",
    "\n",
    "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0173138&type=printable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan:**\n",
    "- **5th level of DWT** because of 256 sampling rate ---> decomposition into five specific frequency: delta band (δ), theta band (θ), alpha band (α), beta band (β), and gamma band (γ)\n",
    "\n",
    "        (The  six  sub-bands,  particularly cD1, cD2, cD3, cD4, cD5 and cA5, represented the frequency range from the band-limited EEG signal, where cA is the decomposition approximation coefficient and cDs are the decomposition detail coefficients)\n",
    "\n",
    "- choosing mother of wavelet (**MWT**):\n",
    "    - choose few families of filters (need to read about best)\n",
    "\n",
    "\n",
    "- **parametrize classifier** with filter name (move vectorisation to classifier)\n",
    "    - consider: after decomposition each signal into five frequency bands **featurize each band separately** with choosen function (mean, std etc.) \n",
    "\n",
    "\n",
    "- run classifiers and print **comparison** of all \"parameters\" (classifier x MWT x small_function)\n",
    "    - with ANN eg. stop after ~1000 epoches, do comparison and choose best parameters\n",
    "\n",
    "**TOTHINK** Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## halo tu Filip\n",
    "- https://en.wikipedia.org/wiki/Mexican_hat_wavelet wydaje się dla nas najsensowniejsza\n",
    "- zamiast ANN można by spróbować zaimplementować to: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8930304 bo wydaje się potężne i w miare proste (w sensie że może da rade zrobić to interpretowalne) i wygrywa z EEGNetem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
