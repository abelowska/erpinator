{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG classification\n",
    "\n",
    "Link to working-documentation. Please, **update** when have an idea, plan or when sth important was done.\n",
    "\n",
    "\n",
    "https://docs.google.com/document/d/1i7tLHpXD-uXY5BopIGGcu-KWAR1DZvSlxQbYRi4fvBI/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext lab_black\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "import pywt\n",
    "from utils import tmax, tmin\n",
    "\n",
    "# plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=True):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: bool\n",
    "\n",
    "        Whether the epochs with overlapping bad segments are rejected automatically\n",
    "        by default. If False, only segments' channels annotated in .vmrk file as\n",
    "        'bad' will be rejected.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=reject_bad_segments,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"GNG_BK0504-64 el.vhdr\"\n",
    "\n",
    "# Import the BrainVision data into an MNE Raw object\n",
    "raw = mne.io.read_raw_brainvision(\"../data/\" + file)\n",
    "\n",
    "# Construct anootation filename\n",
    "annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "# Read in the event information as MNE annotations\n",
    "annot = mne.read_annotations(\"../data/\" + annot_file)\n",
    "\n",
    "# Add the annotations to our raw object so we can use them with the data\n",
    "raw.set_annotations(annot)\n",
    "\n",
    "events, event_ids = mne.events_from_annotations(raw)\n",
    "\n",
    "event_ids.pop(\"New Segment/\")\n",
    "event_ids.pop(\"Time 0/\")\n",
    "\n",
    "# Read epochs\n",
    "epochs = mne.Epochs(\n",
    "    raw=raw,\n",
    "    events=events,\n",
    "    event_id=event_ids,\n",
    "    tmin=tmin,\n",
    "    tmax=tmax,\n",
    "    baseline=None,\n",
    "    reject_by_annotation=False,\n",
    "    preload=True,\n",
    ")\n",
    "\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_intervals(annotations, description=\"Bad Interval/Bad Amplitude\"):\n",
    "    bad_intervals = []\n",
    "\n",
    "    for annot in annotations:\n",
    "        #         print(annot)\n",
    "        if annot[\"description\"] == description:\n",
    "            channel_epochs = get_epochs_channel_index(annot)\n",
    "            bad_intervals.append(channel_epochs)\n",
    "\n",
    "    return bad_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annotation item and returns dict {epochs: channels} where it occures\n",
    "\n",
    "\n",
    "def get_epochs_channel_index(item):\n",
    "    onset = item[\"onset\"]\n",
    "    duration = item[\"duration\"]\n",
    "    # mock\n",
    "    channel_num = 40  # item['channel_num']\n",
    "\n",
    "    bad_interval_start_index = get_epoch_index(onset)\n",
    "    bad_interval_end_index = get_epoch_index(onset + duration)\n",
    "\n",
    "    epochs_indexes = {bad_interval_start_index, bad_interval_end_index}\n",
    "    #     print(epochs_indexes)\n",
    "\n",
    "    return (channel_num, epochs_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get time in seconds from start of run and return which epoch index it is.\n",
    "\n",
    "\n",
    "def get_epoch_index(onset):\n",
    "    freq = raw.info[\"sfreq\"]\n",
    "    segment_duration = int((tmax - tmin) * freq)\n",
    "\n",
    "    position_in_data_points = onset * freq\n",
    "    epoch_index = int(position_in_data_points // segment_duration)\n",
    "\n",
    "    return epoch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_bad_channels(epochs, bad_intervals):\n",
    "    for channel, epochs_index in bad_intervals:\n",
    "        for index in epochs_index:\n",
    "            epochs[index].get_data()[0][channel] = \"dupa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_channels = find_bad_intervals(raw.annotations)\n",
    "# print(bad_channels)\n",
    "# clear_bad_channels(epochs, bad_channels)\n",
    "\n",
    "# a = [np.zeros(181)]\n",
    "\n",
    "epochs[3]._data[0][40]\n",
    "\n",
    "# epochs[3].get_data()[0][40] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs[3]._data[0][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_index = 3\n",
    "channel_num = 40\n",
    "\n",
    "for i in range(len(epochs)):\n",
    "    epochs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_bad_channel(epochs, epoch_index, channel_num):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.plot(n_epochs=5, event_colors={0: \"g\", 1: \"m\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = load_gonogo_responses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.plot(n_epochs=1, event_colors={0: \"g\", 1: \"m\"})\n",
    "None  # prevents doubled output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_response_epochs = epochs[\"correct_response\"]\n",
    "error_response_epochs = epochs[\"error_response\"]\n",
    "\n",
    "\n",
    "# Calculate averages of events sets\n",
    "correct_response_evoked = correct_response_epochs.average()\n",
    "error_response_evoked = error_response_epochs.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages of two event sets\n",
    "\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    dict(\n",
    "        correct_response=correct_response_evoked, error_response=error_response_evoked\n",
    "    ),\n",
    "    legend=\"upper left\",\n",
    "    show_sensors=\"upper right\",\n",
    "    ylim=dict(eeg=[-10, 10]),\n",
    "    invert_y=True,\n",
    "    combine=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages of error response events per channel\n",
    "\n",
    "error_response_evoked.plot_joint(picks=\"eeg\")\n",
    "error_response_evoked.plot_topomap(times=[0.0, 0.08, 0.1, 0.12, 0.2], ch_type=\"eeg\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages of merged event sets (diff between error and correct) per channel\n",
    "\n",
    "evoked_diff = mne.combine_evoked(\n",
    "    [correct_response_evoked, error_response_evoked], weights=[1, -1]\n",
    ")\n",
    "evoked_diff.plot_joint()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_mean_dict = {}\n",
    "\n",
    "for key in epochs.event_id.keys():\n",
    "    mean_key = key + \"_mean\"\n",
    "    events_mean_dict[mean_key] = epochs[key]._data.mean(axis=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart with averages of correct and error responses per channel\n",
    "\n",
    "colors = [\"b\", \"r\", \"g\"]\n",
    "color_iterator = 0\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "for key in events_mean_dict:\n",
    "    epoch = events_mean_dict[key]\n",
    "    plt.plot(\n",
    "        epoch.T + np.arange(start=1e-6, step=10e-6, stop=301e-7),\n",
    "        label=key,\n",
    "        color=colors[color_iterator],\n",
    "    )\n",
    "    color_iterator = color_iterator + 1\n",
    "\n",
    "plt.yticks([])\n",
    "plt.xticks(np.arange(0, 181, 181 / 8), np.arange(0, 800, 100))\n",
    "plt.xlabel(\"milliseconds\", fontsize=15)\n",
    "plt.ylabel(\"channels\", fontsize=15)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing done with Brain Vision Software:**\n",
    "\n",
    "- Notch filter  0.05-25\n",
    "- Baseline Correction //what baseline?\n",
    "- Ocular Correction\n",
    "- Artifact Rejection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction recommended for eeg data is **Wavelet Transform** (especially **Discrite Wavelet Transform**). Better that FFT for biomedical signals because of its localization characteristics of non-stationary signals in time and frequency domains. DWT decompositing signal into five frequency bands.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Discrete_wavelet_transform\n",
    "\n",
    "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0173138&type=printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for feature extraction: create X-data array and Y-labels array\n",
    "\n",
    "X = epochs._data\n",
    "Y = []\n",
    "\n",
    "for i in range(len(epochs)):\n",
    "    event_id = list(epochs[i].event_id.values())[0]\n",
    "    Y.append(event_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Wavelet Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mother wavelets list for continuous WT\n",
    "\n",
    "MWT_list = pywt.wavelist(kind=\"continuous\")\n",
    "\n",
    "# Remove cmor, fbsp, shan wavelets because of need of special specification\n",
    "# of this wavelet with bandpass and center - do not want to handle with it NOW.\n",
    "MWT_list.remove(\"cmor\")\n",
    "MWT_list.remove(\"fbsp\")\n",
    "MWT_list.remove(\"shan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct list of scales corresponding to pseudo-frequencies [128Hz-1Hz] for each wavelet\n",
    "# Could help understand: https://www.researchgate.net/publication/267491844_Continuous_Wavelet_Transform_EEG_Features_of_Alzheimer%27s_Disease\n",
    "\n",
    "# TODO: refactor\n",
    "\n",
    "signal_frequency = 256\n",
    "\n",
    "\n",
    "# compute coeffs of wavelets listed in MWT_list for an given epoch (one channel)\n",
    "def compute_coeffs(epoch):\n",
    "\n",
    "    wavelets = {}\n",
    "\n",
    "    for MWT in MWT_list:\n",
    "        center_wavelet_frequency = pywt.scale2frequency(MWT, [1])[0]\n",
    "        const = center_wavelet_frequency * signal_frequency\n",
    "\n",
    "        # construct scales\n",
    "        scales = np.arange(const / 128, const / 1, 0.1).tolist()\n",
    "\n",
    "        # compute coeffs\n",
    "        coef, freqs = pywt.cwt(\n",
    "            data=epoch, scales=scales, wavelet=MWT, sampling_period=1 / signal_frequency\n",
    "        )\n",
    "\n",
    "        # Save coeffs from the MWT\n",
    "        wavelets[MWT] = coef\n",
    "\n",
    "    return wavelets\n",
    "\n",
    "\n",
    "epoch = X[0][0]\n",
    "\n",
    "coeffs_dict = compute_coeffs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
