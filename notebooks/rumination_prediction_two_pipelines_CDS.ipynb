{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a73099-38c1-4bf0-afd3-5481df7c3d19",
   "metadata": {},
   "source": [
    "# Rumination prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff9f56-5483-44c7-b9cc-486a31838279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e84d26-b32c-41ae-b3f4-81e0ee55156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import ast\n",
    "import os.path as op\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium.featurize\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\"..\")\n",
    "# from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c69899-6a32-4418-9079-8bd66aed497f",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading data\n",
    "\n",
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc483a7-410f-4b65-9955-9df42c783bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths TODO\n",
    "dir_path = os.path.dirname(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a25389-d030-415b-982d-b68df981b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6  # Start and end of the segments\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1\n",
    "ALL = 2\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ef4f5-f711-4f8e-8ba5-163dc50bd2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_order_list = [\n",
    "    \"Fp1\",\n",
    "    \"AF7\",\n",
    "    \"AF3\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    \"F5\",\n",
    "    \"F7\",\n",
    "    \"FT7\",\n",
    "    \"FC5\",\n",
    "    \"FC3\",\n",
    "    \"FC1\",\n",
    "    \"C1\",\n",
    "    \"C3\",\n",
    "    \"C5\",\n",
    "    \"T7\",\n",
    "    \"TP7\",\n",
    "    \"CP5\",\n",
    "    \"CP3\",\n",
    "    \"CP1\",\n",
    "    \"P1\",\n",
    "    \"P3\",\n",
    "    \"P5\",\n",
    "    \"P7\",\n",
    "    \"P9\",\n",
    "    \"PO7\",\n",
    "    \"PO3\",\n",
    "    \"O1\",\n",
    "    \"Iz\",\n",
    "    \"Oz\",\n",
    "    \"POz\",\n",
    "    \"Pz\",\n",
    "    \"CPz\",\n",
    "    \"Fpz\",\n",
    "    \"Fp2\",\n",
    "    \"AF8\",\n",
    "    \"AF4\",\n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"F2\",\n",
    "    \"F4\",\n",
    "    \"F6\",\n",
    "    \"F8\",\n",
    "    \"FT8\",\n",
    "    \"FC6\",\n",
    "    \"FC4\",\n",
    "    \"FC2\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"C2\",\n",
    "    \"C4\",\n",
    "    \"C6\",\n",
    "    \"T8\",\n",
    "    \"TP8\",\n",
    "    \"CP6\",\n",
    "    \"CP4\",\n",
    "    \"CP2\",\n",
    "    \"P2\",\n",
    "    \"P4\",\n",
    "    \"P6\",\n",
    "    \"P8\",\n",
    "    \"P10\",\n",
    "    \"PO8\",\n",
    "    \"PO4\",\n",
    "    \"O2\",\n",
    "]\n",
    "\n",
    "channels_dict = dict(zip(channels_order_list, np.arange(1, 64, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c412f36-66a2-4fd8-bfa7-caa98c4a315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=\"all\",\n",
    "    personal=True,\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "\n",
    "    On default, loads a train set: chooses only 80% of participants\n",
    "    and for each of them chooses 80% of epochs.\n",
    "    It will choose them deterministically.\n",
    "\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    If test_participants is set to True, it will load remaining 20% of participants.\n",
    "    If test_epochs is set to True, it will load remaining 20% of epochs.\n",
    "    Test epochs are chronologically after train epochs,\n",
    "    because it reflects real usage (first callibration and then classification).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "    personal: bool\n",
    "        whether a model will be both trained and tested on epochs from one person\n",
    "        if false, person's epochs aren't split into test and train\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    print(os.path.abspath(\"\"))\n",
    "    dir_path = os.path.dirname(os.path.abspath(\"\"))\n",
    "    print(dir_path)\n",
    "    header_files_glob = os.path.join(dir_path, \"data/responses_100_600/*.vhdr\")\n",
    "    header_files = glob.glob(header_files_glob)\n",
    "\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    # cut 20% of data for testing\n",
    "    h_train, h_test = train_test_split(header_files, test_size=0.3, random_state=0)\n",
    "\n",
    "    if test_participants:\n",
    "        header_files = h_test\n",
    "    else:\n",
    "        header_files = h_train\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 3 or len(correct) < 3:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, participant_epochs, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991419b-7acd-42bc-b048-e5e9fa3cb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, participant_epochs, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        if info == \"all\":\n",
    "            rumination_df = pd.read_csv(info_filename)\n",
    "        else:\n",
    "            rumination_df = pd.read_csv(info_filename, usecols=[\"File\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"File\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "\n",
    "#     for epoch in correct:\n",
    "#         epoch_df = pd.DataFrame(\n",
    "#             {\"id\": [id], \"epoch\": [epoch], \"marker\": [CORRECT]}\n",
    "#         ).join(info_df)\n",
    "#         participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "#     for epoch in error:\n",
    "#         epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [epoch], \"marker\": [ERROR]}).join(\n",
    "#             info_df\n",
    "#         )\n",
    "#         participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "        \n",
    "#     print(participant_epochs)\n",
    "        \n",
    "    epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [participant_epochs], \"marker\": [ALL]}).join(\n",
    "            info_df\n",
    "        )\n",
    "    participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88517e-884a-4b5b-9652-1bf2e7787aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' means that bad segments are rejected automatically.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = True\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5e07d-9618-47de-a706-687a529fb2fe",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8c673-5477-4126-b09d-870a8fcd9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_100_600_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32394ef-8197-4010-a680-bb85379baedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_500_300_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df_3 = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df_3 = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df_3.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df_3.to_pickle(\"../data/\" + epochs_df_3.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74938fe4-b8ed-4e5f-a5f8-90a24c88265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_400_600_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df2 = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df2 = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df2.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df2.to_pickle(\"../data/\" + epochs_df2.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055815fa-0d47-4bdd-9caf-51d60ced5d91",
   "metadata": {},
   "source": [
    "#### Read data for external testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c990ac1-14fc-4d0c-bf5d-d96f076e1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_100_600_test_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_df = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_df.to_pickle(\"../data/\" + epochs_test_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0137a-0f6b-49c9-b376-a5d60c201045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_500_300_test_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_df_3 = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_df_3 = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_df_3.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_df_3.to_pickle(\"../data/\" + epochs_test_df_3.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea2478-01a2-499e-b6bb-b20e580c0676",
   "metadata": {},
   "source": [
    "---\n",
    "## Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dff3a8-14a7-445c-8940-aba32c28ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from rumination_experiment_transformers_averaged_CDS import *\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a593d8-7928-410c-88b9-0b4b25f1c493",
   "metadata": {},
   "source": [
    "#### Create X train and y train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afda6e-cc8b-492f-bb80-ac3a0b0cde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of the analysed condition: erroneous responses or correct responses\n",
    "\n",
    "dataset = ERROR\n",
    "dataset_name = \"correct_response\" if dataset == CORRECT else \"error_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326f5ec-d352-4f43-a403-c4debfbec641",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100 = epochs_df\n",
    "# X_train_df_400 = epochs_df2\n",
    "X_train_df_500 = epochs_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04f8f0-845e-4b0d-918f-1dded2d84fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df_500 = epochs_test_df_3\n",
    "X_test_df_100 = epochs_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484eb207-0fc2-495f-aa9f-380ecc815b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train_df_100\n",
    "X_test_df = X_test_df_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f1ac9-2bd1-46c6-b404-a394ac24603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape 1-D: scores\n",
    "rumination = np.array(X_train_df[\"Rumination Full Scale\"].to_list())\n",
    "deppression = np.array(X_train_df[\"DASS-21 Depression scale\"].to_list())\n",
    "anxiety = np.array(X_train_df[\"DASS-21 Anxiety scale\"].to_list())\n",
    "stress = np.array(X_train_df[\"DASS-21 Stress scale\"].to_list())\n",
    "\n",
    "\n",
    "y_train = rumination\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f04e5-5e91-42df-a5a2-1a5e28a92e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rum_test = np.array(X_test_df[\"Rumination Full Scale\"].to_list())\n",
    "y_rum_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b37add-c98f-4625-8049-6a2bd087b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366c3c6-dd06-4837-9bd8-a286d9c530f6",
   "metadata": {},
   "source": [
    "---\n",
    "### Experiments \n",
    "\n",
    "Parameters of experiments:\n",
    "- regressors\n",
    "- hyperparameters\n",
    "- preprocessing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962b4be-4bc7-4e61-9bb7-4151a2f59008",
   "metadata": {},
   "source": [
    "#### Prepare experiment estimating \n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fe861-b8ef-4c53-a68c-5241ba5348ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating model with grid search\n",
    "\n",
    "\n",
    "def rate_regressor(\n",
    "    X_train, y_train, X_test, y_test, regressor, regressor_params, base_steps, cv=3\n",
    "):\n",
    "    # define cross-validation method\n",
    "    cv_kf = KFold(n_splits=3)\n",
    "\n",
    "    pipeline = Pipeline([base_steps, regressor])\n",
    "    param_grid = regressor_params\n",
    "    # print(f\"Param grid {param_grid}\")\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv_kf,\n",
    "        scoring={\"r2\", \"neg_mean_absolute_error\", \"neg_mean_squared_error\"},\n",
    "        refit=\"r2\",\n",
    "        return_train_score=True,\n",
    "        n_jobs=10,\n",
    "        verbose=1,\n",
    "        error_score=\"raise\",\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfa7df-bda2-4249-97c5-999e73d1abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating p-value with permutation test\n",
    "\n",
    "\n",
    "def calculate_p_permutations(estimator, X, y, cv=3, n_permutations=1000, n_jobs=10):\n",
    "\n",
    "    score_, perm_scores_, pvalue_ = permutation_test_score(\n",
    "        estimator, X, y, cv=cv, n_permutations=n_permutations, n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    # summarize\n",
    "    print(f\"     The permutation P-value is = {pvalue_:.4f}\")\n",
    "    print(f\"     The permutation score is = {score_:.4f}\\n\")\n",
    "\n",
    "    return score_, pvalue_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ce1e5-4ec9-4ab1-b3d7-28a80d119ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conducting experiment and saving selected info do result df\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    tested_regressors,\n",
    "    regressor_params,\n",
    "    pipeline_name,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    dataset_name,\n",
    "    base_steps,\n",
    "    preprocessed_pipeline,\n",
    "    X_test_df,\n",
    "    y_rum,\n",
    "    results_df,\n",
    "):\n",
    "\n",
    "    for (regressor, params) in tested_regressors:\n",
    "        print(f\"Rating {regressor} \\n\")\n",
    "        tested_params = {**regressor_params, **params}\n",
    "\n",
    "        # enter to grid search\n",
    "        grid_result = rate_regressor(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            regressor,\n",
    "            tested_params,\n",
    "            base_steps,\n",
    "            cv=3,\n",
    "        )\n",
    "\n",
    "        #     predictions = grid_result.predict(X_test)\n",
    "        #     r2 = grid_result.score(X_test, y_test)\n",
    "        #     mae = mean_absolute_error(y_test, predictions)\n",
    "        #     r2_adj = r2_adjusted_scorer(y_test, predictions, len(X_test[0]), len(X_test))\n",
    "\n",
    "        best_estimator_index = grid_result.best_index_\n",
    "        mean_cv_r2 = grid_result.cv_results_[\"mean_test_r2\"][best_estimator_index]\n",
    "        std_cv_r2 = grid_result.cv_results_[\"std_test_r2\"][best_estimator_index]\n",
    "        mean_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        mean_cv_neg_mean_squared_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_squared_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_squared_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_squared_error\"\n",
    "        ][best_estimator_index]\n",
    "        \n",
    "        mean_train_r2 = grid_result.cv_results_[\"mean_train_r2\"][best_estimator_index]\n",
    "        mean_train_mae = grid_result.cv_results_[\"mean_train_neg_mean_absolute_error\"][best_estimator_index]\n",
    "        mean_train_mse = grid_result.cv_results_[\"mean_train_neg_mean_squared_error\"][best_estimator_index]\n",
    "\n",
    "\n",
    "        print(f\"     Best parameters: {grid_result.best_params_}\")\n",
    "        print(f\"     mean r2: {mean_cv_r2}           ± {round(std_cv_r2,3)}\")\n",
    "        print(f\"     mean r2 train: {mean_train_r2}\")\n",
    "\n",
    "        cv_results = grid_result.cv_results_\n",
    "\n",
    "        # calculate p-value\n",
    "        scores_, pvalue_ = calculate_p_permutations(\n",
    "            grid_result.best_estimator_, X_train, y_train\n",
    "        )\n",
    "        \n",
    "        pre_processed_test_X = preprocessed_pipeline.transform(X_test_df)\n",
    "        estimator = grid_result.best_estimator_\n",
    "        score = estimator.score(pre_processed_test_X, y_rum)\n",
    "        \n",
    "        print(print(f\"     external validation r2: {score}\"))\n",
    "        \n",
    "\n",
    "        # insert selected info to df\n",
    "        data = {\n",
    "            \"data_set\": dataset_name,\n",
    "            \"pipeline_name\": pipeline_name,\n",
    "            \"model\": regressor[0],\n",
    "            \"parameters\": grid_result.best_params_,\n",
    "            \"mean_cv_r2\": mean_cv_r2,\n",
    "            \"std_cv_r2\": std_cv_r2,\n",
    "            \"mean_cv_mae\": mean_cv_neg_mean_absolute_error,\n",
    "            \"std_cv_mae\": std_cv_neg_mean_absolute_error,\n",
    "            \"mean_cv_mse\":mean_cv_neg_mean_squared_error,\n",
    "            \"std_cv_mse\": std_cv_neg_mean_squared_error,\n",
    "            \"cv_results\": cv_results,\n",
    "            \"mean_train_r2\": mean_train_r2,\n",
    "            \"mean_train_mae\":mean_train_mae,\n",
    "            \"mean_train_mse\":mean_train_mse,\n",
    "            \"p-value\": pvalue_,\n",
    "            \"best_estimator\": grid_result.best_estimator_,\n",
    "            \"pre_processed_pipeline\": preprocessed_pipeline,\n",
    "            \"external_score\":score\n",
    "        }\n",
    "\n",
    "        results_df = results_df.append(data, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fea081-c0ff-47c9-ba47-6323d0fa33bd",
   "metadata": {},
   "source": [
    "### Perform Experiments\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385045e-557e-44ab-a650-439c38ff8ab6",
   "metadata": {},
   "source": [
    "#### Global parameters common for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19073a0-ee3d-4ace-bbfd-177c753cefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define estimators and their hyperparameters\n",
    "\n",
    "en = (\"en\", ElasticNet(random_state=random_state))\n",
    "en_params = dict(\n",
    "    en__alpha=np.logspace(-7, 3, num=20, base=10),\n",
    "    en__l1_ratio=np.logspace(-8, 0, num=17, base=10),\n",
    ")\n",
    "\n",
    "kr = (\"kr\", KernelRidge(kernel=\"rbf\"))\n",
    "kr_params = dict(\n",
    "    kr__alpha=np.logspace(-5, 3, num=20, base=10),\n",
    "    kr__gamma=np.logspace(-5, 3, num=20, base=10),\n",
    ")\n",
    "\n",
    "\n",
    "svr = (\"svr\", SVR())\n",
    "svr_params = dict(\n",
    "    svr__kernel=[\"linear\", \"rbf\"],\n",
    "    svr__C=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    svr__gamma=[\"scale\"],\n",
    "    svr__epsilon=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    ")\n",
    "\n",
    "tested_regressors = [\n",
    "    (svr, svr_params), \n",
    "    (kr, kr_params), \n",
    "    (en, en_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6db57-8862-4bb5-8b18-f46cbc176dd7",
   "metadata": {},
   "source": [
    "----\n",
    "# Check within-subject and between-subject variance of feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e4384-3e06-40b8-8fb8-1b5e1bc38459",
   "metadata": {},
   "source": [
    "# Between subject variation\n",
    "- without spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f933b8-7084-4689-b107-41278c91417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df_rum = pd.read_pickle(\"../data/split0.3/regression_union_100-600_baselined_centered-2_diff_boxes_diff_pe-ind_diff_models.pkl\")\n",
    "\n",
    "# ern_fex = results_df_rum[results_df_rum['model'] == 'en'].best_estimator[11]['features'].transformer_list[0][1]['feature_extraction']\n",
    "# pe_fex = results_df_rum[results_df_rum['model'] == 'en'].best_estimator[11]['features'].transformer_list[1][1]['feature_extraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b9705-01f0-400b-8731-74ae0d12c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_df = pd.DataFrame({'pipeline': [], 'values': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4be3c8-3f83-4350-b705-4e69f5ba4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "box= [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"Pz\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776dfd2c-8395-42fa-8d11-78372ce326f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d719f0-74c8-4677-b822-2e25411f8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-100:0'\n",
    "\n",
    "# ern_features = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                 ])\n",
    "\n",
    "\n",
    "# pe_features = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                 ])\n",
    "\n",
    "# ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "# x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             (\"average\", Evoked()),\n",
    "#             ('extract_data', ExtractData()),\n",
    "#             (\"lowpass_filter\", LowpassFilter()),\n",
    "#             (\"binning\", BinTransformer(step=12)),\n",
    "#             (\"baseline\", ErnBaselined()),\n",
    "#             (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "#             (\"features\", ern_pe_features),\n",
    "# ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "# x_feature_100_between = np.sum(x_pre, axis=1)\n",
    "# x_100_std_between = np.std(x_feature_100_between, axis=0)\n",
    "\n",
    "# values = x_feature_100_between.flatten().tolist()\n",
    "# names = [pipeline_name] * len(x_feature_100_between)\n",
    "\n",
    "# temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "# between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c600f-75c0-490b-94df-9f33b62182d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78512c92-97d5-4ca1-b46a-5249f148f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-500:-300'\n",
    "\n",
    "# ern_features = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                 ])\n",
    "\n",
    "\n",
    "# pe_features = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                 ])\n",
    "\n",
    "# ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "# x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             (\"average\", Evoked()),\n",
    "#             ('extract_data', ExtractData()),\n",
    "#             (\"lowpass_filter\", LowpassFilter()),\n",
    "#             (\"binning\", BinTransformer(step=12)),\n",
    "#             (\"baseline\", ErnBaselined()),\n",
    "#             (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "#             (\"features\", ern_pe_features),\n",
    "# ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "# x_feature_500_between = np.sum(x_pre, axis=1)\n",
    "# x_500_std_between = np.std(x_feature_500_between, axis=0)\n",
    "\n",
    "# values = x_feature_500_between.flatten().tolist()\n",
    "# names = [pipeline_name] * len(x_feature_500_between)\n",
    "\n",
    "# temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "# between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477bf4f8-8a5b-409f-8f66-01d4106b5f3e",
   "metadata": {},
   "source": [
    "- different lowpass filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902f5bb-ecf5-4067-b8c4-a121a057c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-100:0 '+ str(cutoff)\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2_prim()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=7)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                ('neg', ReverseSignal()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "    x_feature_100_between = np.sum(x_pre, axis=1)\n",
    "    x_100_std_between = np.std(x_feature_100_between, axis=0)\n",
    "\n",
    "    values = x_feature_100_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_100_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    ###########################################################################################\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa5649-f027-4996-8efc-87bb64ac0e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [15,20,30,40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-100:0 '+ str(cutoff) + ' no BS'\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                # (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "    x_feature_100_between = np.sum(x_pre, axis=1)\n",
    "    x_100_std_between = np.std(x_feature_100_between, axis=0)\n",
    "\n",
    "    values = x_feature_100_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_100_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f49399-af66-4367-923f-e8f19937bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40]:\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-500:-300 '+ str(cutoff)\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "    x_feature_500_between = np.sum(x_pre, axis=1)\n",
    "    x_500_std_between = np.std(x_feature_500_between, axis=0)\n",
    "\n",
    "    values = x_feature_500_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_500_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    ##########################################################################\n",
    "    \n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-500:-300 '+ str(cutoff) + ' no BS'\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                # (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "    x_feature_500_between = np.sum(x_pre, axis=1)\n",
    "    x_500_std_between = np.std(x_feature_500_between, axis=0)\n",
    "\n",
    "    values = x_feature_500_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_500_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05a216-d4b7-4728-8801-2485a25b43ea",
   "metadata": {},
   "source": [
    "- with spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124bb984-d695-4ad0-84c6-73f220b7c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c843f4-438c-4a64-84d8-cf7812cae71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 SF '+ str(cutoff)\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2_prim()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                ('neg', ReverseComponent3()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "                # (\"ern_amplitude\", ErnAmplitude2())\n",
    "    ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "    x_feature_100_sf_between = np.sum(x_pre, axis=1)\n",
    "    x_100_std_sf_between = np.std(x_feature_100_sf_between, axis=0)\n",
    "\n",
    "    values = x_feature_100_sf_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_100_sf_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 SF '+ str(cutoff) + ' no BS'\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                ('neg', ReverseComponent3()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                # (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "                # (\"ern_amplitude\", ErnAmplitude2())\n",
    "    ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "    x_feature_100_sf_between = np.sum(x_pre, axis=1)\n",
    "    x_100_std_sf_between = np.std(x_feature_100_sf_between, axis=0)\n",
    "\n",
    "    values = x_feature_100_sf_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_100_sf_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80b568-34a4-4633-befa-036cf1d96adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d9773-4a88-4023-ba85-673f3baf7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [15,20,30,40]:\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-500:-300 SF ' + str(cutoff)\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                ('neg', ReverseComponent3()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "    x_feature_500_sf_between = np.sum(x_pre, axis=1)\n",
    "    x_500_std_sf_between = np.std(x_feature_500_sf_between, axis=0)\n",
    "\n",
    "    values = x_feature_500_sf_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_500_sf_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    ##################################################################\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "    \n",
    "    pipeline_name = '-500:-300 SF ' + str(cutoff) + ' no BS'\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    ])\n",
    "\n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "    x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "                (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "                ('neg', ReverseComponent3()),\n",
    "                (\"binning\", BinTransformer(step=12)),\n",
    "                # (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                (\"features\", ern_pe_features),\n",
    "    ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "    x_feature_500_sf_between = np.sum(x_pre, axis=1)\n",
    "    x_500_std_sf_between = np.std(x_feature_500_sf_between, axis=0)\n",
    "\n",
    "    values = x_feature_500_sf_between.flatten().tolist()\n",
    "    names = [pipeline_name] * len(x_feature_500_sf_between)\n",
    "\n",
    "    temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "    between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f856b-8f27-48f3-9b3b-365121554e17",
   "metadata": {},
   "source": [
    "- with spatial filter and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03450039-0b24-41f7-82ff-857e1e45945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce9087-934b-4fd4-86bc-f39188cc7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-100:0 SF EX'\n",
    "\n",
    "# x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             (\"average\", Evoked()),\n",
    "#             ('extract_data', ExtractData()),\n",
    "#             (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#             (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "#             (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#             (\"lowpass_filter\", LowpassFilter()),\n",
    "#             ('neg', ReverseComponent3()),\n",
    "#             (\"binning\", BinTransformer(step=12)),\n",
    "#             (\"baseline\", ErnBaselined()),\n",
    "#             (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "#             # (\"features\", ern_pe_features),\n",
    "#             # (\"ern_amplitude\", ErnAmplitude2())\n",
    "# ]).fit_transform(X_train_df_100copy)\n",
    "\n",
    "\n",
    "# ern_features_pre = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                                 # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "#                 ]).fit_transform(x_pre)\n",
    "\n",
    "\n",
    "# ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "# pe_features_pre = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                                 # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "\n",
    "#                 ]).fit_transform(x_pre)\n",
    "\n",
    "# pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "# # ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10).fit_transform()\n",
    "\n",
    "# x_pre2 = zip(ern_features, pe_features)\n",
    "# x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "        \n",
    "# x_feature_100_sf_between = np.sum(x_pre2, axis=1)\n",
    "# x_100_std_sf_between = np.std(x_feature_100_sf_between, axis=0)\n",
    "\n",
    "# values = x_feature_100_sf_between.flatten().tolist()\n",
    "# names = [pipeline_name] * len(x_feature_100_sf_between)\n",
    "\n",
    "# temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "# between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072d44e-04b8-4f02-bd3d-9df29694e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe505b8-d960-4bca-a21f-dbe6881ed8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-500:-300 SF EX'\n",
    "\n",
    "# # ern_features = Pipeline(steps=[\n",
    "# #                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "# #                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "# #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                                 (\"scaler\", StandardScaler()),\n",
    "# #                                 (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "# #                 ])\n",
    "\n",
    "\n",
    "# # pe_features = Pipeline(steps = [\n",
    "# #                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "# #                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "# #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                                 (\"scaler\", StandardScaler()),\n",
    "# #                                 (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "# #                 ])\n",
    "\n",
    "# # ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "# # x_pre = Pipeline([\n",
    "# #             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "# #             (\"average\", Evoked()),\n",
    "# #             ('extract_data', ExtractData()),\n",
    "# #             (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "# #             (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "# #             (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "# #             (\"lowpass_filter\", LowpassFilter()),\n",
    "# #             ('neg', ReverseComponent3()),\n",
    "# #             (\"binning\", BinTransformer(step=12)),\n",
    "# #             (\"baseline\", ErnBaselined()),\n",
    "# #             (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "# #             (\"features\", ern_pe_features),\n",
    "# #             # (\"ern_amplitude\", ErnAmplitude2())\n",
    "# # ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "# x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             (\"average\", Evoked()),\n",
    "#             ('extract_data', ExtractData()),\n",
    "#             (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#             (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "#             (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#             (\"lowpass_filter\", LowpassFilter()),\n",
    "#             ('neg', ReverseComponent3()),\n",
    "#             (\"binning\", BinTransformer(step=12)),\n",
    "#             (\"baseline\", ErnBaselined()),\n",
    "#             (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "#             # (\"features\", ern_pe_features),\n",
    "#             # (\"ern_amplitude\", ErnAmplitude2())\n",
    "# ]).fit_transform(X_train_df_500copy)\n",
    "\n",
    "\n",
    "# ern_features_pre = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                                 # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "#                 ]).fit_transform(x_pre)\n",
    "\n",
    "\n",
    "# ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "# pe_features_pre = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                                 # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "\n",
    "#                 ]).fit_transform(x_pre)\n",
    "\n",
    "# pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "# # ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10).fit_transform()\n",
    "\n",
    "# x_pre2 = zip(ern_features, pe_features)\n",
    "# x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "\n",
    "# x_feature_500_sf_between = np.sum(x_pre2, axis=1)\n",
    "# x_500_std_sf_between = np.std(x_feature_500_sf_between, axis=0)\n",
    "\n",
    "# values = x_feature_500_sf_between.flatten().tolist()\n",
    "# names = [pipeline_name] * len(x_feature_500_sf_between)\n",
    "\n",
    "# temp_df = pd.DataFrame(zip(names, values), columns=['pipeline', 'values'])\n",
    "\n",
    "# between_df = between_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f540e-c711-4261-80ee-8d56bfdf40b5",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64b4d2-5413-46f8-aaba-96ba681baa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,8)},font_scale = 1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "ax = sns.pointplot(x = 'values', y='pipeline', data = between_df, orient='h', join=False, estimator=np.std, ci=95,capsize=.05,)\n",
    "# ax.figure.savefig(\"between_subject_std_ern_pe_lowpass.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe15df-0ecc-43c1-9acf-6743a262b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,8)},font_scale = 1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "ax = sns.pointplot(x = 'values', y='pipeline', data = between_df, orient='h', join=False, estimator=np.std, ci=95,capsize=.05,)\n",
    "# ax.figure.savefig(\"between_subject_std_ern_pe_lowpass.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77382c-ef13-4c83-b141-95e559bead46",
   "metadata": {},
   "source": [
    "----\n",
    "# WITHIN SUBJECT\n",
    "- without spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f518d7b-b1cc-4d06-86b2-a696718ada98",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_df = pd.DataFrame({'pipeline': [], 'values': []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da388eb4-07fd-4a6d-881f-7f97e0b88acf",
   "metadata": {},
   "source": [
    "## -100 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d5f22-221c-42b0-ae54-08dcf8ec644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1911c9b-cfd7-46db-9038-e8f1bf7411e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 ' + str(cutoff)\n",
    "\n",
    "    for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "        X = X_train_df_100copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "\n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2_prim()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=2, stop_pe_bin=7)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            ('neg', ReverseSignal()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "            # (\"ern_amplitude\", ErnAmplitude2()),\n",
    "        ]).fit_transform(x_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)\n",
    "        \n",
    "        ##########################################################################\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2018f07-bf40-43b1-8fc3-1d31bf56b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [15,20,30,40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 ' + str(cutoff) + ' no BS'\n",
    "\n",
    "    for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "        X = X_train_df_100copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "\n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "            # (\"ern_amplitude\", ErnAmplitude2()),\n",
    "        ]).fit_transform(x_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79f315-00f4-4d77-9ffa-96a1758c7eed",
   "metadata": {},
   "source": [
    "## -500 to -300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544359a6-7414-4f0a-9d8d-faa9765f6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2eab5d-1760-4f65-9d65-30d7dda19461",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [15,20,30,40]:\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "\n",
    "    pipeline_name = '-500:-300 ' + str(cutoff)\n",
    "\n",
    "    for i in range(0,len(X_train_df_500copy)):\n",
    "\n",
    "        X = X_train_df_500copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "            # (\"ern_amplitude\", ErnAmplitude2()),\n",
    "        ]).fit_transform(x_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)\n",
    "        \n",
    "        #############################################################################\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "\n",
    "    pipeline_name = '-500:-300 ' + str(cutoff)  + ' no BS'\n",
    "\n",
    "    for i in range(0,len(X_train_df_500copy)):\n",
    "\n",
    "        X = X_train_df_500copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "            (\"features\", ern_pe_features),\n",
    "            # (\"ern_amplitude\", ErnAmplitude2()),\n",
    "        ]).fit_transform(x_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2fdb1b-8dcf-405e-8129-ba281ef4f620",
   "metadata": {},
   "source": [
    "- With spatial filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8f609-bc53-4483-8f05-bec95bcbe8f4",
   "metadata": {},
   "source": [
    "### -100 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c094367-344c-49b8-acdb-a42414308e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b008b0-79f3-4346-bc05-7ced5a69a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pip_for_spatial_filter = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list = box)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "]).fit(X_train_df_100copy)\n",
    "\n",
    "spatial_filter = pre_pip_for_spatial_filter['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e89bd-fa73-4221-9bcd-27eabfbdfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28971e9-bf99-4ad1-8492-44710c6ff6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [40]:\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 SF ' + str(cutoff)\n",
    "\n",
    "    for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "        X = X_train_df_100copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "\n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        x_pre_pre = Pipeline([\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\", spatial_filter),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "        ern_features_pre = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2_prim()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "        # ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "        pe_features_pre = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "        # pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "    #     x_pre2 = zip(ern_features, pe_features)\n",
    "    #     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "\n",
    "    #     print(x_pre2.shape)\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features_pre), (\"pe_features\", pe_features_pre)], n_jobs = 10)\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"features\", ern_pe_features),\n",
    "        ]).fit_transform(x_pre_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)\n",
    "        \n",
    "        ########################################################################\n",
    "    X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "\n",
    "    pipeline_name = '-100:0 SF ' + str(cutoff)  + ' no BS'\n",
    "\n",
    "\n",
    "    for i in range(0,len(X_train_df_100copy)):\n",
    "\n",
    "        X = X_train_df_100copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "\n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "        x_pre_pre = Pipeline([\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\", spatial_filter),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "        ern_features_pre = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "        # ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "        pe_features_pre = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                    ])\n",
    "\n",
    "        # pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "    #     x_pre2 = zip(ern_features, pe_features)\n",
    "    #     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "\n",
    "    #     print(x_pre2.shape)\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features_pre), (\"pe_features\", pe_features_pre)], n_jobs = 10)\n",
    "\n",
    "        x_pre2 = Pipeline([\n",
    "            (\"features\", ern_pe_features),\n",
    "        ]).fit_transform(x_pre_pre)\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)\n",
    "\n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f2c68-d047-477e-a514-263030d594bf",
   "metadata": {},
   "source": [
    "### -500 to -300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea380d64-bd87-4f1c-92d8-9cf84560e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f731af0-3c01-45f7-8e72-3197c292a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pip_for_spatial_filter = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list = box)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "]).fit(X_train_df_500copy)\n",
    "\n",
    "spatial_filter = pre_pip_for_spatial_filter['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a8244-b6b8-428c-afc6-50976c0e40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc8de9-dab2-47cf-ac1b-7bf4b2766d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in [15,20,30,40]:\n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "\n",
    "    pipeline_name = '-500:-300 SF ' + str(cutoff)\n",
    "\n",
    "    for i in range(0,len(X_train_df_500copy)):\n",
    "\n",
    "        X = X_train_df_500copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "\n",
    "\n",
    "        x_pre_pre = Pipeline([\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\", spatial_filter),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "        ]).transform(x_pre)\n",
    "\n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "        x_pre2 = Pipeline([(\"features\", ern_pe_features)]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "    #     x_pre_pre = Pipeline([\n",
    "    #         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "    #         (\"spatial_filter\", spatial_filter),\n",
    "    #         (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "    #         (\"lowpass_filter\", LowpassFilter()),\n",
    "    #         ('neg', ReverseComponent3()),\n",
    "    #         (\"binning\", BinTransformer(step=12)),\n",
    "    #         (\"baseline\", ErnBaselined()),\n",
    "    #         (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "    #     ern_features_pre = Pipeline(steps=[\n",
    "    #                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "    #                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "    #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "    #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "    #                                 (\"scaler\", StandardScaler()),\n",
    "    #                 ]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "    #     ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "    #     pe_features_pre = Pipeline(steps = [\n",
    "    #                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "    #                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "    #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "    #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "    #                                 (\"scaler\", StandardScaler()),\n",
    "    #                 ]).fit_transform(x_pre_pre)\n",
    "\n",
    "    #     pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "\n",
    "    #     x_pre2 = zip(ern_features, pe_features)\n",
    "    #     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)     \n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)\n",
    "        \n",
    "        ##################################################################\n",
    "                \n",
    "    X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))\n",
    "\n",
    "    pipeline_name = '-500:-300 SF ' + str(cutoff) + ' no BS'\n",
    "\n",
    "\n",
    "    for i in range(0,len(X_train_df_500copy)):\n",
    "\n",
    "        X = X_train_df_500copy[i:i+1]    \n",
    "        x_pre = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                ('extract_data', ExtractDataEpochs()),\n",
    "        ]).fit_transform(X)  \n",
    "        x_pre = x_pre[0] \n",
    "\n",
    "\n",
    "\n",
    "        x_pre_pre = Pipeline([\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\", spatial_filter),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "            (\"lowpass_filter\", LowpassFilter(cutoff=cutoff)),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "        ]).transform(x_pre)\n",
    "\n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                                    (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "                                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                                    # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                                    # (\"postprocessing\", PostprocessingTransformer()),\n",
    "                                    # (\"scaler\", StandardScaler()),\n",
    "                                    # (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "                    ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "        x_pre2 = Pipeline([(\"features\", ern_pe_features)]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "    #     x_pre_pre = Pipeline([\n",
    "    #         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "    #         (\"spatial_filter\", spatial_filter),\n",
    "    #         (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "    #         (\"lowpass_filter\", LowpassFilter()),\n",
    "    #         ('neg', ReverseComponent3()),\n",
    "    #         (\"binning\", BinTransformer(step=12)),\n",
    "    #         (\"baseline\", ErnBaselined()),\n",
    "    #         (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "\n",
    "    #     ern_features_pre = Pipeline(steps=[\n",
    "    #                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "    #                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "    #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "    #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "    #                                 (\"scaler\", StandardScaler()),\n",
    "    #                 ]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "    #     ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "    #     pe_features_pre = Pipeline(steps = [\n",
    "    #                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "    #                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "    #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "    #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "    #                                 (\"scaler\", StandardScaler()),\n",
    "    #                 ]).fit_transform(x_pre_pre)\n",
    "\n",
    "    #     pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "\n",
    "\n",
    "    #     x_pre2 = zip(ern_features, pe_features)\n",
    "    #     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "\n",
    "\n",
    "        # f_vector = np.mean(x_pre2, axis=1)\n",
    "        f_vector = np.sum(x_pre2, axis=1)     \n",
    "        f_variance = np.std(f_vector)\n",
    "\n",
    "        data = {'pipeline' : pipeline_name,\n",
    "                'values' : f_variance,\n",
    "               }\n",
    "\n",
    "        within_df = within_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0178ed8-953a-43ac-b55f-439a9570834a",
   "metadata": {},
   "source": [
    "- with SF and FEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d944d58-6674-4a72-872f-286a84d1efd1",
   "metadata": {},
   "source": [
    "### -100 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963ad9e-c503-446c-aba1-4de09c3c8a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736e65a-ad72-4cc2-b481-3a73ed06ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_pip_for_spatial_filter = Pipeline([\n",
    "#         (\"channels_extraction\",PickChannels(channels_list = box)),\n",
    "#         (\"average\", Evoked()),\n",
    "#         ('extract_averaged_data', ExtractData()),\n",
    "#         # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "#         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "# ]).fit(X_train_df_100copy)\n",
    "\n",
    "# spatial_filter = pre_pip_for_spatial_filter['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d55ccb-b177-43f9-ab8e-e2f21405ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_100copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802e617-f7f9-47cd-a5dd-9354b72d3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-100:0 SF EX'\n",
    "\n",
    "# for i in range(0,len(X_train_df_100copy)):\n",
    "    \n",
    "#     X = X_train_df_100copy[i:i+1]    \n",
    "#     x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             ('extract_data', ExtractDataEpochs()),\n",
    "#     ]).fit_transform(X)  \n",
    "    \n",
    "#     x_pre = x_pre[0] \n",
    "    \n",
    "#     x_pre_pre = Pipeline([\n",
    "#         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         (\"spatial_filter\", spatial_filter),\n",
    "#         (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#         (\"lowpass_filter\", LowpassFilter()),\n",
    "#         ('neg', ReverseComponent3()),\n",
    "#         (\"binning\", BinTransformer(step=12)),\n",
    "#         (\"baseline\", ErnBaselined()),\n",
    "#         (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "    \n",
    "#     ern_features_pre = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                 ]).fit_transform(x_pre_pre)\n",
    "        \n",
    "#     ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "#     pe_features_pre = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                 ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "#     pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "        \n",
    "#     x_pre2 = zip(ern_features, pe_features)\n",
    "#     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "    \n",
    "#     # print(x_pre2.shape)\n",
    "\n",
    "#     # ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "    \n",
    "#     # x_pre2 = Pipeline([\n",
    "#     #     (\"features\", ern_pe_features),\n",
    "#     # ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "#     # f_vector = np.mean(x_pre2, axis=1)\n",
    "#     f_vector = np.sum(x_pre2, axis=1)\n",
    "    \n",
    "#     f_variance = np.std(f_vector)\n",
    "    \n",
    "#     data = {'pipeline' : pipeline_name,\n",
    "#             'values' : f_variance,\n",
    "#            }\n",
    "    \n",
    "#     within_df = within_df.append(data, ignore_index = True)\n",
    "    \n",
    "    \n",
    "# #     variances_100_sf.append(f_variance)\n",
    "    \n",
    "# # variances_100_sf = np.array(variances_100_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c162ca-88be-4a78-a575-ccea414858a2",
   "metadata": {},
   "source": [
    "### -500 to -300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46ef21-b854-474d-ad1b-e58875ff28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6653ec9-2160-4892-870d-a13efb422bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_pip_for_spatial_filter = Pipeline([\n",
    "#         (\"channels_extraction\",PickChannels(channels_list = box)),\n",
    "#         (\"average\", Evoked()),\n",
    "#         ('extract_averaged_data', ExtractData()),\n",
    "#         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "# ]).fit(X_train_df_500copy)\n",
    "\n",
    "# spatial_filter = pre_pip_for_spatial_filter['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddabf8-a497-4f85-8e1b-4a54bc7697d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_df_500copy = pd.DataFrame(copy.deepcopy(X_train_df_500.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea3d9c-9d02-4c1f-99bc-b35f0f863558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = '-500:-300 SF EX'\n",
    "\n",
    "# for i in range(0,len(X_train_df_500copy)):\n",
    "    \n",
    "#     X = X_train_df_500copy[i:i+1]    \n",
    "#     x_pre = Pipeline([\n",
    "#             (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#             ('extract_data', ExtractDataEpochs()),\n",
    "#     ]).fit_transform(X)  \n",
    "#     x_pre = x_pre[0] \n",
    "\n",
    "    \n",
    "    \n",
    "# #     x_pre_pre = Pipeline([\n",
    "# #         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "# #         (\"spatial_filter\", spatial_filter),\n",
    "# #         (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "# #         (\"lowpass_filter\", LowpassFilter()),\n",
    "# #         ('neg', ReverseComponent3()),\n",
    "# #         (\"binning\", BinTransformer(step=12)),\n",
    "# #         (\"baseline\", ErnBaselined()),\n",
    "# #         (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "# #     ]).transform(x_pre)\n",
    "    \n",
    "    \n",
    "# #     ern_features = Pipeline(steps=[\n",
    "# #                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "# #                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "# #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                                 (\"scaler\", StandardScaler()),\n",
    "# #                                 (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "# #                 ])\n",
    "\n",
    "\n",
    "# #     pe_features = Pipeline(steps = [\n",
    "# #                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "# #                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "# #                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                                 (\"scaler\", StandardScaler()),\n",
    "# #                                 (\"feature_extraction\", FastICA(random_state=random_state, n_components=3))\n",
    "# #                 ])\n",
    "\n",
    "# #     ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "    \n",
    "# #     x_pre2 = Pipeline([(\"features\", ern_pe_features)]).fit_transform(x_pre_pre)\n",
    "\n",
    "\n",
    "#     x_pre_pre = Pipeline([\n",
    "#         (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         (\"spatial_filter\", spatial_filter),\n",
    "#         (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#         (\"lowpass_filter\", LowpassFilter()),\n",
    "#         ('neg', ReverseComponent3()),\n",
    "#         (\"binning\", BinTransformer(step=12)),\n",
    "#         (\"baseline\", ErnBaselined()),\n",
    "#         (\"centering\", CenteredSignalAfterBaseline3())]).transform(x_pre)\n",
    "\n",
    "    \n",
    "#     ern_features_pre = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                 ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "    \n",
    "#     ern_features = Pipeline([(\"feature_extraction\", ern_fex)]).transform(ern_features_pre)\n",
    "\n",
    "\n",
    "#     pe_features_pre = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=3, stop_pe_bin=8)),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 # (\"scaler\", StandardScaler()),\n",
    "#                 ]).fit_transform(x_pre_pre)\n",
    "    \n",
    "#     pe_features = Pipeline([(\"feature_extraction\", pe_fex)]).transform(pe_features_pre)\n",
    "    \n",
    "    \n",
    "#     x_pre2 = zip(ern_features, pe_features)\n",
    "#     x_pre2 = np.array(list(x_pre2)).reshape(x_pre.shape[0],-1)\n",
    "    \n",
    "\n",
    "#     # f_vector = np.mean(x_pre2, axis=1)\n",
    "#     f_vector = np.sum(x_pre2, axis=1)     \n",
    "#     f_variance = np.std(f_vector)\n",
    "    \n",
    "#     data = {'pipeline' : pipeline_name,\n",
    "#             'values' : f_variance,\n",
    "#            }\n",
    "    \n",
    "#     within_df = within_df.append(data, ignore_index = True)\n",
    "# #     variances_500_sf.append(f_variance)\n",
    "    \n",
    "# # variances_500_sf = np.array(variances_500_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e22eb78-66cb-4a82-8f99-8390dd787ae6",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3737dbe-a664-4a90-ae8f-1291930c528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,12)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "ax = sns.violinplot(x = 'values', y = 'pipeline', data = within_df, orient='h', )\n",
    "# ax.figure.savefig(\"within_subject_std_ern_pe_with_lowpass_BS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e76de-d0cd-4631-9655-743cf0da16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,12)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "ax = sns.violinplot(x = 'values', y = 'pipeline', data = within_df, orient='h', )\n",
    "# ax.figure.savefig(\"within_subject_std_ern_pe_with_lowpass_BS.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090c94b-a945-4e59-b577-af7c353c9f8c",
   "metadata": {},
   "source": [
    "## Internal consistency\n",
    "\n",
    "consistency = betweenPerson / between_person + within_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d1f02-808b-45b9-81b7-5f60ebd86894",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_df = pd.DataFrame({'pipeline': [], 'internal_variance': []})\n",
    "\n",
    "for pipeline in between_df['pipeline'].unique().tolist():\n",
    "    \n",
    "    between_std = np.std(np.array(between_df.loc[between_df['pipeline'] == pipeline, 'values'].tolist()))                     \n",
    "    within_list = np.array(within_df.loc[within_df['pipeline'] == pipeline, 'values'].tolist())\n",
    "    \n",
    "    for person_variance in within_list:\n",
    "        \n",
    "        internal = between_std/(between_std + person_variance)    \n",
    "        data = {'pipeline' : pipeline,\n",
    "                'internal_variance' : internal,\n",
    "               }\n",
    "    \n",
    "        internal_df = internal_df.append(data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4dc4f-dbf9-4b00-996f-2a8a10342b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "ax = sns.violinplot(x = 'internal_variance', y = 'pipeline', data = internal_df, orient='h', inner=\"quartile\")\n",
    "# ax.figure.savefig(\"internal_consistency_ern_pe_lowpass_BS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5f85b-da98-4a7c-a545-1de98d624830",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_df[(within_df['pipeline'] == '-100:0 SF') & (within_df['values'] > 0.00005)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e1a01-a5c8-4520-babd-ebc0abb9d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 182 == 0\n",
    "\n",
    "indexes = within_df[(within_df['pipeline'] == '-100:0 SF') & (within_df['values'] > 0.00005)].index\n",
    "indexes = np.array(indexes.tolist())\n",
    "\n",
    "indexes_new = [index - 196 for index in indexes]\n",
    "indexes_new\n",
    "# index 8, 19, 33, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f43282-a29d-49e2-84fc-965089f83f85",
   "metadata": {},
   "source": [
    "---\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcdfb3-a4bc-4c5b-8e99-b3ab5fce156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df_100.to_dict()))\n",
    "# X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d81fa9-377c-4b0f-8a4b-6e0812b75900",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = ['Fpz', 'AFz', 'Fz', 'FCz', 'C1', 'Cz', 'C2', 'CPz', 'P1', 'Pz', 'P2']\n",
    "red_box8_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d8306-c6c5-43db-ad1c-72d24c8e7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pip = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list = red_box8_prim)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "        (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        ('neg', ReverseComponent3()),\n",
    "        (\"binning\", BinTransformer(step=12)),\n",
    "        # (\"baseline\", ErnBaselined()),\n",
    "        (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "]).fit(X_train_df_copy)\n",
    "\n",
    "X = pre_pip.transform(X_train_df_copy)\n",
    "X_mean = np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910d763-3d55-4e28-8ff9-650f9291c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e2a9f-1477-4e57-8e84-cb89503828d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = pre_pip['spatial_filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe856a13-86c7-487d-bdb4-78444029f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = sf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a19723-dfc8-4b08-a8a6-3badfe3d4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_positive(number):\n",
    "    if number > 0:\n",
    "          return True  \n",
    "\n",
    "    return False\n",
    "\n",
    "def check_negative(number):\n",
    "    if number < 0:\n",
    "          return True  \n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b15f7e-84e5-4eb8-b496-c3ef5e9b3df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_copy = components.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c5665-a4b8-459d-b512-a8e657205af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a68b04-4bad-45ee-9375-829bec5a1972",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_1 = [components_copy[1]]\n",
    "components_2 = [components_copy[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b80e2-4571-4ca3-9649-56575d174244",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ffd8e7-f451-4dc7-9ba4-a7bfda471957",
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in components_1:\n",
    "    print(component)\n",
    "\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for item in component:\n",
    "        if item > 0:\n",
    "            positive.append(item)\n",
    "            negative.append(0)\n",
    "        else:\n",
    "            positive.append(0)\n",
    "            negative.append(item)\n",
    "    print(positive)\n",
    "    print(negative)\n",
    "    \n",
    "positive = np.array(positive).reshape(11,-1)\n",
    "negative = np.array(negative).reshape(11,-1)\n",
    "\n",
    "positive_compo = X_mean * positive\n",
    "negative_compo = X_mean * negative\n",
    "\n",
    "positive_signal_1 = np.sum(positive_compo, axis=0)\n",
    "negative_signal_1 = np.sum(negative_compo, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13788d11-4c19-4bf9-8911-75f78d32c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in components_2:\n",
    "    print(component)\n",
    "\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for item in component:\n",
    "        if item > 0:\n",
    "            positive.append(item)\n",
    "            negative.append(0)\n",
    "        else:\n",
    "            positive.append(0)\n",
    "            negative.append(item)\n",
    "    print(positive)\n",
    "    print(negative)\n",
    "    \n",
    "positive = np.array(positive).reshape(11,-1)\n",
    "negative = np.array(negative).reshape(11,-1)\n",
    "\n",
    "positive_compo = X_mean * positive\n",
    "negative_compo = X_mean * negative\n",
    "\n",
    "positive_signal_2 = np.sum(positive_compo, axis=0)\n",
    "negative_signal_2 = np.sum(negative_compo, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a3850-3000-4a57-ae91-79e4e0935e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_1 = positive_signal_1 + negative_signal_1\n",
    "c_2 = positive_signal_2 + negative_signal_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91252e-da53-47ed-8d5c-502ada6ee2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee5e17-f111-4dd8-9ffd-bbe31e25ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(positive_signal_2)\n",
    "plt.plot(-negative_signal_2)\n",
    "\n",
    "# plt.plot(X_mean[1], lw=4)\n",
    "plt.plot(c_2, lw = 4)\n",
    "plt.savefig(\"differences_component_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af943d-026d-4cc1-b251-9fcb6313b7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aec0a3-71a6-44b0-9650-c3f19674dd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fdf95-311b-49a3-8986-7672d57b0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.plot(X_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6b6a6-36d6-418d-8b87-8caa71d0feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,10):\n",
    "    plt.plot(X[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a11b97-6817-4a7d-8b08-3d587fa77d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,10):\n",
    "    plt.plot(X[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf4bab-f079-4c8d-9ec2-2890d085221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,10):\n",
    "    plt.plot(X[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efcd2b-cf3b-4a1c-814b-2e0190d4181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,20):\n",
    "    plt.plot(X[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebfcbd-7384-4da8-b7a5-860dfd8dbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,4):\n",
    "    plt.plot(X[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed92d8-c44e-4e6c-b0d1-45cf6af771d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bez lowpasss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,4):\n",
    "    plt.plot(X[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f879f6-c3ec-4c89-975c-a37162b216dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,4):\n",
    "    plt.plot(X[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a266fc5-e2bd-4eeb-b7ad-c209267eeb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,4):\n",
    "    plt.plot(X[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931bd8b-511b-4111-a775-8db9ec102071",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1 no CDS\n",
    "\n",
    "- channel extraction to red box\n",
    "- choose error and average\n",
    "- baseline to positivity peak\n",
    "- center to ERN from 1 component\n",
    "- split on ERN and PE\n",
    "- spatial filter with PCA\n",
    "- feature extraction with ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f6e53-35a9-496a-844b-334e44c4dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters of pipeline\n",
    "timepoints_count = 181\n",
    "\n",
    "min_spatial_filter = 3\n",
    "max_spatial_filter = 6\n",
    "step_spatial_filter = 1\n",
    "\n",
    "min_feature_selection = 2\n",
    "max_feature_selection = 6\n",
    "step_feature_selection = 1\n",
    "\n",
    "\n",
    "# define proper parameters for training. In this case define range of number of feature extraction to search\n",
    "regressor_params = dict(\n",
    "    # features__ern_features__feature_extraction__n_components=np.arange(\n",
    "    #     min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    # ),\n",
    "    # features__pe_features__feature_extraction__n_components=np.arange(\n",
    "    #     min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    # ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd012625-00b4-44b0-8f55-f2fcef6ec591",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_box = [\n",
    "    \"F3\",\"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\",\"Fpz\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "# bez Fpz - no significant\n",
    "red_box2 = [\n",
    "    \"F3\",\"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "\n",
    "# bez Fpz i z dodanym F1 - no sognificant\n",
    "red_box3 = [\n",
    "    \"F3\",\"F1\",\"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box4 = [\n",
    "    \"Fpz\",\n",
    "    \"F3\",\"F1\",\"Fz\", \"F2\", \"F4\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\",\"Cz\",\"C2\",\"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box4_prim = [\n",
    "    \"Fpz\",\n",
    "    \"F1\",\"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"C1\",\"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"P2\",\n",
    "]\n",
    "\n",
    "#\n",
    "red_box5 = [\n",
    "    \"AFz\", \n",
    "    \"F3\",\"F1\",\"Fz\", \"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box6 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"Pz\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu\n",
    "red_box7 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box7_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu i na środku\n",
    "red_box8 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\", \"Cz\",\"C2\", \"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\"\n",
    "]\n",
    "\n",
    "red_box8_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu i na środku i na poczatku\n",
    "red_box9 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F3\", \"F1\", \"Fz\", \"F2\", \"F4\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\", \"Cz\",\"C2\", \"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box9_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F1\", \"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "red_box10 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F3\", \"F1\", \"Fz\", \"F2\", \"F4\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CP3\", \"CP1\", \"CPz\", \"CP2\", \"CP4\",\n",
    "    \"Pz\",\n",
    "]\n",
    "\n",
    "red_box10_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F1\", \"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CP1\", \"CPz\", \"CP2\",\n",
    "    \"Pz\",\n",
    "]\n",
    "\n",
    "\n",
    "box_list = [red_box6, red_box7_prim, red_box8_prim, red_box9_prim, red_box10_prim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02deb63b-06f2-4e3f-839e-656cf305045a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wider signal, ERN: -2,1 Pe: 1,6\n",
    "\n",
    "import copy\n",
    "\n",
    "results_baseline_100 = pd.DataFrame()\n",
    "\n",
    "# manually test different numbers of spatial filter components\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "    \n",
    "# pe_indices = [(3,8)]\n",
    "# for this_bin in [12]:\n",
    "    # for ind in pe_indices:\n",
    "        # print(f\"----------PE INDICES: {ind}-------------------------------------------------\")\n",
    "\n",
    "for box in box_list:\n",
    "\n",
    "    X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "    X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "\n",
    "    print(f\"----------BOX: {box}\")\n",
    "\n",
    "    for n_components in range(min_spatial_filter, 4, step_spatial_filter): \n",
    "\n",
    "        pipeline_name = f\"PCA_{n_components}_baseline_100_0\"\n",
    "\n",
    "        ############################################################################################\n",
    "        preprocessed_pipeline = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"average\", Evoked()),\n",
    "            ('extract_averaged_data', ExtractData()),\n",
    "            # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "            (\"lowpass_filter\", LowpassFilter()),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()) \n",
    "\n",
    "        ]).fit(X_train_df_copy)\n",
    "\n",
    "        preprocessed_X = preprocessed_pipeline.transform(X_train_df_copy)\n",
    "\n",
    "        ##########################################################################################\n",
    "\n",
    "        ern_first_comp = Pipeline(steps=[\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"ern_data_extraction\", ErnTransformer()),\n",
    "            (\"get_first\", GetComponent(0)),\n",
    "            (\"ern_peak_to_peak\", ErnAmplitude2()),\n",
    "        ])\n",
    "\n",
    "        ern_second_comp = Pipeline(steps=[\n",
    "            (\"ern_data_extraction\", ErnTransformer()),\n",
    "            (\"get_second\", GetComponent(1)),\n",
    "            (\"diff_ern_peak\", ErnAmplitude2_prim()),\n",
    "        ])\n",
    "\n",
    "        # ern_third_comp = Pipeline(steps=[\n",
    "        #     (\"ern_data_extraction\", ErnTransformer(0,4)),\n",
    "        #     (\"get_third\", GetComponent(2)),\n",
    "        #     (\"second_diff_ern_peak\", ErnAmplitude2()),\n",
    "        # ])\n",
    "\n",
    "        ern_compo_features = FeatureUnion([(\"first\", ern_first_comp), \n",
    "                                           (\"second\", ern_second_comp),\n",
    "                                           # (\"third\", ern_third_comp),\n",
    "                                          ],n_jobs = 10)\n",
    "\n",
    "\n",
    "        ern_features = Pipeline(steps=[\n",
    "                        # (\"ern_data_extraction\", ErnTransformer()),\n",
    "                        # (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                        (\"ern_compo_features\", ern_compo_features),\n",
    "                        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                        (\"postprocessing\", PostprocessingTransformer()),\n",
    "                        (\"scaler\", StandardScaler()),\n",
    "                        # (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "        ])\n",
    "\n",
    "        ####\n",
    "\n",
    "        pe_first_comp = Pipeline(steps=[\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"ern_data_extraction\", PeTransformer(2,7)),\n",
    "            (\"get_first\", GetComponent(0)),\n",
    "            (\"ern_peak_to_peak\", PeAmplitude2()),\n",
    "        ])\n",
    "\n",
    "        pe_second_comp = Pipeline(steps=[\n",
    "            (\"ern_data_extraction\", PeTransformer(3,8)),\n",
    "            (\"get_second\", GetComponent(1)),\n",
    "            (\"diff_ern_peak\", PeAmplitude2()),\n",
    "        ])\n",
    "\n",
    "        pe_third_comp = Pipeline(steps=[\n",
    "            (\"ern_data_extraction\", PeTransformer(3,8)),\n",
    "            (\"get_third\", GetComponent(2)),\n",
    "            (\"second_diff_ern_peak\", PeAmplitude2()),\n",
    "        ])\n",
    "\n",
    "        pe_compo_features = FeatureUnion([(\"first\", pe_first_comp), \n",
    "                                           (\"second\", pe_second_comp),\n",
    "                                           (\"third\", pe_third_comp),\n",
    "                                          ],n_jobs = 10)\n",
    "\n",
    "\n",
    "\n",
    "        pe_features = Pipeline(steps = [\n",
    "                        # (\"pe_data_extraction\", PeTransformer(start_pe_bin=ind[0], stop_pe_bin=ind[1])),\n",
    "                        # (\"pe_amplitude\", PeAmplitude2()),\n",
    "                        (\"pe_compo_features\", pe_compo_features),\n",
    "                        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                        (\"postprocessing\", PostprocessingTransformer()),\n",
    "                        (\"scaler\", StandardScaler()),\n",
    "                        # (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "        ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "        steps = ('features', ern_pe_features)\n",
    "        # steps = ('features', ern_features)\n",
    "\n",
    "        ############################################################################################\n",
    "\n",
    "        regressor_steps = steps\n",
    "\n",
    "        # rate different models\n",
    "        results_baseline_100 = run_experiment(\n",
    "            tested_regressors,\n",
    "            regressor_params,\n",
    "            pipeline_name,\n",
    "            preprocessed_X,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            dataset_name,\n",
    "            regressor_steps,\n",
    "            preprocessed_pipeline,\n",
    "            X_test_df_copy,\n",
    "            y_rum_test,\n",
    "            results_baseline_100,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae7677-da40-4c11-8a78-06d9b85f0828",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_baseline_100.to_pickle(\"../data/regression_union_100-600_cached_splited_compos_ern-compo2-max_pe-compo2-max_diff_models_no-fex_mne.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f2fe2-52ba-47f0-959a-9cc864f90de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wider signal, ERN: -2,1 Pe: 1,6\n",
    "\n",
    "# import copy\n",
    "\n",
    "# results_baseline_100 = pd.DataFrame()\n",
    "\n",
    "# # manually test different numbers of spatial filter components\n",
    "\n",
    "# if not sys.warnoptions:\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "#     os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "    \n",
    "# pe_indices = [(3,8)]\n",
    "# for this_bin in [12]:\n",
    "#     for ind in pe_indices:\n",
    "#         print(f\"----------PE INDICES: {ind}-------------------------------------------------\")\n",
    "\n",
    "#         for box in box_list:\n",
    "\n",
    "#             X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "#             X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "\n",
    "#             print(f\"----------BOX: {box}\")\n",
    "\n",
    "#             for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter): \n",
    "\n",
    "#                 pipeline_name = f\"PCA_{n_components}_baseline_500_300\"\n",
    "\n",
    "#                 ############################################################################################\n",
    "#                 preprocessed_pipeline = Pipeline([\n",
    "#                     (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "#                     (\"average\", Evoked()),\n",
    "#                     ('extract_averaged_data', ExtractData()),\n",
    "#                     # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "#                     (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#                     (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "#                     (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "#                     (\"lowpass_filter\", LowpassFilter()),\n",
    "#                     ('neg', ReverseComponent3()),\n",
    "#                     (\"binning\", BinTransformer(step=this_bin)),\n",
    "#                     (\"baseline\", ErnBaselined()),\n",
    "#                     (\"centering\", CenteredSignalAfterBaseline3()) \n",
    "\n",
    "#                 ]).fit(X_train_df_copy)\n",
    "\n",
    "#                 preprocessed_X = preprocessed_pipeline.transform(X_train_df_copy)\n",
    "\n",
    "#                 ###########################################################################################\n",
    "\n",
    "#                 ern_features = Pipeline(steps=[\n",
    "#                                 (\"ern_data_extraction\", ErnTransformer()),\n",
    "#                                 (\"ern_amplitude\", ErnAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 (\"scaler\", StandardScaler()),\n",
    "#                                 # (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "#                 ])\n",
    "\n",
    "\n",
    "#                 pe_features = Pipeline(steps = [\n",
    "#                                 (\"pe_data_extraction\", PeTransformer(start_pe_bin=ind[0], stop_pe_bin=ind[1])),\n",
    "#                                 (\"pe_amplitude\", PeAmplitude2()),\n",
    "#                                 (\"data_channel_swap\", ChannelDataSwap()),\n",
    "#                                 (\"postprocessing\", PostprocessingTransformer()),\n",
    "#                                 (\"scaler\", StandardScaler()),\n",
    "#                                 # (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "#                 ])\n",
    "\n",
    "#                 ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "#                 steps = ('features', ern_pe_features)\n",
    "\n",
    "#                 ############################################################################################\n",
    "\n",
    "#                 regressor_steps = steps\n",
    "\n",
    "#                 # rate different models\n",
    "#                 results_baseline_100 = run_experiment(\n",
    "#                     tested_regressors,\n",
    "#                     regressor_params,\n",
    "#                     pipeline_name,\n",
    "#                     preprocessed_X,\n",
    "#                     X_test,\n",
    "#                     y_train,\n",
    "#                     y_test,\n",
    "#                     dataset_name,\n",
    "#                     regressor_steps,\n",
    "#                     preprocessed_pipeline,\n",
    "#                     X_test_df_copy,\n",
    "#                     y_rum_test,\n",
    "#                     results_baseline_100,\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5047d74-933e-4599-8df4-eaa414abfd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_baseline_100_no_fex_df.to_pickle(\"../data/regression_union_100-600_cached_baselined_centered_diff_boxes_diff_pe-ind_diff_models_no-fex_mne.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ede838-ee97-4a2c-9822-9512c5ece755",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_copy_best = results_copy.loc[(results_copy['mean_cv_r2'] > 0.08) & (results_copy['external_score'] > 0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca789f4b-f04a-4a85-b44f-892ab259ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_copy_best.sort_values(by=['overfit'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1b379-3686-4412-b07c-bea6629d2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_rum = pd.read_pickle(\"../data/split0.3/regression_union_100-600_baselined_centered-2_diff_boxes_diff_pe-ind_diff_models.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60152c4-f1b3-4210-9202-53eb287e0bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee7449-0101-4e74-9223-447c6611956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_rum = pd.read_pickle(\"../data/split0.3/regression_union_100-600_baselined_centered-2_diff_boxes_diff_pe-ind_diff_models.pkl\")\n",
    "\n",
    "results_df_rum[results_df_rum['model'] == 'en']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4188e-1073-4b89-9ff9-e1e4b0f145ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_fex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ab444-6d3d-4093-9a51-900cd72027a1",
   "metadata": {},
   "source": [
    "# Experiment 4 with CDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b2dea-fd07-4fed-8139-de7235c47b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e900885-4a75-4ba3-b57f-89e2eb148a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters of pipeline\n",
    "timepoints_count = 181\n",
    "\n",
    "min_spatial_filter = 3\n",
    "max_spatial_filter = 7\n",
    "step_spatial_filter = 1\n",
    "\n",
    "min_feature_selection = 2\n",
    "max_feature_selection = 6\n",
    "step_feature_selection = 1\n",
    "\n",
    "\n",
    "# define proper parameters for training. In this case define range of number of feature extraction to search\n",
    "regressor_params = dict(\n",
    "    features__feature_extraction__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37343e-094f-4190-80aa-1633251ed6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "\n",
    "preprocessed_X_train = Pipeline([\n",
    "        ('current_source_density', CurrentSourceDensity()),\n",
    "]).fit_transform(X_train_df_copy)\n",
    "\n",
    "preprocessed_X_test = Pipeline([\n",
    "        ('current_source_density', CurrentSourceDensity()),\n",
    "]).fit_transform(X_test_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c3704-67cf-48cc-a41f-2a836548fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole signal centered to ERN (from - 2) and abs()\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "results_df_cds = pd.DataFrame()\n",
    "\n",
    "# manually test different numbers of spatial filter components\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "for box in box_list:\n",
    "\n",
    "    # X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "    # X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "    \n",
    "    preprocessed_X_train_copy = pd.DataFrame(copy.deepcopy(preprocessed_X_train.to_dict()))\n",
    "    preprocessed_X_test_copy = pd.DataFrame(copy.deepcopy(preprocessed_X_test.to_dict()))\n",
    "    \n",
    "    print(f\"BOX: {box}\")\n",
    "\n",
    "    for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter): \n",
    "\n",
    "        pipeline_name = f\"PCA_{n_components}_CDS_SF_SPLIT_FE\"\n",
    "\n",
    "        ############################################################################################\n",
    "        preprocessed_pipeline = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"average\", Evoked()),\n",
    "            # (\"bandpass_filter\",BandpassFilter()),\n",
    "            ('extract_averaged_data', ExtractData()),\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "            (\"lowpass_filter\", LowpassFilter()),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "        ]).fit(preprocessed_X_train_copy)\n",
    "\n",
    "        preprocessed_X = preprocessed_pipeline.transform(preprocessed_X_train_copy)\n",
    "\n",
    "        ###########################################################################################\n",
    "\n",
    "        ern_pe_features = Pipeline([\n",
    "            # (\"abs\", AbsSignal()),\n",
    "            (\"data_channel_swap\", ChannelDataSwap()),\n",
    "            (\"postprocessing\", PostprocessingTransformer()),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "            ])\n",
    "\n",
    "        steps = ('features', ern_pe_features)\n",
    "\n",
    "        ############################################################################################\n",
    "\n",
    "        regressor_steps = steps\n",
    "\n",
    "        # rate different models\n",
    "        results_df_cds = run_experiment(\n",
    "            tested_regressors,\n",
    "            regressor_params,\n",
    "            pipeline_name,\n",
    "            preprocessed_X,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            dataset_name,\n",
    "            regressor_steps,\n",
    "            preprocessed_pipeline,\n",
    "            preprocessed_X_test_copy,\n",
    "            y_rum_test,\n",
    "            results_df_cds,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c2b2f9-2daf-464e-90df-666e6862427f",
   "metadata": {},
   "source": [
    "# Experiment 4 no CDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8da33-8b74-48fe-bac5-4d7ee9a34ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole signal centered to ERN (from - 2) and abs()\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# manually test different numbers of spatial filter components\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "for box in box_list:\n",
    "\n",
    "    X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "    X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "    \n",
    "    # preprocessed_X_train_copy = pd.DataFrame(copy.deepcopy(preprocessed_X_train.to_dict()))\n",
    "    # preprocessed_X_test_copy = pd.DataFrame(copy.deepcopy(preprocessed_X_test.to_dict()))\n",
    "    \n",
    "    print(f\"BOX: {box}\")\n",
    "\n",
    "    for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter): \n",
    "\n",
    "        pipeline_name = f\"PCA_{n_components}_CDS_SF_SPLIT_FE\"\n",
    "\n",
    "        ############################################################################################\n",
    "        preprocessed_pipeline = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"average\", Evoked()),\n",
    "            # (\"bandpass_filter\",BandpassFilter()),\n",
    "            ('extract_averaged_data', ExtractData()),\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "            (\"lowpass_filter\", LowpassFilter()),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "        ]).fit(X_train_df_copy)\n",
    "\n",
    "        preprocessed_X = preprocessed_pipeline.transform(X_train_df_copy)\n",
    "\n",
    "        ###########################################################################################\n",
    "\n",
    "        ern_pe_features = Pipeline([\n",
    "            # (\"abs\", AbsSignal()),\n",
    "            (\"data_channel_swap\", ChannelDataSwap()),\n",
    "            (\"postprocessing\", PostprocessingTransformer()),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "            ])\n",
    "\n",
    "        steps = ('features', ern_pe_features)\n",
    "\n",
    "        ############################################################################################\n",
    "\n",
    "        regressor_steps = steps\n",
    "\n",
    "        # rate different models\n",
    "        results_df = run_experiment(\n",
    "            tested_regressors,\n",
    "            regressor_params,\n",
    "            pipeline_name,\n",
    "            preprocessed_X,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            dataset_name,\n",
    "            regressor_steps,\n",
    "            preprocessed_pipeline,\n",
    "            X_test_df_copy,\n",
    "            y_rum_test,\n",
    "            results_df,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c81eb5-2612-4b2e-9118-a7ed794f0cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63df9f58-5edb-474b-83e9-ebec3ac2741a",
   "metadata": {},
   "source": [
    "# Experiment 4\n",
    "- binning\n",
    "- extracted baseline: (-1,1) where 0 is argmin from 0-3 bins\n",
    "- extracted ERN after centering as (-1,1) bins where 0 is ERN\n",
    "- extracted Pe as (1,6) where 0 is ERN\n",
    "- whole signal is abs\n",
    "- feature extraction separately on extracted signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a856997-5408-4471-bbe8-6a607d296f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_box = [\n",
    "    \"F3\",\"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\",\"Fpz\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "# bez Fpz - no significant\n",
    "red_box2 = [\n",
    "    \"F3\",\"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "\n",
    "# bez Fpz i z dodanym F1 - no sognificant\n",
    "red_box3 = [\n",
    "    \"F3\",\"F1\",\"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box4 = [\n",
    "    \"Fpz\",\n",
    "    \"F3\",\"F1\",\"Fz\", \"F2\", \"F4\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\",\"Cz\",\"C2\",\"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box4_prim = [\n",
    "    \"Fpz\",\n",
    "    \"F1\",\"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"C1\",\"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"P2\",\n",
    "]\n",
    "\n",
    "#\n",
    "red_box5 = [\n",
    "    \"AFz\", \n",
    "    \"F3\",\"F1\",\"Fz\", \"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box6 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"Pz\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu\n",
    "red_box7 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box7_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu i na środku\n",
    "red_box8 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\", \"Cz\",\"C2\", \"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\"\n",
    "]\n",
    "\n",
    "red_box8_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu i na środku i na poczatku\n",
    "red_box9 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F3\", \"F1\", \"Fz\", \"F2\", \"F4\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\", \"Cz\",\"C2\", \"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box9_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F1\", \"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "red_box10 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F3\", \"F1\", \"Fz\", \"F2\", \"F4\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CP3\", \"CP1\", \"CPz\", \"CP2\", \"CP4\",\n",
    "    \"Pz\",\n",
    "]\n",
    "\n",
    "red_box10_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F1\", \"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CP1\", \"CPz\", \"CP2\",\n",
    "    \"Pz\",\n",
    "]\n",
    "\n",
    "\n",
    "box_list = [red_box6, red_box7, red_box7_prim, red_box8, red_box8_prim, red_box9, red_box9_prim, red_box4, red_box4_prim, red_box10, red_box10_prim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a712bc-c547-4db2-aacc-0ce8f94d38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters of pipeline\n",
    "timepoints_count = 181\n",
    "\n",
    "min_spatial_filter = 3\n",
    "max_spatial_filter = 7\n",
    "step_spatial_filter = 1\n",
    "\n",
    "min_feature_selection = 2\n",
    "max_feature_selection = 6\n",
    "step_feature_selection = 1\n",
    "\n",
    "\n",
    "# define proper parameters for training. In this case define range of number of feature extraction to search\n",
    "regressor_params = dict(\n",
    "    features__ern_features__feature_extraction__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    "    features__pe_features__feature_extraction__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372b6da-81b9-4baa-8f56-550d717ca2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wider signal, ERN: -2,1 Pe: 2,6. , extract baseline no amplitude\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# manually test different numbers of spatial filter components\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "for box in box_list:\n",
    "\n",
    "    X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "    X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "    \n",
    "    print(f\"BOX: {box}\")\n",
    "\n",
    "    for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter): \n",
    "\n",
    "        pipeline_name = f\"PCA_{n_components}_CDS_SF_SPLIT_FE\"\n",
    "\n",
    "        ############################################################################################\n",
    "        preprocessed_pipeline = Pipeline([\n",
    "            (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "            (\"average\", Evoked()),\n",
    "            # (\"bandpass_filter\",BandpassFilter()),\n",
    "            ('extract_averaged_data', ExtractData()),\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "            (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "            (\"lowpass_filter\", LowpassFilter()),\n",
    "            ('neg', ReverseComponent3()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            # (\"baseline\", ErnBaselined()),\n",
    "            # (\"centering\", CenteredSignalAfterBaseline3()) \n",
    "\n",
    "        ]).fit(X_train_df_copy)\n",
    "\n",
    "        preprocessed_X = preprocessed_pipeline.transform(X_train_df_copy)\n",
    "\n",
    "        ###########################################################################################\n",
    "#extract 1,1 where 0 is ERN\n",
    "        ern_features = Pipeline(steps=[\n",
    "                        (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                        (\"ern_data_extraction\", ErnTransformer(start_ern_bin=1, stop_ern_bin=4)),\n",
    "                        # (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                        (\"abs\", AbsSignal()),\n",
    "                        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                        (\"postprocessing\", PostprocessingTransformer()),\n",
    "                        (\"scaler\", StandardScaler()),\n",
    "                        (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "        ])\n",
    "\n",
    "#extract 1,6 where 0 i ERN\n",
    "        pe_features = Pipeline(steps = [\n",
    "                        (\"centering\", CenteredSignalAfterBaseline3()),\n",
    "                        (\"pe_data_extraction\", PeTransformer()),\n",
    "                        (\"abs\", AbsSignal()),\n",
    "                        # (\"pe_amplitude\", PeAmplitude2()),\n",
    "                        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                        (\"postprocessing\", PostprocessingTransformer()),\n",
    "                        (\"scaler\", StandardScaler()),\n",
    "                        (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "        ])\n",
    "# extract -1,1 where 0 is min (0,4)        \n",
    "        base_features = Pipeline(steps = [\n",
    "                        (\"base_data_extraction\", ExtractBaseline()),\n",
    "                        # (\"pe_amplitude\", PeAmplitude2()),\n",
    "                        (\"abs\", AbsSignal()),\n",
    "                        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                        (\"postprocessing\", PostprocessingTransformer()),\n",
    "                        (\"scaler\", StandardScaler()),\n",
    "                        (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "        ])\n",
    "\n",
    "        ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features), ('base', base_features)], n_jobs = 10)\n",
    "\n",
    "        steps = ('features', ern_pe_features)\n",
    "\n",
    "        ############################################################################################\n",
    "\n",
    "        regressor_steps = steps\n",
    "\n",
    "        # rate different models\n",
    "        results_df = run_experiment(\n",
    "            tested_regressors,\n",
    "            regressor_params,\n",
    "            pipeline_name,\n",
    "            preprocessed_X,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            dataset_name,\n",
    "            regressor_steps,\n",
    "            preprocessed_pipeline,\n",
    "            X_test_df_copy,\n",
    "            y_rum_test,\n",
    "            results_df,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c8f1e-67b6-4ed4-baec-84c05219c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle(\"../data/regression_union_100-600_baselined_centered_diff_boxes_mne.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7199b25-e9c1-4076-b7fc-7e4efb177bea",
   "metadata": {},
   "source": [
    "---\n",
    "# With CDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36005b5-883a-45ab-8468-8e7b47579de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters of pipeline\n",
    "timepoints_count = 181\n",
    "\n",
    "min_spatial_filter = 3\n",
    "max_spatial_filter = 7\n",
    "step_spatial_filter = 1\n",
    "\n",
    "min_feature_selection = 2\n",
    "max_feature_selection = 6\n",
    "step_feature_selection = 1\n",
    "\n",
    "\n",
    "# define proper parameters for training. In this case define range of number of feature extraction to search\n",
    "regressor_params = dict(\n",
    "    features__ern_features__feature_extraction__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    "    features__pe_features__feature_extraction__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd261574-7848-4f7e-a364-88e934ceac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_box = [\n",
    "    \"F3\",\"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\",\"Fpz\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "# bez Fpz - no significant\n",
    "red_box2 = [\n",
    "    \"F3\",\"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "\n",
    "# bez Fpz i z dodanym F1 - no sognificant\n",
    "red_box3 = [\n",
    "    \"F3\",\"F1\",\"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box4 = [\n",
    "    \"Fpz\",\n",
    "    \"F3\",\"F1\",\"Fz\", \"F2\", \"F4\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\",\"Cz\",\"C2\",\"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box4_prim = [\n",
    "    \"Fpz\",\n",
    "    \"F1\",\"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"C1\",\"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"P2\",\n",
    "]\n",
    "\n",
    "#\n",
    "red_box5 = [\n",
    "    \"AFz\", \n",
    "    \"F3\",\"F1\",\"Fz\", \"F2\", \"F4\",\n",
    "    \"C1\",\"Cz\", \"FCz\",\n",
    "    \"C3\",\"C2\",\"C4\",\n",
    "    \"P1\", \"P2\",\n",
    "    \"P3\", \"CPz\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box6 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"Pz\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu\n",
    "red_box7 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box7_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu i na środku\n",
    "red_box8 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\", \"Cz\",\"C2\", \"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\"\n",
    "]\n",
    "\n",
    "red_box8_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "# linia i klaster z tyłu i na środku i na poczatku\n",
    "red_box9 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F3\", \"F1\", \"Fz\", \"F2\", \"F4\",\n",
    "    \"FCz\",\n",
    "    \"C3\", \"C1\", \"Cz\",\"C2\", \"C4\",\n",
    "    \"CPz\",\n",
    "    \"P3\", \"P1\", \"Pz\", \"P2\", \"P4\",\n",
    "]\n",
    "\n",
    "red_box9_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F1\", \"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "red_box10 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F3\", \"F1\", \"Fz\", \"F2\", \"F4\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CP3\", \"CP1\", \"CPz\", \"CP2\", \"CP4\",\n",
    "    \"Pz\",\n",
    "]\n",
    "\n",
    "red_box10_prim = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F1\", \"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CP1\", \"CPz\", \"CP2\",\n",
    "    \"Pz\",\n",
    "]\n",
    "\n",
    "\n",
    "box_list = [red_box6, red_box7, red_box7_prim, red_box8, red_box8_prim, red_box9, red_box9_prim, red_box4, red_box4_prim, red_box10, red_box10_prim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673476d-2ab4-4853-aed2-7ddf6280a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "\n",
    "preprocessed_X_train = Pipeline([\n",
    "        ('current_source_density', CurrentSourceDensity()),\n",
    "]).fit_transform(X_train_df_copy)\n",
    "\n",
    "preprocessed_X_test = Pipeline([\n",
    "        ('current_source_density', CurrentSourceDensity()),\n",
    "]).fit_transform(X_test_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8a414-e3ab-4dfc-85ea-e5c1af6cc0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669ded1-2ed5-4106-bff6-eb44e26637e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "results_df_2 = pd.DataFrame()\n",
    "\n",
    "# manually test different numbers of spatial filter components\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "for step in [7,12,16]:\n",
    "    print(f\"-----------------STEP: {step}-----------\")\n",
    "    for box in box_list:\n",
    "\n",
    "        preprocessed_X_train_copy = pd.DataFrame(copy.deepcopy(preprocessed_X_train.to_dict()))\n",
    "        preprocessed_X_test_copy = pd.DataFrame(copy.deepcopy(preprocessed_X_test.to_dict()))\n",
    "\n",
    "        print(f\"BOX: {box}\")\n",
    "\n",
    "        for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter): \n",
    "\n",
    "            pipeline_name = f\"PCA_{n_components}_CDS_SF_SPLIT_FE\"\n",
    "\n",
    "            ############################################################################################\n",
    "            preprocessed_pipeline = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=box)),\n",
    "                (\"average\", Evoked()),\n",
    "                # (\"bandpass_filter\",BandpassFilter()),\n",
    "                ('extract_averaged_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "                (\"lowpass_filter\", LowpassFilter()),\n",
    "                (\"binning\", BinTransformer(step=step)),\n",
    "                (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline()) \n",
    "\n",
    "            ]).fit(preprocessed_X_train_copy)\n",
    "\n",
    "            preprocessed_X = preprocessed_pipeline.transform(preprocessed_X_train_copy)\n",
    "\n",
    "            ###########################################################################################\n",
    "\n",
    "            ern_features = Pipeline(steps=[\n",
    "                            (\"ern_data_extraction\", ErnTransformer()),\n",
    "                            (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                            (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                            (\"postprocessing\", PostprocessingTransformer()),\n",
    "                            (\"scaler\", StandardScaler()),\n",
    "                            (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "            ])\n",
    "\n",
    "\n",
    "            pe_features = Pipeline(steps = [\n",
    "                            (\"pe_data_extraction\", PeTransformer()),\n",
    "                            (\"pe_amplitude\", PeAmplitude2()),\n",
    "                            (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                            (\"postprocessing\", PostprocessingTransformer()),\n",
    "                            (\"scaler\", StandardScaler()),\n",
    "                            (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "            ])\n",
    "\n",
    "            ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "            steps = ('features', ern_pe_features)\n",
    "\n",
    "            ############################################################################################\n",
    "\n",
    "            regressor_steps = steps\n",
    "\n",
    "            # rate different models\n",
    "            results_df_2 = run_experiment(\n",
    "                tested_regressors,\n",
    "                regressor_params,\n",
    "                pipeline_name,\n",
    "                preprocessed_X,\n",
    "                X_test,\n",
    "                y_train,\n",
    "                y_test,\n",
    "                dataset_name,\n",
    "                regressor_steps,\n",
    "                preprocessed_pipeline,\n",
    "                preprocessed_X_test_copy,\n",
    "                y_rum_test,\n",
    "                results_df_2,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e1c8f-2fb5-4612-9017-f9a629ace9f0",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cbb6b-8259-44a6-8ff6-949fe9d7e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.read_pickle(\n",
    "#     \"../data/split0.3/regression_union_100-600_baselined_centered_ampl-2-pe-ern_0.3-5_significant.pkl\"\n",
    "# )\n",
    "data_df = results_static_ICA_bin_union_100_600_baselined_peak_to_peak_components_df\n",
    "data_df.name = \"union_100_600_baselined_centered_no_scaler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90458d7d-3a55-479c-9c82-bba82801046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdc3c7-5692-47ba-8626-fe94ec398b0c",
   "metadata": {},
   "source": [
    "#### Extract coefficients of ERN and PE features extraction (ICA) and coefficient od estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42575b1-e957-4341-94bd-b2efb0604898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ern_features = data_df.best_estimator[1][\"features\"].transformer_list[0][1][\"feature_selection\"].components_\n",
    "# pe_features = data_df.best_estimator[1][\"features\"].transformer_list[1][1][\"feature_selection\"].components_\n",
    "\n",
    "# without additional metric as feature\n",
    "ern_features = data_df.best_estimator[0][\"features\"][\"ern_pe_features\"].transformer_list[0][1][\"feature_selection\"].components_\n",
    "pe_features = data_df.best_estimator[0][\"features\"][\"ern_pe_features\"].transformer_list[1][1][\"feature_selection\"].components_\n",
    "\n",
    "coeffs = data_df.best_estimator[0][\"en\"].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fc2ed-1541-4b69-88c2-3e1f3fa9333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ca93e-e420-401b-b38f-e356ec42a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea39cb-334d-4a4f-8312-e722e0b4bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26fe94-2a8e-4acb-8cda-9b6e0fc5e8c6",
   "metadata": {},
   "source": [
    "#### Weigh components with coeffs from estimator and sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591889e2-aa90-447e-afde-2385b2bee260",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_components_weighed = np.array([ern_features[i] * coeffs[i] for i in range(0,ern_features.shape[0])])\n",
    "pe_components_weighed = np.array([pe_features[i-ern_features.shape[0]] * coeffs[i] for i in range(ern_features.shape[0], ern_features.shape[0] + pe_features.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aefe62-2e54-405f-9b33-74523fd6fbe7",
   "metadata": {},
   "source": [
    "#### Sum all feature extraction components to extract direct weigh of given bin at given spatial filter component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279af91-4d5b-49ec-b4bf-44cf202bdca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_weighed_ern_sum = sum(ern_components_weighed)\n",
    "components_weighed_pe_sum = sum(pe_components_weighed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13104848-45d7-45f9-90d0-999642cbaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_weighed_ern_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3905a32-13c0-465f-af63-e7c7f8af4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rum_ern = components_weighed_ern_sum * ern_ampl_mean\n",
    "mean_rum_ern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc551b-646b-4155-bc5b-75f891cc378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rum_pe = components_weighed_pe_sum * pe_ampl_mean\n",
    "mean_rum_pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe975747-5514-4b6d-a7d0-5b7c9249c822",
   "metadata": {},
   "source": [
    "#### Extract components of spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e552573-ab6f-457c-a172-2cba7f5a6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_features = Pipeline(steps=[\n",
    "                (\"ern_data_extraction\", ErnTransformer()),\n",
    "                (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                (\"postprocessing\", PostprocessingTransformer()),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                # (\"feature_selection\", FastICA(random_state=random_state))\n",
    "])\n",
    "\n",
    "\n",
    "pe_features = Pipeline(steps = [\n",
    "                (\"pe_data_extraction\", PeTransformer()),\n",
    "                # (\"pe_centered\", CenteredPeAfterBaseline()),\n",
    "                (\"pe_amplitude\", PeAmplitude2()),\n",
    "                (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                (\"postprocessing\", PostprocessingTransformer()),\n",
    "                # (\"scaler\", StandardScaler()),\n",
    "                # (\"feature_selection\", FastICA(random_state=random_state))\n",
    "])\n",
    "\n",
    "ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "features = Pipeline([\n",
    "    ('ern_pe_features', ern_pe_features)\n",
    "\n",
    "])\n",
    "\n",
    "# steps = ('features', features)\n",
    "\n",
    "preprocessed_pipeline = Pipeline([(\"extract_epochs\", EEGdata(dataset=dataset)),\n",
    "    # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "    (\n",
    "        \"channels_filtering\",\n",
    "        ChannelExtraction(significant_channels)\n",
    "    ),\n",
    "    (\n",
    "        \"average_epochs\",\n",
    "        AveragePerParticipant(),\n",
    "    ),\n",
    "    (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "    (\n",
    "        \"spatial_filter\",\n",
    "        PCA(n_components=3, random_state=random_state),\n",
    "    ),\n",
    "    (\n",
    "        \"spatial_filter_postprocessing\",\n",
    "        SpatialFilterPostprocessing(\n",
    "            timepoints_count=181,\n",
    "        ),\n",
    "    ),\n",
    "    (\"lowpass_filter\", LowpassFilter()),\n",
    "    (\"binning\", BinTransformer(step=12)),\n",
    "    (\"baseline\", ErnBaselined()),\n",
    "    (\"centering\", CenteredSignalAfterBaseline()),\n",
    "    # ('features', features)\n",
    "\n",
    "                          ]).fit(X_train)\n",
    "preprocessed_X_test = preprocessed_pipeline.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0176e-1f92-4939-8aaa-3883ea50c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X = preprocessed_pipeline.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8784737-7a9a-45a8-a1b2-4a4281f31a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394cf0f-a7b8-4192-91b3-093be1d95197",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_ampl = preprocessed_X[:,3:6]\n",
    "pe_ampl_mean = np.mean(pe_ampl, axis=0)\n",
    "pe_ampl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6736c-5c08-437d-8e09-f93c907a4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_ampl = preprocessed_X[:,0:3]\n",
    "ern_ampl_mean = np.mean(ern_ampl, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a14b5-650c-46b2-89cc-07f49ac1b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_ampl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dd765-9269-410d-831a-cf7e85e71010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = ('features', features)\n",
    "preprocessed_pipeline = Pipeline([(\"extract_epochs\", EEGdata(dataset=dataset)),\n",
    "            # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "            (\n",
    "                \"channels_filtering\",\n",
    "                ChannelExtraction(significant_channels)\n",
    "            ),\n",
    "            (\n",
    "                \"average_epochs\",\n",
    "                AveragePerParticipant(),\n",
    "            ),\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\n",
    "                \"spatial_filter\",\n",
    "                PCA(n_components=3, random_state=random_state),\n",
    "            ),\n",
    "            (\n",
    "                \"spatial_filter_postprocessing\",\n",
    "                SpatialFilterPostprocessing(\n",
    "                    timepoints_count=181,\n",
    "                ),\n",
    "            ),\n",
    "            (\"lowpass_filter\", LowpassFilter()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline()),\n",
    "            ('ern_pe_features', ern_pe_features)\n",
    "                                  ]).fit(X_train)\n",
    "\n",
    "preprocessed_X = preprocessed_pipeline.transform(X_train)\n",
    "pre_processed_test_X = preprocessed_pipeline.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7e58b-f842-489e-863e-1e2f78d3ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ern_features = Pipeline(steps=[\n",
    "#                     (\"ern_extraction\", CenteredERN(step=16)),\n",
    "#                     (\"binning\", BinTransformer(step=16)),\n",
    "# #                     (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                     (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                     (\"scaler\", StandardScaler()),\n",
    "# #                     (\"feature_selection\", FastICA(random_state=random_state))\n",
    "# # \n",
    "# ])\n",
    "\n",
    "# pe_features = Pipeline(steps = [\n",
    "#                         (\"pe_extraction\", CenteredPe(step=16)),\n",
    "#                         (\"binning\", BinTransformer(step=16)),\n",
    "# #                         # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                         # (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                         # (\"scaler\", StandardScaler()),\n",
    "# #                         # (\"feature_selection\", FastICA(random_state=random_state))\n",
    "#         ])\n",
    "    \n",
    "# #         ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "# #         features = Pipeline([\n",
    "# #             ('ern_pe_features', ern_pe_features)\n",
    "\n",
    "# #         ])\n",
    "\n",
    "# #         steps = ('features', features)\n",
    "\n",
    "# ern_fitted = ern_features.fit_transform(preprocessed_X)\n",
    "# ern_test_fitted = ern_features.transform(pre_processed_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f195f5-64d8-4087-893e-1224bcba013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_fitted_mean = np.mean(ern_fitted, axis=0)\n",
    "ern_test_fitted_mean = np.mean(ern_test_fitted, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca03f64-94fd-465a-aa8d-45eea6d64eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_test_fitted_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a1499-e289-4e78-835d-e6f5fcfe56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ern_fitted_mean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac3ce80-9136-4bea-9680-193356081190",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb0ee3-7b85-4ee4-9afb-65aefd48565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filter_n_components = 3\n",
    "\n",
    "this_steps = spatial_filter_bins_steps(spatial_filter_n_components=spatial_filter_n_components, timepoints_count=181)\n",
    "pre_processed_X = Pipeline(steps=this_steps).fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b02a2-044a-4553-b3b5-69d0841d228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_test_X = preprocessed_pipeline.transform(X_test_df)\n",
    "pre_processed_X = preprocessed_pipeline.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3486f6-a736-48c1-b8ce-a1c0d99cfb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged signal within components through all participants\n",
    "mean_X_1 = np.mean(pre_processed_X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9cb59-d7a8-4d1d-b5cd-462943105f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_fitted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c8a68d-95f0-4c5b-8ada-7b021f8b823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd337e5-ad95-41be-aecb-f3906b10f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6757d7-cbc7-42a1-abec-a3cbde22dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged signal within components through all participants\n",
    "mean_X = np.mean(preprocessed_X, axis=0)\n",
    "mean_2_X = np.mean(pre_processed_test_X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6573c8-792b-4eba-a2ea-474177d202e7",
   "metadata": {},
   "source": [
    "-----\n",
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22a81b-c033-4672-93a7-f7003ed1dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices in bins\n",
    "\n",
    "tmin, tmax = -0.1, 0.6  # Start and end of the segments\n",
    "signal_frequency = 256\n",
    "\n",
    "step_in_ms = 50  # in miliseconds (?)\n",
    "step_tp = int(signal_frequency * step_in_ms / 1000) # in timepoints\n",
    "\n",
    "# indices for slicing epoch into ERN part and Pe part (in sec)\n",
    "start_ern = 0\n",
    "stop_ern = 0.15\n",
    "start_pe = 0.15\n",
    "stop_pe = 0.35\n",
    "\n",
    "start_ern_bin = int((signal_frequency * (start_ern - tmin)) / step_tp) + 1\n",
    "stop_ern_bin = int(signal_frequency * (stop_ern - tmin) / step_tp) + 1\n",
    "start_pe_bin = int(signal_frequency * (start_pe - tmin) / step_tp) + 1\n",
    "stop_pe_bin = int(signal_frequency * (stop_pe - tmin) / step_tp) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dbc2b-8302-4624-a55b-67bde67f741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ern_bin = 0\n",
    "stop_ern_bin = 3\n",
    "start_pe_bin = 3\n",
    "stop_pe_bin = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13efc2d-cf0c-4314-9917-19144e6cf4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filter_n_components = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88488c65-4b2a-4c04-a963-506505712f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0, 1 or 2\n",
    "this_component = 2\n",
    "\n",
    "# pe_step = int(pe_features.shape[1]/ spatial_filter_n_components)\n",
    "# ern_step = int(ern_features.shape[1]/ spatial_filter_n_components)\n",
    "# spatial_filter_step = int(pre_processed_X.shape[1]/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7c0a8-e843-4a1b-924e-ad9618f1b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filter_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310d788-67e6-4bd0-aac2-74dc87d23f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_components_weighed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a781ecdb-0d91-4437-9969-2d380b8df101",
   "metadata": {},
   "outputs": [],
   "source": [
    "-ern_fitted_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3a287-972a-4976-b72c-8857c162ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_components_weighed[0][0:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86258f07-0dde-46c7-8ed2-c6822dd21f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938b9f5-af49-4100-93f6-e15de0582164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# ax1 = plt.twinx()\n",
    "ax1.set(ylim=(np.min(ern_components_weighed)-0.1, np.max(pe_components_weighed)+0.05))\n",
    "ax1.tick_params(axis='y', color=\"magenta\", width=3, length=10)\n",
    "\n",
    "plt.axhline(y=0, color=\"grey\", linewidth = 2, linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=2, color=\"grey\", linewidth = 2, linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=6, color=\"grey\", linewidth = 2, linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(0,5):\n",
    "#     sns.lineplot(np.arange(0,3), ern_components_weighed[i][this_component:3], ax=ax1)\n",
    "\n",
    "# for i in range(0,pe_features.shape[0]):\n",
    "#     sns.scatterplot(np.arange(5,6), pe_components_weighed[i][this_component], ax=ax1)\n",
    "    \n",
    "\n",
    "ax2 = plt.twinx()\n",
    "ax2.set(ylim=(-1e-5,2.5e-5))\n",
    "ax2.tick_params(axis='y', color=\"black\")\n",
    "\n",
    "# ax3 = plt.twinx()\n",
    "# ax3.set(ylim=(min(components_weighed_ern_sum), max(components_weighed_ern_sum)))\n",
    "# ax3.tick_params(axis='y', color=\"magenta\")\n",
    "\n",
    "sns.scatterplot(x=[4], y= components_weighed_pe_sum[this_component], ax=ax1, color=\"magenta\")\n",
    "sns.scatterplot(x=[1], y= components_weighed_ern_sum[this_component], ax=ax1, color=\"magenta\")\n",
    "# sns_plot = sns.scatterplot(np.arange(5,6), components_weighed_pe_sum[this_component*pe_step:(this_component+1)*pe_step], ax=ax1, color=\"magenta\")\n",
    "# plt.axhline(y=0, color=\"magenta\", linewidth = 2)\n",
    "\n",
    "sns_plot = sns.lineplot(np.arange(0,10), -mean_X[this_component], ax=ax2, color=\"black\", linewidth = 3)\n",
    "\n",
    "\n",
    "sns_plot.figure.savefig(f\"{data_df.name}_output_{this_component}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547dfae-7072-4983-9462-d97eacf968bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ccd1e-bb51-4402-983e-ee5e909984a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_rum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e73ff6-3d71-4b09-818d-5c883636c881",
   "metadata": {},
   "source": [
    "# CURRENT BEST RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55ab31-ce10-4b62-95c7-6a252a3586b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(\n",
    "    \"../data/split0.3/regression_union_100-600_ampl_bins50_0.3_significant.pkl\"\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552610c-11bb-4836-8abd-a1bf40f70ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(\n",
    "    \"../data/split0.3/regression_union_100-600_centered_signal_ampl_0.3-5_significant.pkl\"\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93acf9d8-6d12-4b97-80dc-41e7515501a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(\n",
    "    \"../data/split0.3/regression_union_100-600_centered_signal_baselined-to-0-bin_signal_0.3-5.pkl\"\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d60ee-ad10-4611-9fd2-3b5123682bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(\n",
    "    \"../data/split0.3/regression_union_100-600_baselined_centered_ampl-2-pe-ern_0.3-5_significant.pkl\"\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c76ff-bede-43d8-8755-f9870d928bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
