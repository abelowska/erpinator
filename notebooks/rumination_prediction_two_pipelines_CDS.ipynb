{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a73099-38c1-4bf0-afd3-5481df7c3d19",
   "metadata": {},
   "source": [
    "# Rumination prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff9f56-5483-44c7-b9cc-486a31838279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e84d26-b32c-41ae-b3f4-81e0ee55156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import ast\n",
    "import os.path as op\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium.featurize\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\"..\")\n",
    "# from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c69899-6a32-4418-9079-8bd66aed497f",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading data\n",
    "\n",
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc483a7-410f-4b65-9955-9df42c783bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths TODO\n",
    "dir_path = os.path.dirname(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a25389-d030-415b-982d-b68df981b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6  # Start and end of the segments\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1\n",
    "ALL = 2\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ef4f5-f711-4f8e-8ba5-163dc50bd2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_order_list = [\n",
    "    \"Fp1\",\n",
    "    \"AF7\",\n",
    "    \"AF3\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    \"F5\",\n",
    "    \"F7\",\n",
    "    \"FT7\",\n",
    "    \"FC5\",\n",
    "    \"FC3\",\n",
    "    \"FC1\",\n",
    "    \"C1\",\n",
    "    \"C3\",\n",
    "    \"C5\",\n",
    "    \"T7\",\n",
    "    \"TP7\",\n",
    "    \"CP5\",\n",
    "    \"CP3\",\n",
    "    \"CP1\",\n",
    "    \"P1\",\n",
    "    \"P3\",\n",
    "    \"P5\",\n",
    "    \"P7\",\n",
    "    \"P9\",\n",
    "    \"PO7\",\n",
    "    \"PO3\",\n",
    "    \"O1\",\n",
    "    \"Iz\",\n",
    "    \"Oz\",\n",
    "    \"POz\",\n",
    "    \"Pz\",\n",
    "    \"CPz\",\n",
    "    \"Fpz\",\n",
    "    \"Fp2\",\n",
    "    \"AF8\",\n",
    "    \"AF4\",\n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"F2\",\n",
    "    \"F4\",\n",
    "    \"F6\",\n",
    "    \"F8\",\n",
    "    \"FT8\",\n",
    "    \"FC6\",\n",
    "    \"FC4\",\n",
    "    \"FC2\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"C2\",\n",
    "    \"C4\",\n",
    "    \"C6\",\n",
    "    \"T8\",\n",
    "    \"TP8\",\n",
    "    \"CP6\",\n",
    "    \"CP4\",\n",
    "    \"CP2\",\n",
    "    \"P2\",\n",
    "    \"P4\",\n",
    "    \"P6\",\n",
    "    \"P8\",\n",
    "    \"P10\",\n",
    "    \"PO8\",\n",
    "    \"PO4\",\n",
    "    \"O2\",\n",
    "]\n",
    "\n",
    "channels_dict = dict(zip(channels_order_list, np.arange(1, 64, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c412f36-66a2-4fd8-bfa7-caa98c4a315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=\"all\",\n",
    "    personal=True,\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "\n",
    "    On default, loads a train set: chooses only 80% of participants\n",
    "    and for each of them chooses 80% of epochs.\n",
    "    It will choose them deterministically.\n",
    "\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    If test_participants is set to True, it will load remaining 20% of participants.\n",
    "    If test_epochs is set to True, it will load remaining 20% of epochs.\n",
    "    Test epochs are chronologically after train epochs,\n",
    "    because it reflects real usage (first callibration and then classification).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "    personal: bool\n",
    "        whether a model will be both trained and tested on epochs from one person\n",
    "        if false, person's epochs aren't split into test and train\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    print(os.path.abspath(\"\"))\n",
    "    dir_path = os.path.dirname(os.path.abspath(\"\"))\n",
    "    print(dir_path)\n",
    "    header_files_glob = os.path.join(dir_path, \"data/responses/*.vhdr\")\n",
    "    header_files = glob.glob(header_files_glob)\n",
    "\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    # cut 20% of data for testing\n",
    "    h_train, h_test = train_test_split(header_files, test_size=0.3, random_state=0)\n",
    "\n",
    "    if test_participants:\n",
    "        header_files = h_test\n",
    "    else:\n",
    "        header_files = h_train\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 5 or len(correct) < 5:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, participant_epochs, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991419b-7acd-42bc-b048-e5e9fa3cb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, participant_epochs, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        if info == \"all\":\n",
    "            rumination_df = pd.read_csv(info_filename)\n",
    "        else:\n",
    "            rumination_df = pd.read_csv(info_filename, usecols=[\"File\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"File\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "\n",
    "#     for epoch in correct:\n",
    "#         epoch_df = pd.DataFrame(\n",
    "#             {\"id\": [id], \"epoch\": [epoch], \"marker\": [CORRECT]}\n",
    "#         ).join(info_df)\n",
    "#         participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "#     for epoch in error:\n",
    "#         epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [epoch], \"marker\": [ERROR]}).join(\n",
    "#             info_df\n",
    "#         )\n",
    "#         participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "        \n",
    "#     print(participant_epochs)\n",
    "        \n",
    "    epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [participant_epochs], \"marker\": [ALL]}).join(\n",
    "            info_df\n",
    "        )\n",
    "    participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88517e-884a-4b5b-9652-1bf2e7787aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' means that bad segments are rejected automatically.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = True\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5e07d-9618-47de-a706-687a529fb2fe",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8c673-5477-4126-b09d-870a8fcd9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_100_600_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055815fa-0d47-4bdd-9caf-51d60ced5d91",
   "metadata": {},
   "source": [
    "#### Read data for external testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c990ac1-14fc-4d0c-bf5d-d96f076e1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_100_600_test_df_3-5_all\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_df = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_df.to_pickle(\"../data/\" + epochs_test_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df7179-4753-45e0-96f9-6a9e62336f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = epochs_df\n",
    "X_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449269ab-f50c-425a-b624-c6e9c1d10ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = epochs_test_df\n",
    "X_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea2478-01a2-499e-b6bb-b20e580c0676",
   "metadata": {},
   "source": [
    "---\n",
    "## Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dff3a8-14a7-445c-8940-aba32c28ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from rumination_experiment_transformers_averaged_CDS import *\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a593d8-7928-410c-88b9-0b4b25f1c493",
   "metadata": {},
   "source": [
    "#### Create X train and y train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afda6e-cc8b-492f-bb80-ac3a0b0cde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of the analysed condition: erroneous responses or correct responses\n",
    "\n",
    "dataset = ERROR\n",
    "dataset_name = \"correct_response\" if dataset == CORRECT else \"error_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f1ac9-2bd1-46c6-b404-a394ac24603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape 1-D: scores\n",
    "rumination = np.array(X_train_df[\"Rumination Full Scale\"].to_list())\n",
    "deppression = np.array(X_train_df[\"DASS-21 Depression scale\"].to_list())\n",
    "anxiety = np.array(X_train_df[\"DASS-21 Anxiety scale\"].to_list())\n",
    "stress = np.array(X_train_df[\"DASS-21 Stress scale\"].to_list())\n",
    "\n",
    "\n",
    "y_train = rumination\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f04e5-5e91-42df-a5a2-1a5e28a92e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rum_test = np.array(X_test_df[\"Rumination Full Scale\"].to_list())\n",
    "y_rum_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b37add-c98f-4625-8049-6a2bd087b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366c3c6-dd06-4837-9bd8-a286d9c530f6",
   "metadata": {},
   "source": [
    "---\n",
    "### Experiments \n",
    "\n",
    "Parameters of experiments:\n",
    "- regressors\n",
    "- hyperparameters\n",
    "- preprocessing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962b4be-4bc7-4e61-9bb7-4151a2f59008",
   "metadata": {},
   "source": [
    "#### Prepare experiment estimating \n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fe861-b8ef-4c53-a68c-5241ba5348ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating model with grid search\n",
    "\n",
    "\n",
    "def rate_regressor(\n",
    "    X_train, y_train, X_test, y_test, regressor, regressor_params, base_steps, cv=3\n",
    "):\n",
    "    # define cross-validation method\n",
    "    cv_kf = KFold(n_splits=3)\n",
    "\n",
    "    pipeline = Pipeline([base_steps, regressor])\n",
    "    param_grid = regressor_params\n",
    "    # print(f\"Param grid {param_grid}\")\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv_kf,\n",
    "        scoring={\"r2\", \"neg_mean_absolute_error\", \"neg_mean_squared_error\"},\n",
    "        refit=\"r2\",\n",
    "        return_train_score=True,\n",
    "        n_jobs=10,\n",
    "        verbose=1,\n",
    "        error_score=\"raise\",\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ce1e5-4ec9-4ab1-b3d7-28a80d119ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conducting experiment and saving selected info do result df\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    tested_regressors,\n",
    "    regressor_params,\n",
    "    pipeline_name,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    dataset_name,\n",
    "    base_steps,\n",
    "    preprocessed_pipeline,\n",
    "    X_test_df,\n",
    "    y_rum,\n",
    "    results_df,\n",
    "):\n",
    "\n",
    "    for (regressor, params) in tested_regressors:\n",
    "        print(f\"Rating {regressor} \\n\")\n",
    "        tested_params = {**regressor_params, **params}\n",
    "\n",
    "        # enter to grid search\n",
    "        grid_result = rate_regressor(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            regressor,\n",
    "            tested_params,\n",
    "            base_steps,\n",
    "            cv=3,\n",
    "        )\n",
    "\n",
    "        #     predictions = grid_result.predict(X_test)\n",
    "        #     r2 = grid_result.score(X_test, y_test)\n",
    "        #     mae = mean_absolute_error(y_test, predictions)\n",
    "        #     r2_adj = r2_adjusted_scorer(y_test, predictions, len(X_test[0]), len(X_test))\n",
    "\n",
    "        best_estimator_index = grid_result.best_index_\n",
    "        mean_cv_r2 = grid_result.cv_results_[\"mean_test_r2\"][best_estimator_index]\n",
    "        std_cv_r2 = grid_result.cv_results_[\"std_test_r2\"][best_estimator_index]\n",
    "        mean_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        mean_cv_neg_mean_squared_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_squared_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_squared_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_squared_error\"\n",
    "        ][best_estimator_index]\n",
    "        \n",
    "        mean_train_r2 = grid_result.cv_results_[\"mean_train_r2\"][best_estimator_index]\n",
    "        mean_train_mae = grid_result.cv_results_[\"mean_train_neg_mean_absolute_error\"][best_estimator_index]\n",
    "        mean_train_mse = grid_result.cv_results_[\"mean_train_neg_mean_squared_error\"][best_estimator_index]\n",
    "\n",
    "\n",
    "        print(f\"     Best parameters: {grid_result.best_params_}\")\n",
    "        print(f\"     mean r2: {mean_cv_r2}           Â± {round(std_cv_r2,3)}\")\n",
    "        print(f\"     mean r2 train: {mean_train_r2}\")\n",
    "\n",
    "        cv_results = grid_result.cv_results_\n",
    "\n",
    "        # calculate p-value\n",
    "        scores_, pvalue_ = calculate_p_permutations(\n",
    "            grid_result.best_estimator_, X_train, y_train\n",
    "        )\n",
    "        \n",
    "        pre_processed_test_X = preprocessed_pipeline.transform(X_test_df)\n",
    "        estimator = grid_result.best_estimator_\n",
    "        score = estimator.score(pre_processed_test_X, y_rum)\n",
    "        \n",
    "        print(print(f\"     external validation r2: {score}\"))\n",
    "        \n",
    "\n",
    "        # insert selected info to df\n",
    "        data = {\n",
    "            \"data_set\": dataset_name,\n",
    "            \"pipeline_name\": pipeline_name,\n",
    "            \"model\": regressor[0],\n",
    "            \"parameters\": grid_result.best_params_,\n",
    "            \"mean_cv_r2\": mean_cv_r2,\n",
    "            \"std_cv_r2\": std_cv_r2,\n",
    "            \"mean_cv_mae\": mean_cv_neg_mean_absolute_error,\n",
    "            \"std_cv_mae\": std_cv_neg_mean_absolute_error,\n",
    "            \"mean_cv_mse\":mean_cv_neg_mean_squared_error,\n",
    "            \"std_cv_mse\": std_cv_neg_mean_squared_error,\n",
    "            \"cv_results\": cv_results,\n",
    "            \"mean_train_r2\": mean_train_r2,\n",
    "            \"mean_train_mae\":mean_train_mae,\n",
    "            \"mean_train_mse\":mean_train_mse,\n",
    "            \"p-value\": pvalue_,\n",
    "            \"best_estimator\": grid_result.best_estimator_,\n",
    "            \"pre_processed_pipeline\": preprocessed_pipeline,\n",
    "            \"external_score\":score\n",
    "        }\n",
    "\n",
    "        results_df = results_df.append(data, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfa7df-bda2-4249-97c5-999e73d1abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating p-value with permutation test\n",
    "\n",
    "\n",
    "def calculate_p_permutations(estimator, X, y, cv=3, n_permutations=100, n_jobs=10):\n",
    "\n",
    "    score_, perm_scores_, pvalue_ = permutation_test_score(\n",
    "        estimator, X, y, cv=cv, n_permutations=n_permutations, n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    # summarize\n",
    "    print(f\"     The permutation P-value is = {pvalue_:.3f}\")\n",
    "    print(f\"     The permutation score is = {score_:.3f}\\n\")\n",
    "\n",
    "    return score_, pvalue_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fea081-c0ff-47c9-ba47-6323d0fa33bd",
   "metadata": {},
   "source": [
    "### Perform Experiments\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385045e-557e-44ab-a650-439c38ff8ab6",
   "metadata": {},
   "source": [
    "#### Global parameters common for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8dc3b-60fb-4bb0-848a-876ea8c1bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels that will be included in the experiment\n",
    "\n",
    "red_box = [\n",
    "    \"F3\",\"F1\",\"Fz\", \"F2\", \"F4\",\n",
    "    \"FC3\", \"FC1\", \"FCz\", \"FC2\",\"FC4\",\n",
    "    \"C3\", \"C1\",\"Cz\",\"C2\", \"C4\",\n",
    "    \"CP3\", \"CP1\",\"CPz\",\"CP2\", \"CP4\",\n",
    "    \"P3\",\"P1\", \"Pz\", \"P2\", \"P4\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19073a0-ee3d-4ace-bbfd-177c753cefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define estimators and their hyperparameters\n",
    "\n",
    "en = (\"en\", ElasticNet(random_state=random_state))\n",
    "en_params = dict(\n",
    "    en__alpha=np.logspace(-7, 3, num=20, base=10),\n",
    "    en__l1_ratio=np.logspace(-8, 0, num=17, base=10),\n",
    ")\n",
    "\n",
    "kr = (\"kr\", KernelRidge(kernel=\"rbf\"))\n",
    "kr_params = dict(\n",
    "    kr__alpha=np.logspace(-5, 3, num=20, base=10),\n",
    "    kr__gamma=np.logspace(-5, 3, num=20, base=10),\n",
    ")\n",
    "\n",
    "\n",
    "svr = (\"svr\", SVR())\n",
    "svr_params = dict(\n",
    "    svr__kernel=[\"linear\", \"rbf\"],\n",
    "    svr__C=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    svr__gamma=[\"scale\"],\n",
    "    svr__epsilon=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    ")\n",
    "\n",
    "tested_regressors = [\n",
    "    # (svr, svr_params), \n",
    "    # (kr, kr_params), \n",
    "    (en, en_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff9c2c2-558f-4616-9b88-63631942e80d",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1\n",
    "\n",
    "- CDS\n",
    "- channel extraction to red box\n",
    "- bandpass filter 1-40 Hz\n",
    "- choose error and average\n",
    "- spatial filter with PCA\n",
    "- feature extraction with ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5d755-b1d9-4a03-9e88-95fe9a5a1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters of pipeline\n",
    "timepoints_count = 181\n",
    "\n",
    "min_spatial_filter = 3\n",
    "max_spatial_filter = 6\n",
    "step_spatial_filter = 1\n",
    "\n",
    "min_feature_selection = 1\n",
    "max_feature_selection = 6\n",
    "step_feature_selection = 1\n",
    "\n",
    "\n",
    "# define proper parameters for training. In this case define range of number of feature extraction to search\n",
    "regressor_params = dict(\n",
    "    feature_extraction__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e6594-7424-4a89-b582-0b74baa8ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "# manually test different numbers of spatial filter components\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "    \n",
    "    \n",
    "preprocessed_X_train = Pipeline([\n",
    "        ('current_source_density', CurrentSourceDensity()),\n",
    "        (\"channels_extraction\",PickChannels()),\n",
    "        (\"bandpass_filter\",BandpassFilter()),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "]).fit_transform(X_train_df.copy())\n",
    "\n",
    "preprocessed_X_test = Pipeline([\n",
    "        ('current_source_density', CurrentSourceDensity()),\n",
    "        (\"channels_extraction\",PickChannels()),\n",
    "        (\"bandpass_filter\",BandpassFilter()),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "]).fit_transform(X_test_df.copy())\n",
    "\n",
    "for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter): \n",
    "  \n",
    "    pipeline_name = f\"PCA_{n_components}_CDS_SF_FE\"\n",
    "\n",
    "    ############################################################################################\n",
    "    preprocessed_pipeline = Pipeline([\n",
    "        # (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        # (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "        # (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "        (\"binning\", BinTransformer(step=12)),\n",
    "        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "        (\"postprocessing\", PostprocessingTransformer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]).fit(preprocessed_X_train)\n",
    "\n",
    "    preprocessed_X = preprocessed_pipeline.transform(preprocessed_X_train)\n",
    "    \n",
    "    ###########################################################################################\n",
    "\n",
    "    regressor_steps = (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "\n",
    "    # rate different models\n",
    "    results_df = run_experiment(\n",
    "        tested_regressors,\n",
    "        regressor_params,\n",
    "        pipeline_name,\n",
    "        preprocessed_X,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        dataset_name,\n",
    "        regressor_steps,\n",
    "        preprocessed_pipeline,\n",
    "        preprocessed_X_test,\n",
    "        y_rum_test,\n",
    "        results_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576c276-45c4-4b83-9ae5-920ab0e2464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65026285-0166-4347-a81b-32c04fa58be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f43282-a29d-49e2-84fc-965089f83f85",
   "metadata": {},
   "source": [
    "---\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63801c8-747a-4865-829e-e01c125d37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X_train = Pipeline([\n",
    "        ('current_source_density', CurrentSourceDensity()),\n",
    "        (\"channels_extraction\",PickChannels()),\n",
    "        (\"bandpass_filter\",BandpassFilter()),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        # (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        # (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "        # (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "        # (\"binning\", BinTransformer(step=12)),\n",
    "\n",
    "]).fit_transform(X_train_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e2027-90ca-4b76-bbe5-9031a4b001ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_X_train_no_cds = Pipeline([\n",
    "#         # ('current_source_density', CurrentSourceDensity()),\n",
    "#         (\"channels_extraction\",PickChannels()),\n",
    "#         (\"bandpass_filter\",BandpassFilter()),\n",
    "#         (\"average\", Evoked()),\n",
    "#         ('extract_averaged_data', ExtractData()),\n",
    "#         # (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         # (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "#         # (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#         # (\"binning\", BinTransformer(step=12)),\n",
    "\n",
    "# ]).fit_transform(X_train_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444004f6-5eb6-40ec-90dd-63deca26dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_X_test = Pipeline([\n",
    "#         ('current_source_density', CurrentSourceDensity()),\n",
    "#         (\"channels_extraction\",PickChannels()),\n",
    "#         (\"bandpass_filter\",BandpassFilter()),\n",
    "#         (\"average\", Evoked()),\n",
    "#         ('extract_averaged_data', ExtractData()),\n",
    "#         # (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         # (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "#         # (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#         # (\"binning\", BinTransformer(step=12)),\n",
    "\n",
    "# ]).fit_transform(X_test_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99180daf-e2d3-49cd-993e-0091de66f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_X_test_no_cds = Pipeline([\n",
    "#         # ('current_source_density', CurrentSourceDensity()),\n",
    "#         (\"channels_extraction\",PickChannels()),\n",
    "#         (\"bandpass_filter\",BandpassFilter()),\n",
    "#         (\"average\", Evoked()),\n",
    "#         ('extract_averaged_data', ExtractData()),\n",
    "#         # (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "#         # (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "#         # (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "#         # (\"binning\", BinTransformer(step=12)),\n",
    "\n",
    "# ]).fit_transform(X_test_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d8306-c6c5-43db-ad1c-72d24c8e7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pip = Pipeline([\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=3, random_state=random_state)),\n",
    "        (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=181)),\n",
    "        # ('neg', ReverseComponent2()),\n",
    "        (\"binning\", BinTransformer(step=12)),\n",
    "        (\"baseline\", ErnBaselined()),\n",
    "        (\"centering\", CenteredSignalAfterBaseline2()) \n",
    "\n",
    "]).fit(preprocessed_X_train)\n",
    "\n",
    "X_train_a = pre_pip.transform(preprocessed_X_train)\n",
    "# X_test_a = pre_pip.transform(preprocessed_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb31da3-03f8-489b-acc3-588bbdbd41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f2f86-92f0-4dbf-bca6-537bf2ced900",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X_train_a, axis=0)\n",
    "# X_mean_no_cds = np.mean(preprocessed_X_train_no_cds, axis=0)\n",
    "# X_test_mean = np.mean(X_test_a, axis=0)\n",
    "# X_test_mean_no_cds = np.mean(preprocessed_X_test_no_cds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ddbd8-4cba-4a64-a1b5-0451f46f315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(1,3):\n",
    "    plt.plot(X_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7374f83-27cf-451f-a931-4aabc8387612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(1,3):\n",
    "    plt.plot(X_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72ee5c-8622-4193-a5d8-e9eaf6852f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.plot(X_test_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8a392-56ec-43c8-a124-88c69828c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,15):\n",
    "    plt.plot(X_test_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0cfda9-9759-49f5-a347-456e81588dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,15):\n",
    "    plt.plot(X_test_mean_no_cds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94436e85-20bc-42f9-beb3-a575f2322809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.plot(X_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a9d33-a21d-43cf-b073-3565ea33c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.plot(X_mean_no_cds[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee8580-21d3-4d8d-a3e7-8d70ea41153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.plot(X_test_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690aa02-527a-4cf4-8e11-3783530002f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,3):\n",
    "    plt.plot(X_test_mean_no_cds[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c1356-2a93-4e39-b1e6-77fd0c23209a",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2\n",
    "\n",
    "- CDS\n",
    "- channel extraction to red box\n",
    "- bandpass filter 1-40 Hz\n",
    "- choose error and average\n",
    "- baseline to positivity peak\n",
    "- center to ERN in each component\n",
    "- split on ERN and PE\n",
    "- spatial filter with PCA\n",
    "- feature extraction with ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f2a56-089d-46b4-8219-5316268c6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters of pipeline\n",
    "timepoints_count = 181\n",
    "\n",
    "min_spatial_filter = 3\n",
    "max_spatial_filter = 6\n",
    "step_spatial_filter = 1\n",
    "\n",
    "min_feature_selection = 1\n",
    "max_feature_selection = 6\n",
    "step_feature_selection = 1\n",
    "\n",
    "\n",
    "# define proper parameters for training. In this case define range of number of feature extraction to search\n",
    "regressor_params = dict(\n",
    "    features__ern_features__feature_extraction__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    "    features__pe_features__feature_extraction__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d74dc-f537-47d5-9c29-4f8242efb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X_train = Pipeline([\n",
    "        ('current_source_density', CurrentSourceDensity()),\n",
    "        (\"channels_extraction\",PickChannels()),\n",
    "        (\"bandpass_filter\",BandpassFilter()),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "]).fit_transform(X_train_df.copy())\n",
    "\n",
    "preprocessed_X_test = Pipeline([\n",
    "        ('current_source_density', CurrentSourceDensity()),\n",
    "        (\"channels_extraction\",PickChannels()),\n",
    "        (\"bandpass_filter\",BandpassFilter()),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "]).fit_transform(X_test_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab53f3-a5d6-4d82-9351-e4a7a23a542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "# manually test different numbers of spatial filter components\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter): \n",
    "  \n",
    "    pipeline_name = f\"PCA_{n_components}_CDS_SF_SPLIT_FE\"\n",
    "\n",
    "    ############################################################################################\n",
    "    preprocessed_pipeline = Pipeline([\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "        (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "        (\"binning\", BinTransformer(step=12)),\n",
    "        (\"baseline\", ErnBaselined()),\n",
    "        (\"centering\", CenteredSignalAfterBaseline2()) \n",
    "        \n",
    "    ]).fit(preprocessed_X_train)\n",
    "\n",
    "    preprocessed_X = preprocessed_pipeline.transform(preprocessed_X_train)\n",
    "    \n",
    "    ###########################################################################################\n",
    "    \n",
    "    ern_features = Pipeline(steps=[\n",
    "                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                    (\"postprocessing\", PostprocessingTransformer()),\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "    ])\n",
    "     \n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                    (\"pe_data_extraction\", PeTransformer()),\n",
    "                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                    (\"postprocessing\", PostprocessingTransformer()),\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "    ])\n",
    "    \n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "    steps = ('features', ern_pe_features)\n",
    "    \n",
    "    ############################################################################################\n",
    "\n",
    "    regressor_steps = steps\n",
    "\n",
    "    # rate different models\n",
    "    results_df = run_experiment(\n",
    "        tested_regressors,\n",
    "        regressor_params,\n",
    "        pipeline_name,\n",
    "        preprocessed_X,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        dataset_name,\n",
    "        regressor_steps,\n",
    "        preprocessed_pipeline,\n",
    "        preprocessed_X_test,\n",
    "        y_rum_test,\n",
    "        results_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931bd8b-511b-4111-a775-8db9ec102071",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3\n",
    "\n",
    "- CDS\n",
    "- channel extraction to red box\n",
    "- bandpass filter 1-40 Hz\n",
    "- choose error and average\n",
    "- baseline to positivity peak\n",
    "- center to ERN from 1 component\n",
    "- split on ERN and PE\n",
    "- spatial filter with PCA\n",
    "- feature extraction with ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f6e53-35a9-496a-844b-334e44c4dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters of pipeline\n",
    "timepoints_count = 181\n",
    "\n",
    "min_spatial_filter = 3\n",
    "max_spatial_filter = 9\n",
    "step_spatial_filter = 1\n",
    "\n",
    "min_feature_selection = 2\n",
    "max_feature_selection = 4\n",
    "step_feature_selection = 1\n",
    "\n",
    "\n",
    "# define proper parameters for training. In this case define range of number of feature extraction to search\n",
    "regressor_params = dict(\n",
    "    features__ern_features__feature_extraction__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    "    features__pe_features__feature_extraction__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafffb4-7037-4384-babe-25f1ba946d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_X_train = Pipeline([\n",
    "#         ('current_source_density', CurrentSourceDensity()),\n",
    "#         # (\"channels_extraction\",PickChannels()),\n",
    "#         # (\"average\", Evoked()),\n",
    "#         # (\"bandpass_filter\",BandpassFilter()),\n",
    "#         # ('extract_averaged_data', ExtractData()),\n",
    "# ]).fit_transform(X_train_df.copy())\n",
    "\n",
    "# preprocessed_X_test = Pipeline([\n",
    "#         ('current_source_density', CurrentSourceDensity()),\n",
    "#         # (\"channels_extraction\",PickChannels()),\n",
    "#         # (\"average\", Evoked()),\n",
    "#         # (\"bandpass_filter\",BandpassFilter()),\n",
    "#         # ('extract_averaged_data', ExtractData()),\n",
    "# ]).fit_transform(X_test_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7b710-5f02-49ad-9c23-577c1f4f6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_ern_bin = 0\n",
    "# stop_ern_bin = 3\n",
    "# start_pe_bin = 2\n",
    "# stop_pe_bin = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd012625-00b4-44b0-8f55-f2fcef6ec591",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_box = [\"F3\", \"F2\", \"F4\", \"C1\", \"Cz\", \"FCz\", \"C3\", \"C2\", \"C4\",\"P1\", \"Fpz\", \"P2\", \"P3\", \"CPz\", \"P4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02deb63b-06f2-4e3f-839e-656cf305045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "# manually test different numbers of spatial filter components\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter): \n",
    "  \n",
    "    pipeline_name = f\"PCA_{n_components}_CDS_SF_SPLIT_FE\"\n",
    "\n",
    "    ############################################################################################\n",
    "    preprocessed_pipeline = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list=red_box)),\n",
    "        (\"average\", Evoked()),\n",
    "        # (\"bandpass_filter\",BandpassFilter()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "        (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\"binning\", BinTransformer(step=12)),\n",
    "        (\"baseline\", ErnBaselined()),\n",
    "        (\"centering\", CenteredSignalAfterBaseline()) \n",
    "        \n",
    "    ]).fit(X_train_df)\n",
    "\n",
    "    preprocessed_X = preprocessed_pipeline.transform(X_train_df)\n",
    "    \n",
    "    ###########################################################################################\n",
    "    \n",
    "    ern_features = Pipeline(steps=[\n",
    "                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                    (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                    (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                    (\"postprocessing\", PostprocessingTransformer()),\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "    ])\n",
    "     \n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                    (\"pe_data_extraction\", PeTransformer()),\n",
    "                    (\"pe_amplitude\", PeAmplitude2()),\n",
    "                    (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                    (\"postprocessing\", PostprocessingTransformer()),\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"feature_extraction\", FastICA(random_state=random_state))\n",
    "    ])\n",
    "    \n",
    "    ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "    steps = ('features', ern_pe_features)\n",
    "    \n",
    "    ############################################################################################\n",
    "\n",
    "    regressor_steps = steps\n",
    "\n",
    "    # rate different models\n",
    "    results_df = run_experiment(\n",
    "        tested_regressors,\n",
    "        regressor_params,\n",
    "        pipeline_name,\n",
    "        preprocessed_X,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        dataset_name,\n",
    "        regressor_steps,\n",
    "        preprocessed_pipeline,\n",
    "        X_test_df,\n",
    "        y_rum_test,\n",
    "        results_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e1c8f-2fb5-4612-9017-f9a629ace9f0",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cbb6b-8259-44a6-8ff6-949fe9d7e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.read_pickle(\n",
    "#     \"../data/split0.3/regression_union_100-600_baselined_centered_ampl-2-pe-ern_0.3-5_significant.pkl\"\n",
    "# )\n",
    "data_df = results_static_ICA_bin_union_100_600_baselined_peak_to_peak_components_df\n",
    "data_df.name = \"union_100_600_baselined_centered_no_scaler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90458d7d-3a55-479c-9c82-bba82801046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdc3c7-5692-47ba-8626-fe94ec398b0c",
   "metadata": {},
   "source": [
    "#### Extract coefficients of ERN and PE features extraction (ICA) and coefficient od estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42575b1-e957-4341-94bd-b2efb0604898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ern_features = data_df.best_estimator[1][\"features\"].transformer_list[0][1][\"feature_selection\"].components_\n",
    "# pe_features = data_df.best_estimator[1][\"features\"].transformer_list[1][1][\"feature_selection\"].components_\n",
    "\n",
    "# without additional metric as feature\n",
    "ern_features = data_df.best_estimator[0][\"features\"][\"ern_pe_features\"].transformer_list[0][1][\"feature_selection\"].components_\n",
    "pe_features = data_df.best_estimator[0][\"features\"][\"ern_pe_features\"].transformer_list[1][1][\"feature_selection\"].components_\n",
    "\n",
    "coeffs = data_df.best_estimator[0][\"en\"].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fc2ed-1541-4b69-88c2-3e1f3fa9333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ca93e-e420-401b-b38f-e356ec42a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea39cb-334d-4a4f-8312-e722e0b4bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26fe94-2a8e-4acb-8cda-9b6e0fc5e8c6",
   "metadata": {},
   "source": [
    "#### Weigh components with coeffs from estimator and sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591889e2-aa90-447e-afde-2385b2bee260",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_components_weighed = np.array([ern_features[i] * coeffs[i] for i in range(0,ern_features.shape[0])])\n",
    "pe_components_weighed = np.array([pe_features[i-ern_features.shape[0]] * coeffs[i] for i in range(ern_features.shape[0], ern_features.shape[0] + pe_features.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aefe62-2e54-405f-9b33-74523fd6fbe7",
   "metadata": {},
   "source": [
    "#### Sum all feature extraction components to extract direct weigh of given bin at given spatial filter component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279af91-4d5b-49ec-b4bf-44cf202bdca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_weighed_ern_sum = sum(ern_components_weighed)\n",
    "components_weighed_pe_sum = sum(pe_components_weighed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13104848-45d7-45f9-90d0-999642cbaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_weighed_ern_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3905a32-13c0-465f-af63-e7c7f8af4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rum_ern = components_weighed_ern_sum * ern_ampl_mean\n",
    "mean_rum_ern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc551b-646b-4155-bc5b-75f891cc378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rum_pe = components_weighed_pe_sum * pe_ampl_mean\n",
    "mean_rum_pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe975747-5514-4b6d-a7d0-5b7c9249c822",
   "metadata": {},
   "source": [
    "#### Extract components of spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e552573-ab6f-457c-a172-2cba7f5a6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_features = Pipeline(steps=[\n",
    "                (\"ern_data_extraction\", ErnTransformer()),\n",
    "                (\"ern_amplitude\", ErnAmplitude2()),\n",
    "                (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                (\"postprocessing\", PostprocessingTransformer()),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                # (\"feature_selection\", FastICA(random_state=random_state))\n",
    "])\n",
    "\n",
    "\n",
    "pe_features = Pipeline(steps = [\n",
    "                (\"pe_data_extraction\", PeTransformer()),\n",
    "                # (\"pe_centered\", CenteredPeAfterBaseline()),\n",
    "                (\"pe_amplitude\", PeAmplitude2()),\n",
    "                (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                (\"postprocessing\", PostprocessingTransformer()),\n",
    "                # (\"scaler\", StandardScaler()),\n",
    "                # (\"feature_selection\", FastICA(random_state=random_state))\n",
    "])\n",
    "\n",
    "ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "features = Pipeline([\n",
    "    ('ern_pe_features', ern_pe_features)\n",
    "\n",
    "])\n",
    "\n",
    "# steps = ('features', features)\n",
    "\n",
    "preprocessed_pipeline = Pipeline([(\"extract_epochs\", EEGdata(dataset=dataset)),\n",
    "    # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "    (\n",
    "        \"channels_filtering\",\n",
    "        ChannelExtraction(significant_channels)\n",
    "    ),\n",
    "    (\n",
    "        \"average_epochs\",\n",
    "        AveragePerParticipant(),\n",
    "    ),\n",
    "    (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "    (\n",
    "        \"spatial_filter\",\n",
    "        PCA(n_components=3, random_state=random_state),\n",
    "    ),\n",
    "    (\n",
    "        \"spatial_filter_postprocessing\",\n",
    "        SpatialFilterPostprocessing(\n",
    "            timepoints_count=181,\n",
    "        ),\n",
    "    ),\n",
    "    (\"lowpass_filter\", LowpassFilter()),\n",
    "    (\"binning\", BinTransformer(step=12)),\n",
    "    (\"baseline\", ErnBaselined()),\n",
    "    (\"centering\", CenteredSignalAfterBaseline()),\n",
    "    # ('features', features)\n",
    "\n",
    "                          ]).fit(X_train)\n",
    "preprocessed_X_test = preprocessed_pipeline.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0176e-1f92-4939-8aaa-3883ea50c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X = preprocessed_pipeline.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8784737-7a9a-45a8-a1b2-4a4281f31a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394cf0f-a7b8-4192-91b3-093be1d95197",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_ampl = preprocessed_X[:,3:6]\n",
    "pe_ampl_mean = np.mean(pe_ampl, axis=0)\n",
    "pe_ampl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6736c-5c08-437d-8e09-f93c907a4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_ampl = preprocessed_X[:,0:3]\n",
    "ern_ampl_mean = np.mean(ern_ampl, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a14b5-650c-46b2-89cc-07f49ac1b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_ampl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dd765-9269-410d-831a-cf7e85e71010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = ('features', features)\n",
    "preprocessed_pipeline = Pipeline([(\"extract_epochs\", EEGdata(dataset=dataset)),\n",
    "            # (\"narrow_indices\", NarrowIndices(start=76, stop=257)),\n",
    "            (\n",
    "                \"channels_filtering\",\n",
    "                ChannelExtraction(significant_channels)\n",
    "            ),\n",
    "            (\n",
    "                \"average_epochs\",\n",
    "                AveragePerParticipant(),\n",
    "            ),\n",
    "            (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "            (\n",
    "                \"spatial_filter\",\n",
    "                PCA(n_components=3, random_state=random_state),\n",
    "            ),\n",
    "            (\n",
    "                \"spatial_filter_postprocessing\",\n",
    "                SpatialFilterPostprocessing(\n",
    "                    timepoints_count=181,\n",
    "                ),\n",
    "            ),\n",
    "            (\"lowpass_filter\", LowpassFilter()),\n",
    "            (\"binning\", BinTransformer(step=12)),\n",
    "            (\"baseline\", ErnBaselined()),\n",
    "            (\"centering\", CenteredSignalAfterBaseline()),\n",
    "            ('ern_pe_features', ern_pe_features)\n",
    "                                  ]).fit(X_train)\n",
    "\n",
    "preprocessed_X = preprocessed_pipeline.transform(X_train)\n",
    "pre_processed_test_X = preprocessed_pipeline.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7e58b-f842-489e-863e-1e2f78d3ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ern_features = Pipeline(steps=[\n",
    "#                     (\"ern_extraction\", CenteredERN(step=16)),\n",
    "#                     (\"binning\", BinTransformer(step=16)),\n",
    "# #                     (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                     (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                     (\"scaler\", StandardScaler()),\n",
    "# #                     (\"feature_selection\", FastICA(random_state=random_state))\n",
    "# # \n",
    "# ])\n",
    "\n",
    "# pe_features = Pipeline(steps = [\n",
    "#                         (\"pe_extraction\", CenteredPe(step=16)),\n",
    "#                         (\"binning\", BinTransformer(step=16)),\n",
    "# #                         # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "# #                         # (\"postprocessing\", PostprocessingTransformer()),\n",
    "# #                         # (\"scaler\", StandardScaler()),\n",
    "# #                         # (\"feature_selection\", FastICA(random_state=random_state))\n",
    "#         ])\n",
    "    \n",
    "# #         ern_pe_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)], n_jobs = 10)\n",
    "\n",
    "# #         features = Pipeline([\n",
    "# #             ('ern_pe_features', ern_pe_features)\n",
    "\n",
    "# #         ])\n",
    "\n",
    "# #         steps = ('features', features)\n",
    "\n",
    "# ern_fitted = ern_features.fit_transform(preprocessed_X)\n",
    "# ern_test_fitted = ern_features.transform(pre_processed_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f195f5-64d8-4087-893e-1224bcba013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_fitted_mean = np.mean(ern_fitted, axis=0)\n",
    "ern_test_fitted_mean = np.mean(ern_test_fitted, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca03f64-94fd-465a-aa8d-45eea6d64eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_test_fitted_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a1499-e289-4e78-835d-e6f5fcfe56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ern_fitted_mean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac3ce80-9136-4bea-9680-193356081190",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb0ee3-7b85-4ee4-9afb-65aefd48565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filter_n_components = 3\n",
    "\n",
    "this_steps = spatial_filter_bins_steps(spatial_filter_n_components=spatial_filter_n_components, timepoints_count=181)\n",
    "pre_processed_X = Pipeline(steps=this_steps).fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5b02a2-044a-4553-b3b5-69d0841d228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_test_X = preprocessed_pipeline.transform(X_test_df)\n",
    "pre_processed_X = preprocessed_pipeline.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3486f6-a736-48c1-b8ce-a1c0d99cfb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged signal within components through all participants\n",
    "mean_X_1 = np.mean(pre_processed_X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9cb59-d7a8-4d1d-b5cd-462943105f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_fitted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c8a68d-95f0-4c5b-8ada-7b021f8b823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd337e5-ad95-41be-aecb-f3906b10f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6757d7-cbc7-42a1-abec-a3cbde22dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged signal within components through all participants\n",
    "mean_X = np.mean(preprocessed_X, axis=0)\n",
    "mean_2_X = np.mean(pre_processed_test_X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6573c8-792b-4eba-a2ea-474177d202e7",
   "metadata": {},
   "source": [
    "-----\n",
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22a81b-c033-4672-93a7-f7003ed1dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices in bins\n",
    "\n",
    "tmin, tmax = -0.1, 0.6  # Start and end of the segments\n",
    "signal_frequency = 256\n",
    "\n",
    "step_in_ms = 50  # in miliseconds (?)\n",
    "step_tp = int(signal_frequency * step_in_ms / 1000) # in timepoints\n",
    "\n",
    "# indices for slicing epoch into ERN part and Pe part (in sec)\n",
    "start_ern = 0\n",
    "stop_ern = 0.15\n",
    "start_pe = 0.15\n",
    "stop_pe = 0.35\n",
    "\n",
    "start_ern_bin = int((signal_frequency * (start_ern - tmin)) / step_tp) + 1\n",
    "stop_ern_bin = int(signal_frequency * (stop_ern - tmin) / step_tp) + 1\n",
    "start_pe_bin = int(signal_frequency * (start_pe - tmin) / step_tp) + 1\n",
    "stop_pe_bin = int(signal_frequency * (stop_pe - tmin) / step_tp) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dbc2b-8302-4624-a55b-67bde67f741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ern_bin = 0\n",
    "stop_ern_bin = 3\n",
    "start_pe_bin = 3\n",
    "stop_pe_bin = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13efc2d-cf0c-4314-9917-19144e6cf4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filter_n_components = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88488c65-4b2a-4c04-a963-506505712f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0, 1 or 2\n",
    "this_component = 2\n",
    "\n",
    "# pe_step = int(pe_features.shape[1]/ spatial_filter_n_components)\n",
    "# ern_step = int(ern_features.shape[1]/ spatial_filter_n_components)\n",
    "# spatial_filter_step = int(pre_processed_X.shape[1]/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7c0a8-e843-4a1b-924e-ad9618f1b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filter_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310d788-67e6-4bd0-aac2-74dc87d23f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_components_weighed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a781ecdb-0d91-4437-9969-2d380b8df101",
   "metadata": {},
   "outputs": [],
   "source": [
    "-ern_fitted_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3a287-972a-4976-b72c-8857c162ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_components_weighed[0][0:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86258f07-0dde-46c7-8ed2-c6822dd21f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938b9f5-af49-4100-93f6-e15de0582164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# ax1 = plt.twinx()\n",
    "ax1.set(ylim=(np.min(ern_components_weighed)-0.1, np.max(pe_components_weighed)+0.05))\n",
    "ax1.tick_params(axis='y', color=\"magenta\", width=3, length=10)\n",
    "\n",
    "plt.axhline(y=0, color=\"grey\", linewidth = 2, linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=2, color=\"grey\", linewidth = 2, linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=6, color=\"grey\", linewidth = 2, linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(0,5):\n",
    "#     sns.lineplot(np.arange(0,3), ern_components_weighed[i][this_component:3], ax=ax1)\n",
    "\n",
    "# for i in range(0,pe_features.shape[0]):\n",
    "#     sns.scatterplot(np.arange(5,6), pe_components_weighed[i][this_component], ax=ax1)\n",
    "    \n",
    "\n",
    "ax2 = plt.twinx()\n",
    "ax2.set(ylim=(-1e-5,2.5e-5))\n",
    "ax2.tick_params(axis='y', color=\"black\")\n",
    "\n",
    "# ax3 = plt.twinx()\n",
    "# ax3.set(ylim=(min(components_weighed_ern_sum), max(components_weighed_ern_sum)))\n",
    "# ax3.tick_params(axis='y', color=\"magenta\")\n",
    "\n",
    "sns.scatterplot(x=[4], y= components_weighed_pe_sum[this_component], ax=ax1, color=\"magenta\")\n",
    "sns.scatterplot(x=[1], y= components_weighed_ern_sum[this_component], ax=ax1, color=\"magenta\")\n",
    "# sns_plot = sns.scatterplot(np.arange(5,6), components_weighed_pe_sum[this_component*pe_step:(this_component+1)*pe_step], ax=ax1, color=\"magenta\")\n",
    "# plt.axhline(y=0, color=\"magenta\", linewidth = 2)\n",
    "\n",
    "sns_plot = sns.lineplot(np.arange(0,10), -mean_X[this_component], ax=ax2, color=\"black\", linewidth = 3)\n",
    "\n",
    "\n",
    "sns_plot.figure.savefig(f\"{data_df.name}_output_{this_component}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547dfae-7072-4983-9462-d97eacf968bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ccd1e-bb51-4402-983e-ee5e909984a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_rum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e73ff6-3d71-4b09-818d-5c883636c881",
   "metadata": {},
   "source": [
    "# CURRENT BEST RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55ab31-ce10-4b62-95c7-6a252a3586b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(\n",
    "    \"../data/split0.3/regression_union_100-600_ampl_bins50_0.3_significant.pkl\"\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552610c-11bb-4836-8abd-a1bf40f70ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(\n",
    "    \"../data/split0.3/regression_union_100-600_centered_signal_ampl_0.3-5_significant.pkl\"\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93acf9d8-6d12-4b97-80dc-41e7515501a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(\n",
    "    \"../data/split0.3/regression_union_100-600_centered_signal_baselined-to-0-bin_signal_0.3-5.pkl\"\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d60ee-ad10-4611-9fd2-3b5123682bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(\n",
    "    \"../data/split0.3/regression_union_100-600_baselined_centered_ampl-2-pe-ern_0.3-5_significant.pkl\"\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c76ff-bede-43d8-8755-f9870d928bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
