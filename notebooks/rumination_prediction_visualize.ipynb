{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rumination prediction with cesium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Features are calculated for each band from cwt separately.\n",
    "Different strategies of ICA and PCA application:\n",
    "\n",
    "- ICA + PCA on all kind of features (mean, std itd) -> do not increase effectivness of the model\n",
    "\n",
    "- ICA + PCA on two the best kind of features (std + abs_diff) -> increase effectivness of the model a lot. Second the best method:  \n",
    "\n",
    "    - 9 * (3 from 2*14) = 27 components\n",
    "    \n",
    "            MAPE: 17.9079258661464\n",
    "            MAE: 0.519917817723914\n",
    "            MSE: 0.478030015754372\n",
    "            R^2: 0.376125673763408\n",
    "\n",
    "    - 9 * (2 from 2*14) = 18 components\n",
    "\n",
    "            MAPE: 16.8443619132606\n",
    "            MAE: 0.502078488977514\n",
    "            MSE: 0.454436875307531\n",
    "            R^2: 0.406916950702055\n",
    "\n",
    "\n",
    "\n",
    "- ICA + PCA separately on std features and abs_diffs features -> ok, but not the best way\n",
    "    \n",
    "    - 6*(5+5) = 60 components\n",
    "\n",
    "            MAPE: 24.018846090150365\n",
    "            MAE: 0.6884866303220416\n",
    "            MSE: 0.7185622829832321\n",
    "            R^2: 0.062208343867806604\n",
    "\n",
    "    - 6*(3+3) = 36 components\n",
    "\n",
    "            MAPE: 21.50178345656211\n",
    "            MAE: 0.6201825754057195\n",
    "            MSE: 0.6069731329548679\n",
    "            R^2: 0.2078427255904629\n",
    "\n",
    "    - 6*(2+2) = 24 components\n",
    "\n",
    "            19.438263116024583\n",
    "            0.5637319078555214\n",
    "            0.5362710537170331\n",
    "            0.3001156176565023\n",
    "\n",
    "    - 6*(1+1) = 12 components\n",
    "\n",
    "            19.85447763226303\n",
    "            0.5760456075230935\n",
    "            0.585240921194229\n",
    "            0.23620531480654705\n",
    "\n",
    "    - 5*(3+3) = 30 components\n",
    "\n",
    "            MAPE: 20.536346943303492\n",
    "            MAE: 0.5910318538690427\n",
    "            MSE: 0.5747574611935253\n",
    "            R^2: 0.24988722039619093\n",
    "\n",
    "    - 5*(2+2) = 20 components\n",
    "\n",
    "            19.35858512090129\n",
    "            0.5635286480139711\n",
    "            0.5433040170537524\n",
    "            0.2909369361542158\n",
    "            \n",
    "    - 5*(1+1) = 10 components\n",
    "    \n",
    "            20.250558073766026\n",
    "            0.5886032629783092\n",
    "            0.5852655385369406\n",
    "            0.23617318684890465\n",
    "\n",
    "    - 4*(4+4) = 32 components\n",
    "\n",
    "            MAPE: 22.031474920258777\n",
    "            MAE: 0.6380830691061724\n",
    "            MSE: 0.6277453555688914\n",
    "            R^2: 0.1807330128932182\n",
    "            \n",
    "   \n",
    "- PCA on flattened ICA channels and PCA separately on std features and abs_diffs features -> more research needed\n",
    "    \n",
    "    - 30 from 6*(5+5) = 30 components\n",
    "    \n",
    "            MAPE: 21.44571192549426\n",
    "            MAE: 0.6261229886064391\n",
    "            MSE: 0.6193306483997083\n",
    "            R^2: 0.19171500061917268\n",
    "       \n",
    "    - 30 from 6*(4+4) = 30 components\n",
    "    \n",
    "            MAPE: 21.24921683786726\n",
    "            MAE: 0.615903392719023\n",
    "            MSE: 0.608353978391557\n",
    "            R^2: 0.20604059185814527\n",
    "    \n",
    "    - 30 from 6*(3+3) = 30 components\n",
    "    \n",
    "            MAPE: 21.11155762577906\n",
    "            MAE: 0.6154703465611149\n",
    "            MSE: 0.604724357788718\n",
    "            R^2: 0.21077758960611548\n",
    "    \n",
    "    - 30 from 5*(5+5) = 30 components\n",
    "    \n",
    "            MAPE: 21.67828659174979\n",
    "            MAE: 0.6323799032109819\n",
    "            MSE: 0.6303037217274262\n",
    "            R^2: 0.17739410338791517\n",
    "    \n",
    "    - 30 from 5*(4+4) = 30 components\n",
    "    \n",
    "            MAPE: 21.317971705648553\n",
    "            MAE: 0.615749388609156\n",
    "            MSE: 0.6219822296997861\n",
    "            R^2: 0.1882544365488662\n",
    "    \n",
    "    - 30 from 5*(3+3) = 30 components\n",
    "    \n",
    "            MAPE: 21.220223056606272\n",
    "            MAE: 0.6138419950553302\n",
    "            MSE: 0.6099786827888294\n",
    "            R^2: 0.20392019914685844\n",
    "    \n",
    "    - 30 from 4*(4+4) = 30 components\n",
    "    \n",
    "            MAPE; 22.469724109237735\n",
    "            MAE: 0.6509080551097475\n",
    "            MSE: 0.6534929953956176\n",
    "            R^2: 0.14712991074547543\n",
    "   \n",
    "   \n",
    "- PCA on flattened ICA channels and (std + abs_diff) feature sets -> **the best results:**\n",
    "    \n",
    "        Example:\n",
    "        \n",
    "        \n",
    "     -  PCA: 18; ICA: 9:\n",
    "     \n",
    "             MAPE: 16.1051630051677\n",
    "             MAE: 0.481721094094576\n",
    "             MSE: 0.412577754295216\n",
    "             R^2: 0.461547057719921\n",
    "             \n",
    "             \n",
    "     - PCA: 18; ICA: 18:\n",
    "     \n",
    "             MAPE: 11.6348912814115\n",
    "             MAE: 0.35025761618236\n",
    "             MSE: 0.237247454583717\n",
    "             R^2: 0.690369660896321\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium.featurize\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(info_filename=info_filename)\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "Data is now read into dataframe and each epoch is a single record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "Sorting participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped_df = epochs_df.groupby(\"id\")\n",
    "epochs_df[\"error_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == ERROR).sum()\n",
    ")\n",
    "epochs_df[\"correct_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs_df = epochs_df.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "## Training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.svm import SVR\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Computes ICA and then at each channel computes CWT (ica_n_components = N).\n",
    "- For each band (frequency) from CWT set it computes features given in feature_dict parameter (eg. std or mean).\n",
    "- Then it computes PCA on flattened ICA channels and features (outer_components = N)\n",
    "- Ending feature vector has shape: outer_components from (ica_n_components * len(feature_dict) * frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset of standard features for EEG analysis provided by Guo et al. (2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_signal(t, m, e):\n",
    "    return np.std(m)\n",
    "\n",
    "\n",
    "def abs_diffs_signal(t, m, e):\n",
    "    return np.sum(np.abs(np.diff(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guo_features = {\n",
    "    \"std\": std_signal,\n",
    "    \"abs_diffs\": abs_diffs_signal,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressions grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning! It takes a lot of time! One run with ICA_n = 35 takes ~20 min** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a pipeline which allows manipulation of vectorization's parameters. Base_steps dictionary consists of all steps of vectorization including standarization of data.\n",
    "\n",
    "In rate_regression function, using GridSearchCV, cross-validation splitting strategy can be specified. Default cv = 5.\n",
    "Results of cross-validated search are in **grid_search.cv_results** and chosen model is in **grid_search.best_estimator_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined data transformers - custom data transformation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IcaPreprocessingTransformer():\n",
    "    def transform(X):\n",
    "        timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "        return timepoints_per_channel.T\n",
    "\n",
    "    return FunctionTransformer(func=transform)\n",
    "\n",
    "\n",
    "def CwtVectorizer(timepoints_count, mwt=\"morl\", cwt_density=2):\n",
    "    def transform(X):\n",
    "        X_ica_transposed = X.T\n",
    "        ica_n_components = X.shape[1]\n",
    "\n",
    "        epochs_count = int(X_ica_transposed.shape[1] / timepoints_count)\n",
    "        data_per_channel = X_ica_transposed.reshape(\n",
    "            ica_n_components, epochs_count, timepoints_count\n",
    "        )\n",
    "\n",
    "        cwt_per_channel = []\n",
    "        for data in data_per_channel:\n",
    "            data_cwt = np.array([cwt(epoch, mwt, cwt_density) for epoch in data])\n",
    "            cwt_per_channel.append(data_cwt)\n",
    "        cwt_per_channel = np.array(cwt_per_channel)\n",
    "        return cwt_per_channel\n",
    "\n",
    "    return FunctionTransformer(func=transform)\n",
    "\n",
    "\n",
    "def CwtFeatureVectorizer(feature_dict):\n",
    "    def transform(X):\n",
    "        vectorized_data = []\n",
    "\n",
    "        for data_cwt in X:\n",
    "            # cesium functions\n",
    "            feature_set_cwt = cesium.featurize.featurize_time_series(\n",
    "                times=None,\n",
    "                values=data_cwt,\n",
    "                errors=None,\n",
    "                features_to_use=list(feature_dict.keys()),\n",
    "                custom_functions=feature_dict,\n",
    "            )\n",
    "            features_per_epoch = feature_set_cwt.to_numpy()\n",
    "            vectorized_data.append(features_per_epoch)\n",
    "        vectorized_data = np.array(vectorized_data)\n",
    "        return vectorized_data\n",
    "\n",
    "    return FunctionTransformer(func=transform)\n",
    "\n",
    "\n",
    "# reshape data from (channels x epoch x features) to (epochs x channles x features)\n",
    "# and then flatten it to (epoch x channels*features)\n",
    "def PostprocessingTransformer():\n",
    "    def transform(X):\n",
    "        vectorized_data = np.stack(X, axis=1)\n",
    "        epochs_per_channel_feature = vectorized_data.reshape(\n",
    "            vectorized_data.shape[0], -1\n",
    "        )\n",
    "        return epochs_per_channel_feature\n",
    "\n",
    "    return FunctionTransformer(func=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction with SVR-rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"epoch\"].to_list())\n",
    "y = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"Rumination Full Scale\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_steps = [\n",
    "    (\"ica_preprocessing\", IcaPreprocessingTransformer()),\n",
    "    (\"ica\", FastICA(random_state=5)),\n",
    "    (\"cwt\", CwtVectorizer(timepoints_count=X.shape[-1])),\n",
    "    (\"cwt_feature\", CwtFeatureVectorizer(feature_dict=guo_features)),\n",
    "    (\"postprocessing\", PostprocessingTransformer()),\n",
    "    (\"pca\", PCA(random_state=5)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svr\", SVR()),\n",
    "]\n",
    "\n",
    "regressor_params = dict(\n",
    "    ica__n_components=[1, 2, 3, 4],\n",
    "    pca__n_components=[1, 2, 3, 4],\n",
    "    # svr__C=np.arange(1, 2, 1),\n",
    "    # svr__gamma=[0.1],\n",
    "    # svr__epsilon=[0.1],\n",
    "    svr__kernel=[\"linear\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = Pipeline(steps=base_steps)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, shuffle=False\n",
    ")\n",
    "\n",
    "# X_test, y_test = X_train, y_train\n",
    "\n",
    "for params in ParameterGrid(regressor_params):\n",
    "    pipeline.set_params(**params)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(params, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
