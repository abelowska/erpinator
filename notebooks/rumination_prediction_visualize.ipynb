{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rumination prediction with cesium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium.featurize\n",
    "from time import sleep\n",
    "from random import shuffle\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore np.corrcoef RuntimeWarnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import HTML\n",
    "\n",
    "# display(HTML('<span style=\"color: #ff0000\">red</span>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# grid_search = GridSearchCV(\n",
    "#     pipeline,\n",
    "#     regressor_params,\n",
    "#     cv=4,\n",
    "#     scoring={\"r2\": \"r2\"},\n",
    "#     refit=\"r2\",\n",
    "#     n_jobs=-1,\n",
    "#     verbose=10,\n",
    "# )\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# predictions = grid_search.predict(X_test)\n",
    "# r2 = grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(info_filename=info_filename)\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "Data is now read into dataframe and each epoch is a single record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "Sorting participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped_df = epochs_df.groupby(\"id\")\n",
    "epochs_df[\"error_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == ERROR).sum()\n",
    ")\n",
    "epochs_df[\"correct_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs_df = epochs_df.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "false"
   },
   "source": [
    "## Training and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Computes ICA and then at each channel computes CWT (ica_n_components = N).\n",
    "- For each band (frequency) from CWT set it computes features given in feature_dict parameter (eg. std or mean).\n",
    "- Then it computes PCA on flattened ICA channels and features (outer_components = N)\n",
    "- Ending feature vector has shape: outer_components from (ica_n_components * len(feature_dict) * frequencies)\n",
    "\n",
    "Subset of standard features for EEG analysis provided by Guo et al. (2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressions grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning! It takes a lot of time! One run with ICA_n = 35 takes ~20 min** \n",
    "\n",
    "It is a pipeline which allows manipulation of vectorization's parameters. Base_steps dictionary consists of all steps of vectorization including standarization of data.\n",
    "\n",
    "In rate_regression function, using GridSearchCV, cross-validation splitting strategy can be specified. Default cv = 5.\n",
    "Results of cross-validated search are in **grid_search.cv_results** and chosen model is in **grid_search.best_estimator_**\n",
    "\n",
    "Defined data transformers - custom data transformation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_signal(t, m, e):\n",
    "    return np.std(m)\n",
    "\n",
    "\n",
    "def abs_diffs_signal(t, m, e):\n",
    "    return np.sum(np.abs(np.diff(m)))\n",
    "\n",
    "\n",
    "def peak_ind(t, m, e):\n",
    "    return np.argmax(np.abs(m))\n",
    "\n",
    "\n",
    "def peak_value(t, m, e):\n",
    "    ind = np.argmax(np.abs(m))\n",
    "    return m[ind]\n",
    "\n",
    "\n",
    "shape_features = {\n",
    "    \"std\": std_signal,\n",
    "    \"abs_diffs\": abs_diffs_signal,\n",
    "}\n",
    "\n",
    "peak_features = {\n",
    "    \"peak_ind\": peak_ind,\n",
    "    \"peak_value\": peak_value,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IcaPreprocessing(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "        return timepoints_per_channel.T\n",
    "\n",
    "\n",
    "class Cwt(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, timepoints_count, mwt=\"morl\", cwt_density=2):\n",
    "        super().__init__()\n",
    "        self.timepoints_count = timepoints_count\n",
    "        self.mwt = mwt\n",
    "        self.cwt_density = cwt_density\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ica_transposed = X.T\n",
    "        ica_n_components = X.shape[1]\n",
    "\n",
    "        epochs_count = int(X_ica_transposed.shape[1] / self.timepoints_count)\n",
    "        data_per_channel = X_ica_transposed.reshape(\n",
    "            ica_n_components, epochs_count, self.timepoints_count\n",
    "        )\n",
    "\n",
    "        cwt_per_channel = []\n",
    "        for data in data_per_channel:\n",
    "            data_cwt = np.array(\n",
    "                [cwt(epoch, self.mwt, self.cwt_density) for epoch in data]\n",
    "            )\n",
    "            cwt_per_channel.append(data_cwt)\n",
    "        cwt_per_channel = np.array(cwt_per_channel)\n",
    "        return cwt_per_channel\n",
    "\n",
    "\n",
    "class CwtFeatureVectorizer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, feature_dict):\n",
    "        super().__init__()\n",
    "        self.feature_dict = feature_dict\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectorized_data = []\n",
    "        for data_cwt in X:\n",
    "            # cesium functions\n",
    "            feature_set_cwt = cesium.featurize.featurize_time_series(\n",
    "                times=None,\n",
    "                values=data_cwt,\n",
    "                errors=None,\n",
    "                features_to_use=list(self.feature_dict.keys()),\n",
    "                custom_functions=self.feature_dict,\n",
    "            )\n",
    "            features_per_epoch = feature_set_cwt.to_numpy()\n",
    "            vectorized_data.append(features_per_epoch)\n",
    "        vectorized_data = np.array(vectorized_data)\n",
    "        return vectorized_data\n",
    "\n",
    "\n",
    "# reshape data from (channels x epoch x features) to (epochs x channles x features)\n",
    "# and then flatten it to (epoch x channels*features)\n",
    "class PostprocessingTransformer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectorized_data = np.stack(X, axis=1)\n",
    "        epochs_per_channel_feature = vectorized_data.reshape(\n",
    "            vectorized_data.shape[0], -1\n",
    "        )\n",
    "        return epochs_per_channel_feature\n",
    "\n",
    "\n",
    "class PCAForEachChannel(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, n_components=3, random_state=0):\n",
    "        super().__init__()\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # X has a shape CHANNELS x EPOCHS x FREQ x TIMEPOINTS\n",
    "        # or CHANNELS x EPOCHS x ...\n",
    "        self.PCAs = []\n",
    "        for channel in X:\n",
    "            flattened = channel.reshape(channel.shape[0], -1)\n",
    "            # flattened has a shape EPOCHS x (FREQ*TIMEPOINTS)\n",
    "            pca = PCA(n_components=self.n_components, random_state=self.random_state)\n",
    "            pca.fit(flattened)\n",
    "            self.PCAs.append(pca)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, copy=True):\n",
    "        features = []\n",
    "        for channel, pca in zip(X, self.PCAs):\n",
    "            flattened = channel.reshape(channel.shape[0], -1)\n",
    "            # flattened has a shape EPOCHS x (FREQ*TIMEPOINTS)\n",
    "            ch_transformed = pca.transform(flattened)\n",
    "            features.append(ch_transformed)\n",
    "\n",
    "        features = np.array(features)\n",
    "        # transform it from shape ICA_COMP x EPOCH x WAVELET_COMP\n",
    "        #                      to EPOCH x ICA_COMP x WAVELET_COMP\n",
    "        features = features.transpose((1, 0, 2))\n",
    "\n",
    "        # flatten features into shape EPOCH x (ICA_COMP*WAVELET_COMP)\n",
    "        features = features.reshape(features.shape[0], -1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"epoch\"].to_list())\n",
    "y = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"Rumination Full Scale\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "base_steps = [\n",
    "    (\"ica_preprocessing\", IcaPreprocessing()),\n",
    "    (\"ica\", FastICA(random_state=0)),\n",
    "    (\"featurize\", FeatureUnion([\n",
    "        ('shape', Pipeline([\n",
    "            (\"cwt\", Cwt(mwt=\"morl\", timepoints_count=X.shape[-1])),\n",
    "            (\"cwt_feature\", CwtFeatureVectorizer(feature_dict=shape_features)),\n",
    "            (\"pca\", PCAForEachChannel(n_components=3, random_state=0)),\n",
    "        ])),\n",
    "        ('peaks', Pipeline([\n",
    "            (\"cwt\", Cwt(mwt=\"mexh\", timepoints_count=X.shape[-1])),\n",
    "            (\"cwt_feature\", CwtFeatureVectorizer(feature_dict=peak_features)),\n",
    "            (\"pca\", PCAForEachChannel(n_components=3, random_state=0)),\n",
    "        ])),\n",
    "    ])),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svr\", SVR()),\n",
    "    # (\"regression\", Lasso()),\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "# base_steps = [\n",
    "#     (\"ica_preprocessing\", IcaPreprocessingTransformer()),\n",
    "#     (\"ica\", FastICA(random_state=0)),\n",
    "#     (\"cwt\", Cwt(mwt=\"morl\", timepoints_count=X.shape[-1])),\n",
    "#     (\"cwt_feature\", CwtFeatureVectorizer(feature_dict=shape_features)),\n",
    "#     (\"pca\", PCAForEachChannel(random_state=0)),\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "#     (\"svr\", SVR()),\n",
    "#     # (\"regression\", Lasso()),\n",
    "# ]\n",
    "\n",
    "# base_steps = [\n",
    "#     (\"ica_preprocessing\", IcaPreprocessingTransformer()),\n",
    "#     (\"ica\", FastICA(random_state=0)),\n",
    "#     (\"cwt\", Cwt(timepoints_count=X.shape[-1])),\n",
    "#     (\"cwt_feature\", CwtFeatureVectorizer(feature_dict=guo_features)),\n",
    "#     (\"postprocessing\", PostprocessingTransformer()),\n",
    "#     (\"pca\", PCA(random_state=0)),\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "#     (\"svr\", SVR()),\n",
    "#     # (\"regression\", Lasso()),\n",
    "# ]\n",
    "\n",
    "# base_steps = [\n",
    "#     (\"ica_preprocessing\", IcaPreprocessingTransformer()),\n",
    "#     (\"ica\", FastICA(random_state=0)),\n",
    "#     (\"cwt\", Cwt(timepoints_count=X.shape[-1])),\n",
    "#     (\"pca\", PCAForEachChannel(random_state=0)),\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "#     # (\"svr\", SVR()),\n",
    "#     (\"regression\", Lasso()),\n",
    "# ]\n",
    "\n",
    "regressor_params = dict(\n",
    "    ica__n_components=[5],\n",
    "    # pca__n_components=[5],\n",
    "    #     cwt__mwt=[\"mexh\"],\n",
    "    # regression__alpha=[0.3],\n",
    "    # svr__C=[0.01, 0.02, 0.04, 0.1],\n",
    "    svr__C=[0.1],\n",
    "    # svr__epsilon=[0.1],\n",
    "    # svr__kernel=[\"linear\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = \"/home/filip/.erpinator_cache\"\n",
    "pipeline = Pipeline(base_steps, memory=cachedir)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=0, shuffle=False\n",
    "# )\n",
    "\n",
    "# X_test, y_test = X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\" \" * 102 + \"corr           r2\")\n",
    "\n",
    "# get params randomly\n",
    "all_params = list(ParameterGrid(regressor_params))\n",
    "# shuffle(all_params)\n",
    "    \n",
    "for params in all_params:\n",
    "    pipeline.set_params(**params)\n",
    "\n",
    "    scores = []\n",
    "    skf = KFold(n_splits=2)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        scores.append([corr, r2])\n",
    "        \n",
    "    # print scores\n",
    "    print( f\"{str(params):95}\" , end=' ')\n",
    "    means = np.mean(scores, axis=0)\n",
    "    sems = scipy.stats.sem(scores, axis=0)\n",
    "    for mean, sem in zip(means, sems):\n",
    "        print(f\"{mean:5.2f}±{sem:4.2f}\", end=\"   \")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
