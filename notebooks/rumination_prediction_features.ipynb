{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Rumination prediction with statistical feature functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprisingly good. The best statistical functions: **abs_diff** and std, but std perform worse than abs_diff.\n",
    "\n",
    "Function is calculated for each band from cwt separately, the PCA is computed for principal components extraction. Everything is made in each ICA channel separately.\n",
    "\n",
    "**Research for the best amount of ICA and PCA components is needed.**\n",
    "\n",
    "Results for ICA= 6, PCA=5:\n",
    "\n",
    "- Vectorization with mean function: \n",
    "\n",
    "        22.262470602898563\n",
    "        0.6553535562156957\n",
    "        0.6701843820323544\n",
    "        0.1253460744825673\n",
    "\n",
    "\n",
    "- Vectorization with std function: \n",
    "\n",
    "        22.356530287958208\n",
    "        0.6528160797078707\n",
    "        0.6622073864400982\n",
    "        0.1357568072535158\n",
    "\n",
    "\n",
    "- Vectorization with mean2 function: \n",
    "\n",
    "        23.522736451248715\n",
    "        0.6898912347618341\n",
    "        0.7282693285547807\n",
    "        0.049539732449879414\n",
    "        \n",
    "        \n",
    "- **Vectorization with abs_diffs function:**\n",
    "\n",
    "        20.316528317106584\n",
    "        0.598258644924345\n",
    "        0.5699238437388514\n",
    "        0.256195547767771\n",
    "\n",
    "\n",
    "- Vectorization with skew function: \n",
    "\n",
    "        24.617083295052065\n",
    "        0.7180929985106101\n",
    "        0.7818632257758061\n",
    "        -0.02040536601092713\n",
    "\n",
    "\n",
    "- Vectorization with kurtosis function: \n",
    "\n",
    "        24.84531814461336\n",
    "        0.7272395445327172\n",
    "        0.8003162367100916\n",
    "        -0.04448828838871188\n",
    "        \n",
    "        \n",
    "- Vectorization with variation function: \n",
    "\n",
    "        26.167193910667653\n",
    "        0.7467452978933837\n",
    "        0.8497374027769138\n",
    "        -0.10898757852870222\n",
    "        \n",
    "        \n",
    "- Vectorization with median_abs_deviation function: \n",
    "\n",
    "        22.938335987122773\n",
    "        0.6675833664351787\n",
    "        0.6887154337895299\n",
    "        0.1011613014590178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data load all info from given file but ones can specify it passing list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' | 'annot' | 'peak-to-peak'\n",
    "\n",
    "        Whether the epochs with overlapping bad segments are rejected by default.\n",
    "\n",
    "        'auto' means that bad segments are rejected automatically.\n",
    "        'annot' rejection based on annotations and reject only channels annotated in .vmrk file as\n",
    "        'bad'.\n",
    "        'peak-to-peak' rejection based on peak-to-peak amplitude of channels.\n",
    "\n",
    "        Rejected with 'annot' and 'amplitude' channels are zeroed.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(\"../data/\" + file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(\"../data/\" + annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = True\n",
    "\n",
    "    if reject_bad_segments != \"auto\":\n",
    "        this_reject_by_annotation = False\n",
    "\n",
    "    # Read epochs\n",
    "    temp_epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    if reject_bad_segments == \"annot\":\n",
    "        custom_annotations = get_annotations(annot_file)\n",
    "        bads = get_bads_by_annotation(custom_annotations)\n",
    "    elif reject_bad_segments == \"peak-to-peak\":\n",
    "        bads = get_bads_by_peak_to_peak_amplitude(temp_epochs)\n",
    "    else:\n",
    "        epochs = temp_epochs\n",
    "        return epochs\n",
    "\n",
    "    if mask is None:\n",
    "        epochs = clear_bads(temp_epochs, bads)\n",
    "    elif len(mask) == 64:\n",
    "        epochs = reject_with_mask(temp_epochs, mask, bads)\n",
    "    else:\n",
    "        print(\n",
    "            \"Given mask has wrong shape. Expected len of 64 but got {}\".format(\n",
    "                len(mask)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=[\"Rumination Full Scale\"],\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    header_files = glob.glob(\"../data/responses/*.vhdr\")\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 10 or len(correct) < 10:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, correct, error, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, correct, error, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        rumination_df = pd.read_csv(info_filename, usecols=[\"File\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"File\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "\n",
    "    for epoch in correct:\n",
    "        epoch_df = pd.DataFrame(\n",
    "            {\"id\": [id], \"epoch\": [epoch], \"marker\": [CORRECT]}\n",
    "        ).join(info_df)\n",
    "        participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    for epoch in error:\n",
    "        epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [epoch], \"marker\": [ERROR]}).join(\n",
    "            info_df\n",
    "        )\n",
    "        participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(info_filename=info_filename)\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data is now read into dataframe and each epoch is a single record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sorting participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped_df = epochs_df.groupby(\"id\")\n",
    "epochs_df[\"error_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == ERROR).sum()\n",
    ")\n",
    "epochs_df[\"correct_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs_df = epochs_df.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ICA reduces channles from 64 to given amount of independent components\n",
    "- Continous Wavelet Transform decompose signal of channel from each epoch into set of wavelets functions\n",
    "- Feature function given in peature_function parameter is calculated for each band from CWT\n",
    "- PCA reducing dimention of features into computed best ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "\n",
    "def vectorize(\n",
    "    X,\n",
    "    feature_function,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"PCA\",\n",
    "    extracted_n_components=3,\n",
    "):\n",
    "    print(\"X shape: {}\".format(X.shape))\n",
    "\n",
    "    # compute ICA for reducing dim from 64-channel to ica-n-components signal.\n",
    "    # for ICA shape must be like  (n_samples, n_features) -> timepoints_per_channel.shape.T == (epochs*timepoints, num_of_channels)\n",
    "    timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    X_ica = ica.fit_transform(timepoints_per_channel.T)\n",
    "\n",
    "    # reshaping X_ica for recover (channel, epoch, timepoints) structure instead (epochs*timepoints, channel)\n",
    "    X_ica_transposed = X_ica.T\n",
    "    data_per_channel = X_ica_transposed.reshape(\n",
    "        ica_n_components, X.shape[0], X.shape[-1]\n",
    "    )\n",
    "\n",
    "    vectorized_data = []\n",
    "\n",
    "    for data in data_per_channel:\n",
    "        data_cwt = np.array([cwt(epoch, mwt, cwt_density) for epoch in data])\n",
    "\n",
    "        # for calculating features per scale data must be transpose\n",
    "        # from shape (epoch, frequency, times) to shape (frequency,epoch,times)\n",
    "        data_per_scale = np.transpose(data_cwt, (1, 0, 2))\n",
    "        epochs_per_scale = []\n",
    "\n",
    "        for scale in data_per_scale:\n",
    "            this_epoch = []\n",
    "            for epoch in scale:\n",
    "                epoch_stats = feature_function(epoch)\n",
    "                this_epoch.append(epoch_stats)\n",
    "\n",
    "            epochs_per_scale.append(this_epoch)\n",
    "\n",
    "        epochs_per_scale = np.array(epochs_per_scale)\n",
    "        scales_per_epoch = np.transpose(epochs_per_scale, (1, 0))\n",
    "\n",
    "        pca = PCA(n_components=extracted_n_components)\n",
    "        pca_components_per_epoch = pca.fit_transform(scales_per_epoch)\n",
    "        vectorized_data.append(pca_components_per_epoch)\n",
    "\n",
    "    vectorized_data = np.array(vectorized_data)\n",
    "    vectorized_data = np.stack(vectorized_data, axis=1)\n",
    "    epochs_per_channel_feature = vectorized_data.reshape(vectorized_data.shape[0], -1)\n",
    "\n",
    "    print(\"Vectorized X shape: {}\".format(epochs_per_channel_feature.shape))\n",
    "\n",
    "    return epochs_per_channel_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"epoch\"].to_list())\n",
    "y = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"Rumination Full Scale\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_X_df = pd.DataFrame(data={\"y\": y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization with different statistical functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "def mean_signal(m):\n",
    "    return np.mean(m)\n",
    "\n",
    "\n",
    "def std_signal(m):\n",
    "    return np.std(m)\n",
    "\n",
    "\n",
    "def mean_square_signal(m):\n",
    "    return np.mean(m ** 2)\n",
    "\n",
    "\n",
    "def abs_diffs_signal(m):\n",
    "    return np.sum(np.abs(np.diff(m)))\n",
    "\n",
    "\n",
    "def skew_signal(m):\n",
    "    return scipy.stats.skew(m)\n",
    "\n",
    "\n",
    "def kurtosis_signal(m):\n",
    "    return scipy.stats.kurtosis(m)\n",
    "\n",
    "\n",
    "def variation_signal(m):\n",
    "    return scipy.stats.variation(m)\n",
    "\n",
    "\n",
    "def median_abs_deviation_signal(m):\n",
    "    return scipy.stats.median_abs_deviation(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_features_dict = {\n",
    "    \"mean\": mean_signal,\n",
    "    \"std\": std_signal,\n",
    "    \"mean2\": mean_square_signal,\n",
    "    \"abs_diffs\": abs_diffs_signal,\n",
    "    \"skew\": skew_signal,\n",
    "    \"kurtosis\": kurtosis_signal,\n",
    "    \"variation\": variation_signal,\n",
    "    \"median_abs_deviation\": median_abs_deviation_signal,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization. Results added to dataframe. Column_name = func_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = 9\n",
    "pca = 2\n",
    "for func_name, func in stat_features_dict.items():\n",
    "    vectorized_X = vectorize(\n",
    "        X,\n",
    "        feature_function=func,\n",
    "        ica_n_components=ica,\n",
    "        mwt=\"morl\",\n",
    "        extracted_n_components=pca,\n",
    "    )\n",
    "    vectorized_X_df[func_name] = vectorized_X.tolist()\n",
    "    vectorized_X_df[\"ica_n_components\"] = ica\n",
    "    vectorized_X_df[\"pca_n_components\"] = pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick vectorization wit only one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = 9\n",
    "pca = 2\n",
    "func_name = \"std\"\n",
    "func = stat_features_dict[func_name]\n",
    "\n",
    "vectorized_X = vectorize(\n",
    "    X,\n",
    "    feature_function=func,\n",
    "    ica_n_components=ica,\n",
    "    mwt=\"morl\",\n",
    "    extracted_n_components=pca,\n",
    ")\n",
    "\n",
    "vectorized_X_df[func_name] = vectorized_X.tolist()\n",
    "vectorized_X_df[\"ica_n_components\"] = ica\n",
    "vectorized_X_df[\"pca_n_components\"] = pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    ## Note: does not handle mix 1d representation\n",
    "    # if _is_1d(y_true):\n",
    "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and prediction with SVR model all types of vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func_name in stat_features_dict:\n",
    "    vectorized_X = np.array(vectorized_X_df[func_name].to_list())\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        vectorized_X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(\"Vectorization with {} function: \".format(func_name))\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    rescaled_X_train = scaler.transform(X_train)\n",
    "    model = SVR(kernel=\"rbf\", C=2, gamma=0.1, epsilon=0.1)\n",
    "    model.fit(rescaled_X_train, y_train)\n",
    "\n",
    "    # transform the validation dataset\n",
    "    rescaled_X_test = scaler.transform(X_test)\n",
    "    predictions = model.predict(rescaled_X_test)\n",
    "    print(mean_absolute_percentage_error(y_test, predictions))\n",
    "    print(mean_absolute_error(y_test, predictions))\n",
    "    print(mean_squared_error(y_test, predictions))\n",
    "    print(model.score(rescaled_X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    vectorized_X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaled_X_train = scaler.transform(X_train)\n",
    "model = SVR(kernel=\"rbf\", C=1, gamma=0.1, epsilon=0.1)\n",
    "model.fit(rescaled_X_train, y_train)\n",
    "\n",
    "# transform the validation dataset\n",
    "rescaled_X_test = scaler.transform(X_test)\n",
    "predictions = model.predict(rescaled_X_test)\n",
    "print(mean_absolute_percentage_error(y_test, predictions))\n",
    "print(mean_absolute_error(y_test, predictions))\n",
    "print(mean_squared_error(y_test, predictions))\n",
    "print(model.score(rescaled_X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressions grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_steps = [(\"scaler\", StandardScaler())]\n",
    "\n",
    "\n",
    "def rate_regression(\n",
    "    X_train, y_train, X_test, y_test, regressor, regressor_params, cv=2\n",
    "):\n",
    "    pipeline = Pipeline(steps=basic_steps + [regressor])\n",
    "    param_grid = regressor_params\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv,\n",
    "        scoring={\"r2\": \"r2\", \"mae\": \"neg_mean_absolute_error\"},\n",
    "        refit=\"r2\",\n",
    "        n_jobs=3,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    predictions = grid_search.predict(X_test)\n",
    "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = grid_search.score(X_test, y_test)\n",
    "    print(f\"Best result: MAPE {mape} MAE {mae} MSE {mse} R^2 {r2}\")\n",
    "    grid_search_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    return grid_search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = (\"svr\", SVR(kernel=\"rbf\"))\n",
    "svr_params = dict(\n",
    "    svr__C=np.arange(1, 5, 1),\n",
    "    svr__gamma=[0.1],\n",
    "    svr__epsilon=[0.1],\n",
    ")\n",
    "svr_grid_search_df = rate_regression(X_train, y_train, X_test, y_test, svr, svr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = (\"knn\", KNeighborsRegressor())\n",
    "knn_params = dict(\n",
    "    knn__n_neighbors=np.arange(5, 100, 5),\n",
    ")\n",
    "knn_grid_search_df = rate_regression(X_train, y_train, X_test, y_test, knn, knn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = (\"gbr\", GradientBoostingRegressor())\n",
    "gbr_params = dict(\n",
    "    gbr__n_estimators=np.arange(1, 100, 5),\n",
    ")\n",
    "gbr_grid_search_df = rate_regression(X_train, y_train, X_test, y_test, gbr, gbr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy Classifier for baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dummy_regr.predict(X_test)\n",
    "print(mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(model.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
