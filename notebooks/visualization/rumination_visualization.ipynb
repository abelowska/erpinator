{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import inspect\n",
    "import itertools\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "from cachier import cachier\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import HBox, VBox\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import *\n",
    "from architecture import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# ignore FastICA did not converge warnings\n",
    "# TODO investigate why doesn't it converge\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs = pd.read_pickle(pickled_data_filename)\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs = create_df_data(info_filename=info_filename)\n",
    "    epochs.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs.to_pickle(pickled_data_filename)\n",
    "\n",
    "# epochs\n",
    "\n",
    "# add new columns with info about error/correct responses amount\n",
    "grouped = epochs.groupby(\"id\")\n",
    "epochs[\"error_sum\"] = grouped[[\"marker\"]].transform(lambda x: (x.values == ERROR).sum())\n",
    "epochs[\"correct_sum\"] = grouped[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs = epochs.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")\n",
    "# epochs\n",
    "\n",
    "_mne_epochs = load_epochs_from_file(\"../../data/responses/GNG_AA0303-64 el.vhdr\")\n",
    "times = _mne_epochs.times\n",
    "\n",
    "_channel_info = _mne_epochs.info[\"chs\"]\n",
    "\n",
    "log_freq = np.log2(get_frequencies())  # for plotting CWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../public_data/regression_PCA_error.pkl\", \"rb\") as file:\n",
    "    models_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = models_data.loc[\n",
    "    (models_data[\"pipeline_name\"] == \"PCA_15_bins\") & (models_data[\"model\"] == \"en\")\n",
    "]\n",
    "\n",
    "model = model_data[\"best_estimator\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_channels = model[\"channels_filtering\"].channel_list\n",
    "# persisted model numerates channels from 1, not 0\n",
    "channel_index_list = np.array(significant_channels) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load channel data\n",
    "channel_locations = np.array([ch[\"loc\"][:3] for ch in _channel_info])\n",
    "channel_names = np.array([ch[\"ch_name\"] for ch in _channel_info])\n",
    "\n",
    "channel_colors = channel_locations - channel_locations.min(axis=0)\n",
    "channel_colors /= channel_colors.max(axis=0)\n",
    "channel_colors = channel_colors * 255 // 1\n",
    "channel_colors = [f\"rgb({c[0]:.0f},{c[1]:.0f},{c[2]:.0f})\" for c in channel_colors]\n",
    "channel_colors = np.array(channel_colors)\n",
    "\n",
    "# trim it to only 15 top electrodes\n",
    "channel_names = channel_names[channel_index_list]\n",
    "channel_locations = channel_locations[channel_index_list]\n",
    "channel_colors = channel_colors[channel_index_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize spatial components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ica_comp(ica_comp, scale=1):\n",
    "    x_loc, y_loc, z_loc = channel_locations.T\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(**base_layout)\n",
    "    fig.update_layout(width=350 * scale, height=350 * scale)\n",
    "\n",
    "    # sort by z_loc for prettier printing\n",
    "    info = list(zip(z_loc, x_loc, y_loc, channel_names, ica_comp))\n",
    "    info.sort()\n",
    "    _, _x_loc, _y_loc, _channel_names, _component = zip(*info)\n",
    "\n",
    "    amp = max(np.abs(_component))\n",
    "\n",
    "    fig.add_scatter(\n",
    "        x=_x_loc,\n",
    "        y=_y_loc,\n",
    "        #         text=_channel_names,\n",
    "        text=_component,\n",
    "        marker_color=_component,\n",
    "        mode=\"markers\",\n",
    "        marker_size=42 * scale,\n",
    "        marker_colorscale=blue_black_red,\n",
    "        marker_cmax=amp,\n",
    "        marker_cmin=-amp,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_erps_after_spatial_filter(\n",
    "    spatial_filter, epochs, individual, plot_limit, erp_type, max_amp, scale=1\n",
    "):\n",
    "    if len(spatial_filter) != len(_channel_info):\n",
    "        # the channels are cropped\n",
    "        # converse spatial_filter back to original 64 channels\n",
    "        # with zeroes for cropped channels\n",
    "        sparse_spatial_filt = np.zeros(len(_channel_info))\n",
    "        for index, coef in zip(channel_index_list, spatial_filter):\n",
    "            sparse_spatial_filt[index] = coef\n",
    "        spatial_filter = sparse_spatial_filt\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(**base_layout)\n",
    "    fig.update_layout(\n",
    "        height=350 * scale,\n",
    "        width=600 * scale,\n",
    "        xaxis_range=[times[0], times[-1]],\n",
    "        yaxis_range=[-max_amp, max_amp],\n",
    "    )\n",
    "\n",
    "    if individual:\n",
    "        grouped = epochs.groupby([\"id\"])\n",
    "        for participant_id in epochs[\"id\"].unique()[:plot_limit]:\n",
    "            df = grouped.get_group(participant_id)\n",
    "\n",
    "            err = np.stack(df.loc[df[\"marker\"] == ERROR][\"epoch\"].values)\n",
    "            cor = np.stack(df.loc[df[\"marker\"] == CORRECT][\"epoch\"].values)\n",
    "            all_ = np.stack(df[\"epoch\"].values)\n",
    "            cor_mean = cor.mean(axis=0)\n",
    "            err_mean = err.mean(axis=0)\n",
    "            all_mean = all_.mean(axis=0)\n",
    "            err_erp = np.tensordot(err_mean, spatial_filter, axes=([0], [0]))\n",
    "            cor_erp = np.tensordot(cor_mean, spatial_filter, axes=([0], [0]))\n",
    "            all_erp = np.tensordot(all_mean, spatial_filter, axes=([0], [0]))\n",
    "            dif_erp = cor_erp - err_erp\n",
    "\n",
    "            if erp_type == \"correct\":\n",
    "                fig.add_scatter(x=times, y=cor_erp)\n",
    "            elif erp_type == \"error\":\n",
    "                fig.add_scatter(x=times, y=err_erp)\n",
    "            elif erp_type == \"all\":\n",
    "                fig.add_scatter(x=times, y=all_erp)\n",
    "            elif erp_type == \"difference\":\n",
    "                fig.add_scatter(x=times, y=dif_erp)\n",
    "            else:\n",
    "                raise ValueError(\"bad argument for erp_type\")\n",
    "            fig.update_traces(line_width=1)\n",
    "\n",
    "    else:\n",
    "        err = np.stack(epochs.loc[epochs[\"marker\"] == ERROR][\"epoch\"].values)\n",
    "        cor = np.stack(epochs.loc[epochs[\"marker\"] == CORRECT][\"epoch\"].values)\n",
    "        cor_mean = cor.mean(axis=0)\n",
    "        err_mean = err.mean(axis=0)\n",
    "        err_erp = np.tensordot(err_mean, spatial_filter, axes=([0], [0]))\n",
    "        cor_erp = np.tensordot(cor_mean, spatial_filter, axes=([0], [0]))\n",
    "\n",
    "        fig.add_scatter(x=times, y=err_erp, line_color=\"red\")\n",
    "        fig.add_scatter(x=times, y=cor_erp, line_color=\"green\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def visualize_spatial_components(\n",
    "    pipeline,\n",
    "    epochs,\n",
    "    individual=False,\n",
    "    plot_limit=200,\n",
    "    erp_type=None,\n",
    "    max_amp=0.0001,\n",
    "    scale=1,\n",
    "    flip_mask=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    pipeline\n",
    "        sklearn Pipeline to be visualized\n",
    "    epochs\n",
    "        dataframe with all the epochs\n",
    "    individual\n",
    "        whether to plot ERP for each person instead of averaged across everyone\n",
    "    plot_limit\n",
    "        how many people to plot - too high makes the plot hard to read\n",
    "        only has effect if individual==True\n",
    "    erp_type\n",
    "        only has effect if individual==True\n",
    "        possible values:\n",
    "            'correct'     plot the average of all correct epochs for each person\n",
    "            'error'       plot the average of all error epochs for each person\n",
    "            'all'         plot the average of all the epochs for each person\n",
    "            'difference'  plot the average of correct epochs minus average of error epochs for each person\n",
    "    max_amp\n",
    "        maximum amplitude for component plotting\n",
    "    scale\n",
    "        scale of the plots\n",
    "    flip_mask\n",
    "        optional array of 1s and -1s\n",
    "        its length must be the same as the number of spatial components\n",
    "        setting -1 for a corresponding spatial component flips its sign for better readability\n",
    "    \"\"\"\n",
    "    fitted_steps = dict(pipeline.steps)\n",
    "    spatial = fitted_steps[\"spatial_filter\"]\n",
    "\n",
    "    for spatial_comp_num, spatial_comp in enumerate(spatial.components_):\n",
    "        if flip_mask is not None:\n",
    "            spatial_comp = spatial_comp * flip_mask[spatial_comp_num]\n",
    "        display(\n",
    "            HBox(\n",
    "                [\n",
    "                    plot_ica_comp(spatial_comp, scale=scale),\n",
    "                    plot_erps_after_spatial_filter(\n",
    "                        spatial_comp,\n",
    "                        epochs,\n",
    "                        individual=individual,\n",
    "                        plot_limit=plot_limit,\n",
    "                        erp_type=erp_type,\n",
    "                        max_amp=max_amp,\n",
    "                        scale=scale,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"en\"].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first 10 participants, plot the difference of their\n",
    "# average of correct epochs minus average of error epochs\n",
    "# passed through the spatial filters\n",
    "\n",
    "visualize_spatial_components(\n",
    "    model,\n",
    "    epochs,\n",
    "    individual=True,\n",
    "    plot_limit=10,\n",
    "    erp_type=\"difference\",\n",
    "    max_amp=0.00014,\n",
    "    flip_mask=[-1, 1, 1, 1, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_shape(pca_comps, mwt, clf_coefs, xs, max_amp, scale=1, heatmap=False):\n",
    "    fig = go.FigureWidget()\n",
    "    fig.update_layout(**base_layout)\n",
    "    fig.update_layout(height=350 * scale, width=600 * scale)\n",
    "    if not heatmap:\n",
    "        fig.update_layout(yaxis_range=[-max_amp, max_amp])\n",
    "    accs = []\n",
    "    for i, comp in enumerate(pca_comps):\n",
    "        if mwt is not None:\n",
    "            # CWT+PCA in practice, multiplies by this shape\n",
    "            # TODO test this block, if this is really the shape\n",
    "            comp = comp.reshape(-1, timepoints_count)\n",
    "            acc = np.zeros_like(xs)\n",
    "            for amps_for_freq, freq in zip(comp, get_frequencies()):\n",
    "                for amp, latency in zip(amps_for_freq, xs):\n",
    "                    wv = get_wavelet(latency, freq, xs, mwt)\n",
    "                    acc += wv * amp\n",
    "        else:\n",
    "            acc = np.copy(comp)\n",
    "\n",
    "        # weight by the component importance from classifier\n",
    "        acc *= clf_coefs[i]\n",
    "        if not heatmap:\n",
    "            fig.add_scatter(x=xs, y=acc)\n",
    "        accs.append(acc)\n",
    "\n",
    "    # show also the sum of all pca comps weighted by importance\n",
    "    acc = np.zeros_like(accs[-1])\n",
    "    for comp, coef in zip(pca_comps, clf_coefs):\n",
    "        acc += comp * coef\n",
    "    accs.append(acc)\n",
    "\n",
    "    if not heatmap:\n",
    "        fig.add_scatter(x=xs, y=acc, line_width=5, line_color=\"yellow\")\n",
    "    else:\n",
    "        # reverse, so that later components are on the bottom\n",
    "        accs = np.array(accs[::-1])\n",
    "        fig.add_heatmap(\n",
    "            x=xs, z=accs, zmin=-max_amp, zmax=max_amp, colorscale=blue_black_red\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def visualize_pipeline(\n",
    "    pipeline,\n",
    "    clf_coefs_all=None,\n",
    "    max_amp=0.018,\n",
    "    scale=1,\n",
    "    heatmap=False,\n",
    "    one_pca=False,\n",
    "    flip_mask=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    pipeline\n",
    "        sklearn Pipeline to be visualized\n",
    "    clf_coefs_ll\n",
    "        optional classifier coefficients to be used to weigh component plots\n",
    "        if left as None, assume that lasso classifier was used, and use its coefs\n",
    "    max_amp\n",
    "        maximum amplitude for component plotting\n",
    "    scale\n",
    "        scale of the plots\n",
    "    heatmap\n",
    "        whether to use a heatmap instead of overlayed plots for each component\n",
    "    one_pca\n",
    "        whether only one PCA if fitted instead of a separate PCA for each spatial component\n",
    "    flip_mask\n",
    "        optional array of 1s and -1s\n",
    "        its length must be the same as the number of spatial components\n",
    "        setting -1 for a corresponding spatial component flips its sign for better readability\n",
    "    \"\"\"\n",
    "    if heatmap:\n",
    "        print(\"the component on the bottom is the sum of all the above components\")\n",
    "\n",
    "    fitted_steps = dict(pipeline.steps)\n",
    "    spatial = fitted_steps[\"spatial_filter\"]\n",
    "\n",
    "    if \"pca\" in fitted_steps:\n",
    "        dims_reduction = fitted_steps[\"pca\"]\n",
    "    elif \"feature_selection\" in fitted_steps:\n",
    "        dims_reduction = fitted_steps[\"feature_selection\"]\n",
    "\n",
    "    if clf_coefs_all is None:\n",
    "        if \"lasso\" in fitted_steps:\n",
    "            clf_coefs_all = fitted_steps[\"lasso\"].coef_\n",
    "        elif \"en\" in fitted_steps:\n",
    "            clf_coefs_all = fitted_steps[\"en\"].coef_\n",
    "        elif \"lda\" in fitted_steps:\n",
    "            clf_coefs_all = fitted_steps[\"lda\"].coef_[0]\n",
    "\n",
    "    if \"binning\" in fitted_steps:\n",
    "        bin_step = fitted_steps[\"binning\"].step\n",
    "        xs = times[bin_step // 2 :: bin_step]\n",
    "    else:\n",
    "        xs = times\n",
    "\n",
    "    if one_pca:\n",
    "        pca_comps_separated = dims_reduction.components_.reshape(\n",
    "            len(dims_reduction.components_), len(spatial.components_), -1\n",
    "        )\n",
    "    else:\n",
    "        pcas = dims_reduction.PCAs\n",
    "        clf_coefs_for_each_ica_comp = clf_coefs_all.reshape(\n",
    "            len(spatial.components_), -1\n",
    "        )\n",
    "        # the line below was tested visually to be the wrong unflattening\n",
    "        # clf_coefs_for_each_ica_comp = clf_coefs_all.reshape(-1, len(ica.components_)).T\n",
    "\n",
    "    for ica_comp_num, ica_comp in enumerate(spatial.components_):\n",
    "        if one_pca:\n",
    "            pca_comps = pca_comps_separated[:, ica_comp_num, :]\n",
    "            clf_coefs = clf_coefs_all\n",
    "        else:\n",
    "            pca_comps = pcas[ica_comp_num].components_\n",
    "            clf_coefs = clf_coefs_for_each_ica_comp[ica_comp_num]\n",
    "\n",
    "        if \"cwt\" in fitted_steps:\n",
    "            mwt = fitted_steps[\"cwt\"].mwt\n",
    "        else:\n",
    "            mwt = None\n",
    "\n",
    "        if flip_mask is not None:\n",
    "            ica_comp = ica_comp * flip_mask[ica_comp_num]\n",
    "            pca_comps = pca_comps * flip_mask[ica_comp_num]\n",
    "\n",
    "        display(\n",
    "            HBox(\n",
    "                [\n",
    "                    plot_ica_comp(ica_comp, scale=scale),\n",
    "                    plot_pca_shape(\n",
    "                        pca_comps,\n",
    "                        mwt,\n",
    "                        clf_coefs,\n",
    "                        xs=xs,\n",
    "                        max_amp=max_amp,\n",
    "                        scale=scale,\n",
    "                        heatmap=heatmap,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        # display(HBox([plot_ica_comp(ica_comp), plot_pca_comps_on_cwt(pca_comps)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of spatial filters, plot what shape the PCA components are trying to match\n",
    "# yellow line shows the sum of all the shapes - its the shape that the whole model\n",
    "# tries to match for that spatial filter\n",
    "\n",
    "visualize_pipeline(\n",
    "    model, heatmap=False, one_pca=True, flip_mask=[-1, 1, 1, 1, 1], max_amp=0.04\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pipeline_but_focus_on_pca_comps(\n",
    "    pipeline,\n",
    "    clf_coefs_all=None,\n",
    "    max_amp=0.015,\n",
    "    scale=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Note: works only with common PCA for all channels and no CWT\n",
    "\n",
    "    pipeline\n",
    "        sklearn Pipeline to be visualized\n",
    "    clf_coefs_ll\n",
    "        optional classifier coefficients to be used to weigh component plots\n",
    "        if left as None, assume that lasso classifier was used, and use its coefs\n",
    "    max_amp\n",
    "        maximum amplitude for component plotting\n",
    "    scale\n",
    "        scale of the plots\n",
    "    \"\"\"\n",
    "    fitted_steps = dict(pipeline.steps)\n",
    "    spatial = fitted_steps[\"spatial_filter\"]\n",
    "    pca = fitted_steps[\"pca\"]\n",
    "\n",
    "    if clf_coefs_all is None:\n",
    "        # clf_coefs_all = fitted_steps[\"lda\"].coef_[0]\n",
    "        clf_coefs_all = fitted_steps[\"lasso\"].coef_\n",
    "\n",
    "    for pca_comp, clf_coef in zip(pca.components_, clf_coefs_all):\n",
    "        pca_comp_2d = pca_comp.reshape(len(spatial.components_), -1)\n",
    "\n",
    "        fig = go.FigureWidget()\n",
    "        fig.update_layout(**base_layout)\n",
    "        fig.update_layout(height=350 * scale, width=600 * scale)\n",
    "        fig.add_heatmap(\n",
    "            x=times,\n",
    "            z=pca_comp_2d * clf_coef,\n",
    "            zmin=-max_amp,\n",
    "            zmax=max_amp,\n",
    "            colorscale=blue_black_red,\n",
    "        )\n",
    "        display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each PCA component, plot a heatmap with its components\n",
    "# with each row corresponding to one spatial filter\n",
    "\n",
    "split_index = 0\n",
    "visualize_pipeline_but_focus_on_pca_comps(pipelines[split_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check that PCA+LDA can be replaced by just a dot product with a shape computed from weighted PCA comps\n",
    "\n",
    "# split_index = 0\n",
    "# fitted_steps = dict(pipelines[split_index].steps)\n",
    "# pcas = fitted_steps[\"pca\"].PCAs\n",
    "# ica_comp_num = 0\n",
    "# pca_comps = pcas[ica_comp_num].components_\n",
    "# lda = fitted_steps[\"lda\"]\n",
    "\n",
    "# # do ICA steps\n",
    "# X_ = pipelines[0].steps[0][1].transform(X)\n",
    "# X_ = pipelines[0].steps[1][1].transform(X_)\n",
    "# X_ = pipelines[0].steps[2][1].transform(X_)\n",
    "\n",
    "# # compute a shape from the weighted PCA comps\n",
    "# acc = np.zeros_like(times)\n",
    "# for comp, coef in zip(pca_comps, lda.coef_[0]):\n",
    "#     acc += comp * coef\n",
    "\n",
    "# features = []\n",
    "# for epoch in X_[0]:\n",
    "#     feature = np.sum(acc * epoch)\n",
    "#     features.append(feature)\n",
    "# features = np.array(features).reshape(-1)\n",
    "\n",
    "# real_features = pipelines[split_index].decision_function(X)\n",
    "\n",
    "# corr = np.corrcoef(features, real_features)[0][1]\n",
    "# np.isclose(corr, 1)\n",
    "# # if True, the dot product gives the same as the normal pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ICA stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv should be 2 !\n",
    "# otherwise the components will be stable, but trivially\n",
    "#  - they are trained on overlapping data, so no wonder they are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def correlations(a0, a1):\n",
    "    \"\"\"Find correlation matrix between 2 matrices.\n",
    "    It's similar to np.corrcoef, but it doesn't subtract the mean,\n",
    "    when calculating the sum of squares.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a0, a1 : array_like\n",
    "        2-D arrays containing multiple variables and observations.\n",
    "        Each row represents a variable, and each column a single\n",
    "        observation of all those variables.\n",
    "        Their number of columns must be equal.\n",
    "    \"\"\"\n",
    "    cov = a0 @ a1.T\n",
    "    sum_of_squares0 = np.sum(a0 * a0, axis=1).reshape(-1, 1)\n",
    "    sum_of_squares1 = np.sum(a1 * a1, axis=1).reshape(1, -1)\n",
    "    return cov / (sum_of_squares0 @ sum_of_squares1) ** (1 / 2)\n",
    "\n",
    "\n",
    "def factor_similarity(a0, a1):\n",
    "    \"\"\"Measure how similar are the factors.\n",
    "    Reordering and rescaling them doesn't change the similarity.\n",
    "    \"\"\"\n",
    "    corr = correlations(a0, a1)\n",
    "    sim = abs(corr)  # don't care if factors' sign is flipped\n",
    "    sim_hor = sim.max(axis=0)  # don't care if factors are reordered\n",
    "    sim_ver = sim.max(axis=1)\n",
    "    # in case some row or comuln have two candidates, choose the more pessimistic axis\n",
    "    mean_sim = min(sim_hor.mean(), sim_ver.mean())\n",
    "    # TODO? a more robust way would be to generate permutations and chack them\n",
    "    return mean_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filters = [pipeline.steps[1][1].components_ for pipeline in pipelines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"correlations between factors found in the first, and the second split\")\n",
    "correlations(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_similarity(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"similarity measures between factors found in each pair of splits, for a single participant\"\n",
    "# )\n",
    "# similarities = np.array(\n",
    "#     [\n",
    "#         [factor_similarity(sf_i, sf_j) for sf_i in spatial_filters]\n",
    "#         for sf_j in spatial_filters\n",
    "#     ]\n",
    "# )\n",
    "# print(similarities)\n",
    "# print(\"mean\", similarities.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mne plotting for comparison\n",
    "# x, y, z = channel_locations.T\n",
    "# mne.viz.plot_topomap(\n",
    "#     spatial_filters[participant][2], np.stack((x, y), axis=-1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try to find corresponding components\n",
    "# best_similarity = 0\n",
    "# for perm in itertools.permutations(range(3)):\n",
    "#     perm = list(perm)\n",
    "#     diag = corr[perm].diagonal()\n",
    "#     similarity = abs(diag).mean()\n",
    "#     if similarity > best_similarity:\n",
    "#         best_similarity = similarity\n",
    "#         best_perm = perm\n",
    "\n",
    "# print(best_similarity)\n",
    "# print(best_perm)\n",
    "# corr[best_perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Visualize personal differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split participants in half - one for training of common model, one for validation\n",
    "\n",
    "grouped = epochs.groupby([\"id\"])\n",
    "ids = epochs[\"id\"].unique()\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "first_split = list(kf.split(ids))[0]\n",
    "train_index, test_index = first_split\n",
    "train_ids = ids[train_index]\n",
    "test_ids = ids[test_index]\n",
    "\n",
    "train_epochs = epochs[epochs[\"id\"].isin(train_ids)]\n",
    "test_epochs = epochs[epochs[\"id\"].isin(test_ids)]\n",
    "\n",
    "X_train = np.array(train_epochs[\"epoch\"].to_list())\n",
    "y_train = np.array(train_epochs[\"marker\"].to_list())\n",
    "X_test = np.array(test_epochs[\"epoch\"].to_list())\n",
    "y_test = np.array(test_epochs[\"marker\"].to_list())\n",
    "\n",
    "\n",
    "pipeline = Pipeline(deepcopy(steps), memory=cachedir)\n",
    "pipeline.set_params(**ParameterGrid(regressor_params)[0])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n",
    "if type(steps[-1][1]) == LinearDiscriminantAnalysis:\n",
    "    y_pred = pipeline.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores on train set\n",
    "if type(steps[-1][1]) == LinearDiscriminantAnalysis:\n",
    "    y_pred = pipeline.predict_proba(X_train)[:, 1]\n",
    "else:\n",
    "    y_pred = pipeline.predict(X_train)\n",
    "\n",
    "auroc = roc_auc_score(y_train, y_pred)\n",
    "corr = np.corrcoef(y_train, y_pred)[0][1]\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pipeline(pipeline, one_pca=True, flip_mask=[-1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_pipeline = Pipeline(pipeline.steps[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/regression.py\n",
    "class l_half_regularization:\n",
    "    \"\"\" Regularization for Ridge Regression \"\"\"\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.alpha * np.sum((np.abs(w) + 0.00001) ** (1 / 2))\n",
    "        # return 0  # to see olny fit error\n",
    "\n",
    "    def grad(self, w):\n",
    "        return self.alpha * 1 / 2 / ((np.abs(w) + 0.00001) ** (1 / 2)) * np.sign(w)\n",
    "\n",
    "\n",
    "class Regression(object):\n",
    "    \"\"\"Base regression model. Models the relationship between a scalar dependent variable y and the independent\n",
    "    variables X.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_iterations: float\n",
    "        The number of training iterations the algorithm will tune the weights for.\n",
    "    learning_rate: float\n",
    "        The step length that will be used when updating the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iterations, learning_rate):\n",
    "        self.n_iterations = n_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def initialize_weights(self, n_features):\n",
    "        \"\"\" Initialize weights randomly [-1/N, 1/N] \"\"\"\n",
    "        limit = 1 / math.sqrt(n_features)\n",
    "        self.w = np.random.uniform(-limit, limit, (n_features,))\n",
    "\n",
    "    def fit(self, X, y, reinit=True):\n",
    "        # Insert constant ones for bias weights\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        if reinit:\n",
    "            self.training_errors = []\n",
    "            self.initialize_weights(n_features=X.shape[1])\n",
    "\n",
    "        # Do gradient descent for n_iterations\n",
    "        for i in range(self.n_iterations):\n",
    "            y_pred = X.dot(self.w)\n",
    "            # Calculate l2 loss\n",
    "            mse = np.mean(0.5 * (y - y_pred) ** 2 + self.regularization(self.w))\n",
    "            self.training_errors.append(mse)\n",
    "            # Gradient of l2 loss w.r.t w\n",
    "            grad_w = -(y - y_pred).dot(X) + self.regularization.grad(self.w)\n",
    "            # Update the weights\n",
    "            self.w -= self.learning_rate * grad_w\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Insert constant ones for bias weights\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y_pred = X.dot(self.w)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class LHalfRegression(Regression):\n",
    "    def __init__(self, reg_factor, n_iterations=1000, learning_rate=0.001):\n",
    "        self.regularization = l_half_regularization(alpha=reg_factor)\n",
    "        super(LHalfRegression, self).__init__(n_iterations, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "features_train = truncated_pipeline.transform(X_train)\n",
    "features_test = truncated_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize regressor\n",
    "reg = LHalfRegression(0, n_iterations=6000, learning_rate=0.0000001)\n",
    "reg.fit(features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually retrain with different alphas\n",
    "# suppested consecutive alphas: 0, 3, 10, 30, 10\n",
    "reg.regularization.alpha = 10\n",
    "reg.fit(features_train, y_train, reinit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n",
    "y_pred = reg.predict(features_test)\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores on train set\n",
    "y_pred = reg.predict(features_train)\n",
    "\n",
    "auroc = roc_auc_score(y_train, y_pred)\n",
    "corr = np.corrcoef(y_train, y_pred)[0][1]\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(reg.training_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pipeline(\n",
    "    truncated_pipeline, one_pca=True, flip_mask=[-1, 1, 1, 1], clf_coefs_all=reg.w[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(np.abs(reg.w[1:]) > 0.01)[0]\n",
    "assert len(indices) <= 3\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = features_test[:, indices[0]]\n",
    "ys = features_test[:, indices[1]]\n",
    "if len(indices) >= 3:\n",
    "    zs = features_test[:, indices[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature points for all participants\n",
    "feature_plot_2d = go.FigureWidget()\n",
    "feature_plot_2d.update_layout(**base_layout)\n",
    "max_amp = 4\n",
    "feature_plot_2d.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    xaxis_range=[-max_amp, max_amp],\n",
    "    yaxis_range=[-max_amp, max_amp],\n",
    ")\n",
    "skip = 16\n",
    "feature_plot_2d.add_scatter(\n",
    "    x=xs[::skip],\n",
    "    y=ys[::skip],\n",
    "    marker_color=test_epochs[\"marker\"][::skip],\n",
    "    mode=\"markers\",\n",
    "    marker_size=4,\n",
    "    marker_colorscale=blue_black_red,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature points for one participant, greens are CORRECT, reds are ERROR\n",
    "participant_num = 1\n",
    "test_id = test_ids[participant_num]\n",
    "\n",
    "error_mask = test_epochs[\"marker\"] == ERROR\n",
    "correct_mask = test_epochs[\"marker\"] == CORRECT\n",
    "\n",
    "id_mask = test_epochs[\"id\"] == test_id\n",
    "x_cor = xs[id_mask & correct_mask]\n",
    "x_err = xs[id_mask & error_mask]\n",
    "y_cor = ys[id_mask & correct_mask]\n",
    "y_err = ys[id_mask & error_mask]\n",
    "\n",
    "grouped_plot_2d = go.FigureWidget()\n",
    "grouped_plot_2d.update_layout(**base_layout)\n",
    "max_amp = 4\n",
    "grouped_plot_2d.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    xaxis_range=[-max_amp, max_amp],\n",
    "    yaxis_range=[-max_amp, max_amp],\n",
    ")\n",
    "grouped_plot_2d.add_scatter(\n",
    "    x=x_cor,\n",
    "    y=y_cor,\n",
    "    marker_color=\"green\",\n",
    "    mode=\"markers\",\n",
    "    # marker_symbol=test_epochs[\"marker\"][:lim] * 4,\n",
    "    marker_size=4,\n",
    "    # marker_colorscale=blue_black_red,\n",
    ")\n",
    "grouped_plot_2d.add_scatter(\n",
    "    x=x_err,\n",
    "    y=y_err,\n",
    "    marker_color=\"red\",\n",
    "    mode=\"markers\",\n",
    "    # marker_symbol=test_epochs[\"marker\"][:lim] * 4,\n",
    "    marker_size=4,\n",
    "    # marker_colorscale=blue_black_red,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each participant, show arrow in feature space\n",
    "# from their error median to correct median\n",
    "# arrows are colored by the chosen scale - hotter color means higher on the scale\n",
    "\n",
    "\n",
    "@interact(column=Dropdown(value=\"Sex\", options=epochs.columns))\n",
    "def update_plots(column):\n",
    "    error_mask = test_epochs[\"marker\"] == ERROR\n",
    "    correct_mask = test_epochs[\"marker\"] == CORRECT\n",
    "\n",
    "    arrow_plot_2d = go.FigureWidget()\n",
    "    arrow_plot_2d.update_layout(**base_layout)\n",
    "    max_amp = 2.7\n",
    "    arrow_plot_2d.update_layout(\n",
    "        width=600,\n",
    "        height=600,\n",
    "        xaxis_range=[-max_amp, max_amp],\n",
    "        yaxis_range=[-max_amp, max_amp],\n",
    "    )\n",
    "\n",
    "    for test_id in test_ids:\n",
    "        id_mask = test_epochs[\"id\"] == test_id\n",
    "        color_val = test_epochs[test_epochs[\"id\"] == test_id][column].iloc[0] / 5\n",
    "        x_cor = xs[id_mask & correct_mask]\n",
    "        x_err = xs[id_mask & error_mask]\n",
    "        y_cor = ys[id_mask & correct_mask]\n",
    "        y_err = ys[id_mask & error_mask]\n",
    "        arrow_plot_2d.add_annotation(\n",
    "            x=np.median(x_cor),\n",
    "            y=np.median(y_cor),\n",
    "            ax=np.median(x_err),\n",
    "            ay=np.median(y_err),\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            axref=\"x\",\n",
    "            ayref=\"y\",\n",
    "            text=\"\",  # if you want only the arrow\n",
    "            showarrow=True,\n",
    "            arrowhead=3,\n",
    "            arrowsize=1,\n",
    "            arrowwidth=1,\n",
    "            arrowcolor=matplotlib.colors.rgb2hex(cm.hot(color_val)),\n",
    "        )\n",
    "\n",
    "    display(arrow_plot_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each participant, show arrow in feature space\n",
    "# from their error median to correct median\n",
    "# arrows are colored by rumination - hotter color means higher rumination\n",
    "\n",
    "error_mask = test_epochs[\"marker\"] == ERROR\n",
    "correct_mask = test_epochs[\"marker\"] == CORRECT\n",
    "\n",
    "arrow_plot_3d = go.FigureWidget()\n",
    "arrow_plot_3d.update_layout(**base_layout)\n",
    "max_amp = 2.7\n",
    "arrow_plot_3d.update_layout(\n",
    "    #     width=600,\n",
    "    #     height=600,\n",
    "    #     xaxis_range=[-max_amp, max_amp],\n",
    "    #     yaxis_range=[-max_amp, max_amp],\n",
    "    #     zaxis_range=[-max_amp, max_amp],\n",
    ")\n",
    "\n",
    "for test_id in test_ids:\n",
    "    id_mask = test_epochs[\"id\"] == test_id\n",
    "    rumination = test_epochs[test_epochs[\"id\"] == test_id][\n",
    "        \"Rumination Full Scale\"\n",
    "    ].iloc[0]\n",
    "    x_cor = xs[id_mask & correct_mask]\n",
    "    x_err = xs[id_mask & error_mask]\n",
    "    y_cor = ys[id_mask & correct_mask]\n",
    "    y_err = ys[id_mask & error_mask]\n",
    "    z_cor = zs[id_mask & correct_mask]\n",
    "    z_err = zs[id_mask & error_mask]\n",
    "    arrow_plot_3d.add_scatter3d(\n",
    "        x=[np.median(x_err), np.median(x_cor)],\n",
    "        y=[np.median(y_err), np.median(y_cor)],\n",
    "        z=[np.median(z_err), np.median(z_cor)],\n",
    "        line_color=matplotlib.colors.rgb2hex(cm.hot(rumination / 5)),\n",
    "        marker_size=[0, 3],\n",
    "    )\n",
    "\n",
    "arrow_plot_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
