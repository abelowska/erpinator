{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import inspect\n",
    "import itertools\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "from cachier import cachier\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import HBox, VBox\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import *\n",
    "from architecture import *\n",
    "from visualization_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# ignore FastICA did not converge warnings\n",
    "# TODO investigate why doesn't it converge\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs = pd.read_pickle(pickled_data_filename)\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs = create_df_data(info_filename=info_filename)\n",
    "    epochs.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs.to_pickle(pickled_data_filename)\n",
    "\n",
    "# # sorting to have the participants with the most error epochs first\n",
    "# # add new columns with info about error/correct responses amount\n",
    "# grouped = epochs.groupby(\"id\")\n",
    "# epochs[\"error_sum\"] = grouped[[\"marker\"]].transform(lambda x: (x.values == ERROR).sum())\n",
    "# epochs[\"correct_sum\"] = grouped[[\"marker\"]].transform(\n",
    "#     lambda x: (x.values == CORRECT).sum()\n",
    "# )\n",
    "# # mergesort for stable sorting\n",
    "# epochs = epochs.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")\n",
    "\n",
    "_mne_epochs = load_epochs_from_file(\"../../data/responses/GNG_AA0303-64 el.vhdr\")\n",
    "times = _mne_epochs.times\n",
    "\n",
    "_channel_info = _mne_epochs.info[\"chs\"]\n",
    "\n",
    "log_freq = np.log2(get_frequencies())  # for plotting CWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../public_data/regression_PCA_error.pkl\", \"rb\") as file:\n",
    "    models_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = models_data.loc[\n",
    "    (models_data[\"pipeline_name\"] == \"PCA_15_bins\") & (models_data[\"model\"] == \"en\")\n",
    "]\n",
    "\n",
    "model = model_data[\"best_estimator\"].iloc[0]\n",
    "model_params = model_data[\"parameters\"].iloc[0]\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_channels = model[\"channels_filtering\"].channel_list\n",
    "# persisted model numerates channels from 1, not 0\n",
    "channel_index_list = np.array(significant_channels) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load channel data\n",
    "channel_locations = np.array([ch[\"loc\"][:3] for ch in _channel_info])\n",
    "channel_names = np.array([ch[\"ch_name\"] for ch in _channel_info])\n",
    "\n",
    "channel_colors = channel_locations - channel_locations.min(axis=0)\n",
    "channel_colors /= channel_colors.max(axis=0)\n",
    "channel_colors = channel_colors * 255 // 1\n",
    "channel_colors = [f\"rgb({c[0]:.0f},{c[1]:.0f},{c[2]:.0f})\" for c in channel_colors]\n",
    "channel_colors = np.array(channel_colors)\n",
    "\n",
    "# trim it to only 15 top electrodes\n",
    "channel_names = channel_names[channel_index_list]\n",
    "channel_locations = channel_locations[channel_index_list]\n",
    "channel_colors = channel_colors[channel_index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize spatial components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first 10 participants, plot their error ERPs\n",
    "# passed through the spatial filters\n",
    "# the bold line is the average across all people\n",
    "\n",
    "visualize_spatial_components(\n",
    "    model,\n",
    "    epochs,\n",
    "    channel_locations,\n",
    "    channel_names,\n",
    "    times,\n",
    "    plot_limit=10,\n",
    "    erp_type=\"error\",\n",
    "    max_amp=0.00014,\n",
    "    flip_mask=[-1, 1, 1, 1, 1],\n",
    "    channel_index_list=channel_index_list,\n",
    "    scale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of spatial filters, plot what shape the PCA components are trying to match\n",
    "# thick black line shows the sum of all the shapes - its the shape that the whole model\n",
    "# tries to match for that spatial filter\n",
    "\n",
    "visualize_pipeline(\n",
    "    model,\n",
    "    channel_locations,\n",
    "    channel_names,\n",
    "    times,\n",
    "    heatmap=False,\n",
    "    one_pca=True,\n",
    "    flip_mask=[-1, 1, 1, 1, 1],\n",
    "    max_amp=0.04,\n",
    "    scale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_time_features_as_heatmap(\n",
    "    model,\n",
    "    times,\n",
    "    one_pca=True,\n",
    "    flip_mask=[-1, 1, 1, 1, 1],\n",
    "    max_amp=0.04,\n",
    "    scale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ICA stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv should be 2 !\n",
    "# otherwise the components will be stable, but trivially\n",
    "#  - they are trained on overlapping data, so no wonder they are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_epochs_df = (\n",
    "    epochs.groupby(\n",
    "        [\"id\", \"marker\"],\n",
    "        sort=False,\n",
    "    )\n",
    "    .apply(\n",
    "        lambda group_df: pd.Series(\n",
    "            {\n",
    "                \"epoch\": np.mean(group_df[\"epoch\"]),\n",
    "                \"Rumination Full Scale\": np.mean(group_df[\"Rumination Full Scale\"]),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "dataset = ERROR\n",
    "X = np.array(\n",
    "    averaged_epochs_df[averaged_epochs_df[\"marker\"] == dataset][\"epoch\"].to_list()\n",
    ")\n",
    "y = np.array(\n",
    "    averaged_epochs_df[averaged_epochs_df[\"marker\"] == dataset][\n",
    "        \"Rumination Full Scale\"\n",
    "    ].to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = deepcopy(ica_bins_steps)\n",
    "\n",
    "# # swap PCA to ICA as spatial_filter\n",
    "# _step_names = [step[0] for step in steps]\n",
    "# _sf_index = _step_names.index(\"spatial_filter\")\n",
    "# steps[_sf_index] = (\"spatial_filter\", FastICA(random_state=0))\n",
    "\n",
    "# del steps[-3]  # delete scaler step\n",
    "# del steps[0]  # delete channel selection step\n",
    "\n",
    "pipelines = custom_gridsearch(\n",
    "    X, y, steps, cv=2, regressor_params=model_params, memory=None\n",
    ")\n",
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(model.steps) == str(pipelines[0].steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first 10 participants, plot the difference of their\n",
    "# average of correct epochs minus average of error epochs\n",
    "# passed through the spatial filters\n",
    "\n",
    "visualize_spatial_components(\n",
    "    pipelines[1],\n",
    "    epochs,\n",
    "    channel_locations,\n",
    "    channel_names,\n",
    "    times,\n",
    "    plot_limit=10,\n",
    "    erp_type=\"error\",\n",
    "    max_amp=0.00014,\n",
    "    flip_mask=[-1, 1, 1, 1, 1],\n",
    "    channel_index_list=channel_index_list,\n",
    "    scale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def correlations(a0, a1):\n",
    "    \"\"\"Find correlation matrix between 2 matrices.\n",
    "    It's similar to np.corrcoef, but it doesn't subtract the mean,\n",
    "    when calculating the sum of squares.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a0, a1 : array_like\n",
    "        2-D arrays containing multiple variables and observations.\n",
    "        Each row represents a variable, and each column a single\n",
    "        observation of all those variables.\n",
    "        Their number of columns must be equal.\n",
    "    \"\"\"\n",
    "    cov = a0 @ a1.T\n",
    "    sum_of_squares0 = np.sum(a0 * a0, axis=1).reshape(-1, 1)\n",
    "    sum_of_squares1 = np.sum(a1 * a1, axis=1).reshape(1, -1)\n",
    "    return cov / (sum_of_squares0 @ sum_of_squares1) ** (1 / 2)\n",
    "\n",
    "\n",
    "def factor_similarity(a0, a1):\n",
    "    \"\"\"Measure how similar are the factors.\n",
    "    Reordering and rescaling them doesn't change the similarity.\n",
    "    \"\"\"\n",
    "    corr = correlations(a0, a1)\n",
    "    sim = abs(corr)  # don't care if factors' sign is flipped\n",
    "    sim_hor = sim.max(axis=0)  # don't care if factors are reordered\n",
    "    sim_ver = sim.max(axis=1)\n",
    "    # in case some row or comuln have two candidates, choose the more pessimistic axis\n",
    "    mean_sim = min(sim_hor.mean(), sim_ver.mean())\n",
    "    # TODO? a more robust way would be to generate permutations and chack them\n",
    "    return mean_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filters = [pipeline[\"spatial_filter\"].components_ for pipeline in pipelines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"correlations between factors found in the first, and the second split\")\n",
    "correlations(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_similarity(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for each of spatial filters, plot what shape the PCA components are trying to match\n",
    "# yellow line shows the sum of all the shapes - its the shape that the whole model\n",
    "# tries to match for that spatial filter\n",
    "\n",
    "visualize_pipeline(\n",
    "    pipelines[1],\n",
    "    channel_locations,\n",
    "    channel_names,\n",
    "    times,\n",
    "    heatmap=False,\n",
    "    one_pca=True,\n",
    "    flip_mask=[1, 1, 1, 1, 1],\n",
    "    max_amp=0.04,\n",
    "    scale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize personal differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split participants in half - one for training of common model, one for validation\n",
    "\n",
    "grouped = epochs.groupby([\"id\"])\n",
    "ids = epochs[\"id\"].unique()\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "first_split = list(kf.split(ids))[0]\n",
    "train_index, test_index = first_split\n",
    "train_ids = ids[train_index]\n",
    "test_ids = ids[test_index]\n",
    "\n",
    "train_epochs = epochs[epochs[\"id\"].isin(train_ids)]\n",
    "test_epochs = epochs[epochs[\"id\"].isin(test_ids)]\n",
    "\n",
    "X_train = np.array(train_epochs[\"epoch\"].to_list())\n",
    "y_train = np.array(train_epochs[\"marker\"].to_list())\n",
    "X_test = np.array(test_epochs[\"epoch\"].to_list())\n",
    "y_test = np.array(test_epochs[\"marker\"].to_list())\n",
    "\n",
    "\n",
    "pipeline = Pipeline(deepcopy(steps), memory=cachedir)\n",
    "pipeline.set_params(**ParameterGrid(regressor_params)[0])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n",
    "if type(steps[-1][1]) == LinearDiscriminantAnalysis:\n",
    "    y_pred = pipeline.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores on train set\n",
    "if type(steps[-1][1]) == LinearDiscriminantAnalysis:\n",
    "    y_pred = pipeline.predict_proba(X_train)[:, 1]\n",
    "else:\n",
    "    y_pred = pipeline.predict(X_train)\n",
    "\n",
    "auroc = roc_auc_score(y_train, y_pred)\n",
    "corr = np.corrcoef(y_train, y_pred)[0][1]\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pipeline(pipeline, one_pca=True, flip_mask=[-1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_pipeline = Pipeline(pipeline.steps[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/regression.py\n",
    "class l_half_regularization:\n",
    "    \"\"\" Regularization for Ridge Regression \"\"\"\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.alpha * np.sum((np.abs(w) + 0.00001) ** (1 / 2))\n",
    "        # return 0  # to see olny fit error\n",
    "\n",
    "    def grad(self, w):\n",
    "        return self.alpha * 1 / 2 / ((np.abs(w) + 0.00001) ** (1 / 2)) * np.sign(w)\n",
    "\n",
    "\n",
    "class Regression(object):\n",
    "    \"\"\"Base regression model. Models the relationship between a scalar dependent variable y and the independent\n",
    "    variables X.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_iterations: float\n",
    "        The number of training iterations the algorithm will tune the weights for.\n",
    "    learning_rate: float\n",
    "        The step length that will be used when updating the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iterations, learning_rate):\n",
    "        self.n_iterations = n_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def initialize_weights(self, n_features):\n",
    "        \"\"\" Initialize weights randomly [-1/N, 1/N] \"\"\"\n",
    "        limit = 1 / math.sqrt(n_features)\n",
    "        self.w = np.random.uniform(-limit, limit, (n_features,))\n",
    "\n",
    "    def fit(self, X, y, reinit=True):\n",
    "        # Insert constant ones for bias weights\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        if reinit:\n",
    "            self.training_errors = []\n",
    "            self.initialize_weights(n_features=X.shape[1])\n",
    "\n",
    "        # Do gradient descent for n_iterations\n",
    "        for i in range(self.n_iterations):\n",
    "            y_pred = X.dot(self.w)\n",
    "            # Calculate l2 loss\n",
    "            mse = np.mean(0.5 * (y - y_pred) ** 2 + self.regularization(self.w))\n",
    "            self.training_errors.append(mse)\n",
    "            # Gradient of l2 loss w.r.t w\n",
    "            grad_w = -(y - y_pred).dot(X) + self.regularization.grad(self.w)\n",
    "            # Update the weights\n",
    "            self.w -= self.learning_rate * grad_w\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Insert constant ones for bias weights\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y_pred = X.dot(self.w)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class LHalfRegression(Regression):\n",
    "    def __init__(self, reg_factor, n_iterations=1000, learning_rate=0.001):\n",
    "        self.regularization = l_half_regularization(alpha=reg_factor)\n",
    "        super(LHalfRegression, self).__init__(n_iterations, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "features_train = truncated_pipeline.transform(X_train)\n",
    "features_test = truncated_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize regressor\n",
    "reg = LHalfRegression(0, n_iterations=6000, learning_rate=0.0000001)\n",
    "reg.fit(features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually retrain with different alphas\n",
    "# suppested consecutive alphas: 0, 3, 10, 30, 10\n",
    "reg.regularization.alpha = 10\n",
    "reg.fit(features_train, y_train, reinit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n",
    "y_pred = reg.predict(features_test)\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores on train set\n",
    "y_pred = reg.predict(features_train)\n",
    "\n",
    "auroc = roc_auc_score(y_train, y_pred)\n",
    "corr = np.corrcoef(y_train, y_pred)[0][1]\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(reg.training_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pipeline(\n",
    "    truncated_pipeline, one_pca=True, flip_mask=[-1, 1, 1, 1], clf_coefs_all=reg.w[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(np.abs(reg.w[1:]) > 0.01)[0]\n",
    "assert len(indices) <= 3\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = features_test[:, indices[0]]\n",
    "ys = features_test[:, indices[1]]\n",
    "if len(indices) >= 3:\n",
    "    zs = features_test[:, indices[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature points for all participants\n",
    "feature_plot_2d = go.FigureWidget()\n",
    "feature_plot_2d.update_layout(**base_layout)\n",
    "max_amp = 4\n",
    "feature_plot_2d.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    xaxis_range=[-max_amp, max_amp],\n",
    "    yaxis_range=[-max_amp, max_amp],\n",
    ")\n",
    "skip = 16\n",
    "feature_plot_2d.add_scatter(\n",
    "    x=xs[::skip],\n",
    "    y=ys[::skip],\n",
    "    marker_color=test_epochs[\"marker\"][::skip],\n",
    "    mode=\"markers\",\n",
    "    marker_size=4,\n",
    "    marker_colorscale=blue_black_red,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature points for one participant, greens are CORRECT, reds are ERROR\n",
    "participant_num = 1\n",
    "test_id = test_ids[participant_num]\n",
    "\n",
    "error_mask = test_epochs[\"marker\"] == ERROR\n",
    "correct_mask = test_epochs[\"marker\"] == CORRECT\n",
    "\n",
    "id_mask = test_epochs[\"id\"] == test_id\n",
    "x_cor = xs[id_mask & correct_mask]\n",
    "x_err = xs[id_mask & error_mask]\n",
    "y_cor = ys[id_mask & correct_mask]\n",
    "y_err = ys[id_mask & error_mask]\n",
    "\n",
    "grouped_plot_2d = go.FigureWidget()\n",
    "grouped_plot_2d.update_layout(**base_layout)\n",
    "max_amp = 4\n",
    "grouped_plot_2d.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    xaxis_range=[-max_amp, max_amp],\n",
    "    yaxis_range=[-max_amp, max_amp],\n",
    ")\n",
    "grouped_plot_2d.add_scatter(\n",
    "    x=x_cor,\n",
    "    y=y_cor,\n",
    "    marker_color=\"green\",\n",
    "    mode=\"markers\",\n",
    "    # marker_symbol=test_epochs[\"marker\"][:lim] * 4,\n",
    "    marker_size=4,\n",
    "    # marker_colorscale=blue_black_red,\n",
    ")\n",
    "grouped_plot_2d.add_scatter(\n",
    "    x=x_err,\n",
    "    y=y_err,\n",
    "    marker_color=\"red\",\n",
    "    mode=\"markers\",\n",
    "    # marker_symbol=test_epochs[\"marker\"][:lim] * 4,\n",
    "    marker_size=4,\n",
    "    # marker_colorscale=blue_black_red,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each participant, show arrow in feature space\n",
    "# from their error median to correct median\n",
    "# arrows are colored by the chosen scale - hotter color means higher on the scale\n",
    "\n",
    "\n",
    "@interact(column=Dropdown(value=\"Sex\", options=epochs.columns))\n",
    "def update_plots(column):\n",
    "    error_mask = test_epochs[\"marker\"] == ERROR\n",
    "    correct_mask = test_epochs[\"marker\"] == CORRECT\n",
    "\n",
    "    arrow_plot_2d = go.FigureWidget()\n",
    "    arrow_plot_2d.update_layout(**base_layout)\n",
    "    max_amp = 2.7\n",
    "    arrow_plot_2d.update_layout(\n",
    "        width=600,\n",
    "        height=600,\n",
    "        xaxis_range=[-max_amp, max_amp],\n",
    "        yaxis_range=[-max_amp, max_amp],\n",
    "    )\n",
    "\n",
    "    for test_id in test_ids:\n",
    "        id_mask = test_epochs[\"id\"] == test_id\n",
    "        color_val = test_epochs[test_epochs[\"id\"] == test_id][column].iloc[0] / 5\n",
    "        x_cor = xs[id_mask & correct_mask]\n",
    "        x_err = xs[id_mask & error_mask]\n",
    "        y_cor = ys[id_mask & correct_mask]\n",
    "        y_err = ys[id_mask & error_mask]\n",
    "        arrow_plot_2d.add_annotation(\n",
    "            x=np.median(x_cor),\n",
    "            y=np.median(y_cor),\n",
    "            ax=np.median(x_err),\n",
    "            ay=np.median(y_err),\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            axref=\"x\",\n",
    "            ayref=\"y\",\n",
    "            text=\"\",  # if you want only the arrow\n",
    "            showarrow=True,\n",
    "            arrowhead=3,\n",
    "            arrowsize=1,\n",
    "            arrowwidth=1,\n",
    "            arrowcolor=matplotlib.colors.rgb2hex(cm.hot(color_val)),\n",
    "        )\n",
    "\n",
    "    display(arrow_plot_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each participant, show arrow in feature space\n",
    "# from their error median to correct median\n",
    "# arrows are colored by rumination - hotter color means higher rumination\n",
    "\n",
    "error_mask = test_epochs[\"marker\"] == ERROR\n",
    "correct_mask = test_epochs[\"marker\"] == CORRECT\n",
    "\n",
    "arrow_plot_3d = go.FigureWidget()\n",
    "arrow_plot_3d.update_layout(**base_layout)\n",
    "max_amp = 2.7\n",
    "arrow_plot_3d.update_layout(\n",
    "    #     width=600,\n",
    "    #     height=600,\n",
    "    #     xaxis_range=[-max_amp, max_amp],\n",
    "    #     yaxis_range=[-max_amp, max_amp],\n",
    "    #     zaxis_range=[-max_amp, max_amp],\n",
    ")\n",
    "\n",
    "for test_id in test_ids:\n",
    "    id_mask = test_epochs[\"id\"] == test_id\n",
    "    rumination = test_epochs[test_epochs[\"id\"] == test_id][\n",
    "        \"Rumination Full Scale\"\n",
    "    ].iloc[0]\n",
    "    x_cor = xs[id_mask & correct_mask]\n",
    "    x_err = xs[id_mask & error_mask]\n",
    "    y_cor = ys[id_mask & correct_mask]\n",
    "    y_err = ys[id_mask & error_mask]\n",
    "    z_cor = zs[id_mask & correct_mask]\n",
    "    z_err = zs[id_mask & error_mask]\n",
    "    arrow_plot_3d.add_scatter3d(\n",
    "        x=[np.median(x_err), np.median(x_cor)],\n",
    "        y=[np.median(y_err), np.median(y_cor)],\n",
    "        z=[np.median(z_err), np.median(z_cor)],\n",
    "        line_color=matplotlib.colors.rgb2hex(cm.hot(rumination / 5)),\n",
    "        marker_size=[0, 3],\n",
    "    )\n",
    "\n",
    "arrow_plot_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
