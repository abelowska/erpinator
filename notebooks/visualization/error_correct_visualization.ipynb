{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_loc = channel_locations.T[0]\n",
    "# y_loc = channel_locations.T[1]\n",
    "# mne.viz.plot_topomap(ica.components_[0], np.stack((x_loc, y_loc), axis=-1))\n",
    "\n",
    "# def plot_pca_comps_on_cwt(pca_comps):\n",
    "#     amplitude = 0.1\n",
    "#     fig = go.FigureWidget(make_subplots(rows=len(pca_comps)))\n",
    "#     fig.update_layout(**base_layout)\n",
    "#     fig.update_layout(height=350, width=600)\n",
    "#     fig.update_xaxes(visible=False)\n",
    "#     for i, comp in enumerate(pca_comps):\n",
    "#         comp = comp.reshape(-1, timepoints_count)\n",
    "#         fig.add_heatmap(\n",
    "#             z=comp,\n",
    "#             x=times,\n",
    "#             row=i + 1,\n",
    "#             col=1,\n",
    "#             zmin=-amplitude,\n",
    "#             zmax=amplitude,\n",
    "#             y=log_freq,\n",
    "#             colorscale=blue_black_red,\n",
    "#         )\n",
    "#     return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import inspect\n",
    "import itertools\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "from cachier import cachier\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ipywidgets import HBox, VBox\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import *\n",
    "from architecture import *\n",
    "from visualization_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# ignore FastICA did not converge warnings\n",
    "# TODO investigate why doesn't it converge\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs = pd.read_pickle(pickled_data_filename)\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs = create_df_data(info_filename=info_filename)\n",
    "    epochs.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs.to_pickle(pickled_data_filename)\n",
    "\n",
    "# epochs\n",
    "\n",
    "# add new columns with info about error/correct responses amount\n",
    "grouped = epochs.groupby(\"id\")\n",
    "epochs[\"error_sum\"] = grouped[[\"marker\"]].transform(lambda x: (x.values == ERROR).sum())\n",
    "epochs[\"correct_sum\"] = grouped[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs = epochs.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")\n",
    "# epochs\n",
    "\n",
    "_mne_epochs = load_epochs_from_file(\"../../data/responses/GNG_AA0303-64 el.vhdr\")\n",
    "times = _mne_epochs.times\n",
    "\n",
    "_channel_info = _mne_epochs.info[\"chs\"]\n",
    "channel_locations = np.array([ch[\"loc\"][:3] for ch in _channel_info])\n",
    "channel_names = [ch[\"ch_name\"] for ch in _channel_info]\n",
    "\n",
    "channel_colors = channel_locations - channel_locations.min(axis=0)\n",
    "channel_colors /= channel_colors.max(axis=0)\n",
    "channel_colors = channel_colors * 255 // 1\n",
    "channel_colors = [f\"rgb({c[0]:.0f},{c[1]:.0f},{c[2]:.0f})\" for c in channel_colors]\n",
    "\n",
    "log_freq = np.log2(get_frequencies())  # for plotting CWT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = \"/home/filip/.erpinator_cache\"\n",
    "\n",
    "# steps = steps_simple  # one PCA for all\n",
    "\n",
    "steps = deepcopy(steps_parallel_pca)\n",
    "steps.pop(3)  # remove CWT\n",
    "steps.pop(-1)  # remove scaler\n",
    "\n",
    "# StandardScaler doesn't seem to change anything for LDA\n",
    "# steps = steps[:-2] + [(\"lasso\", Lasso())]\n",
    "# steps = steps[:-2] + [(\"lda\", LinearDiscriminantAnalysis())]\n",
    "# steps = steps[:-1] + [(\"knr\", KNeighborsRegressor())]\n",
    "steps = steps[:-1] + [(\"lasso\", Lasso())]\n",
    "\n",
    "steps[1] = (\"spatial_filter\", PCA(random_state=0))  # replace ICA with PCA\n",
    "\n",
    "regressor_params = dict(\n",
    "    spatial_filter__n_components=[4],\n",
    "    #     cwt__mwt=[\"morl\"],\n",
    "    #     cwt__octaves=[4],\n",
    "    pca__n_components=[8],\n",
    "    # featurize__power__cwt__mwt=[\"cmor0.5-1\"],\n",
    "    # featurize__power__pca__n_components=[3],\n",
    "    # featurize__shape__cwt__mwt=[\"mexh\"],\n",
    "    # featurize__shape__pca__n_components=[3],\n",
    "    #     svr__C=[0.1],\n",
    "    #     knr__n_neighbors=[11],\n",
    "    lasso__alpha=[0.0000003],\n",
    "    # lda__solver=[\"lsqr\"],  # to turn off scaling, to simplify visualizing\n",
    ")\n",
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One model for all people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(epochs[\"epoch\"].to_list())  # [::20]\n",
    "y = np.array(epochs[\"marker\"].to_list())  # [::20]\n",
    "# y = np.array(epochs[\"Rumination Full Scale\"].to_list())  # [::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pipelines = custom_gridsearch(X, y, steps, cv=2, regressor_params=regressor_params, memory=cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize spatial components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first 10 participants, plot the difference of their\n",
    "# average of correct epochs minus average of error epochs\n",
    "# passed through the spatial filters\n",
    "\n",
    "\n",
    "split_index = 0\n",
    "visualize_spatial_components(\n",
    "    pipelines[split_index],\n",
    "    epochs,\n",
    "    channel_locations,\n",
    "    channel_names,\n",
    "    times,\n",
    "    plot_limit=10,\n",
    "    erp_type=\"error\",\n",
    "    max_amp=0.00014,\n",
    "    flip_mask=[-1, 1, 1, 1],\n",
    "    scale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize temporal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of spatial filters, plot what shape the PCA components are trying to match\n",
    "# yellow line shows the sum of all the shapes - its the shape that the whole model\n",
    "# tries to match for that spatial filter\n",
    "\n",
    "split_index = 1\n",
    "visualize_pipeline(\n",
    "    pipelines[split_index],\n",
    "    channel_locations,\n",
    "    channel_names,\n",
    "    times,\n",
    "    heatmap=False,\n",
    "    one_pca=False,\n",
    "    flip_mask=[-1, 1, -1, 1],\n",
    "    max_amp=230,\n",
    "    scale=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check that PCA+LDA can be replaced by just a dot product with a shape computed from weighted PCA comps\n",
    "\n",
    "# split_index = 0\n",
    "# fitted_steps = dict(pipelines[split_index].steps)\n",
    "# pcas = fitted_steps[\"pca\"].PCAs\n",
    "# ica_comp_num = 0\n",
    "# pca_comps = pcas[ica_comp_num].components_\n",
    "# lda = fitted_steps[\"lda\"]\n",
    "\n",
    "# # do ICA steps\n",
    "# X_ = pipelines[0].steps[0][1].transform(X)\n",
    "# X_ = pipelines[0].steps[1][1].transform(X_)\n",
    "# X_ = pipelines[0].steps[2][1].transform(X_)\n",
    "\n",
    "# # compute a shape from the weighted PCA comps\n",
    "# acc = np.zeros_like(times)\n",
    "# for comp, coef in zip(pca_comps, lda.coef_[0]):\n",
    "#     acc += comp * coef\n",
    "\n",
    "# features = []\n",
    "# for epoch in X_[0]:\n",
    "#     feature = np.sum(acc * epoch)\n",
    "#     features.append(feature)\n",
    "# features = np.array(features).reshape(-1)\n",
    "\n",
    "# real_features = pipelines[split_index].decision_function(X)\n",
    "\n",
    "# corr = np.corrcoef(features, real_features)[0][1]\n",
    "# np.isclose(corr, 1)\n",
    "# # if True, the dot product gives the same as the normal pipeline execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ICA stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv should be 2 !\n",
    "# otherwise the components will be stable, but trivially\n",
    "#  - they are trained on overlapping data, so no wonder they are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filters = [pipeline.steps[1][1].components_ for pipeline in pipelines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"correlations between factors found in the first, and the second split\")\n",
    "correlations(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_similarity(spatial_filters[0], spatial_filters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "heading_collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"similarity measures between factors found in each pair of splits, for a single participant\"\n",
    "# )\n",
    "# similarities = np.array(\n",
    "#     [\n",
    "#         [factor_similarity(sf_i, sf_j) for sf_i in spatial_filters]\n",
    "#         for sf_j in spatial_filters\n",
    "#     ]\n",
    "# )\n",
    "# print(similarities)\n",
    "# print(\"mean\", similarities.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mne plotting for comparison\n",
    "# x, y, z = channel_locations.T\n",
    "# mne.viz.plot_topomap(\n",
    "#     spatial_filters[participant][2], np.stack((x, y), axis=-1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try to find corresponding components\n",
    "# best_similarity = 0\n",
    "# for perm in itertools.permutations(range(3)):\n",
    "#     perm = list(perm)\n",
    "#     diag = corr[perm].diagonal()\n",
    "#     similarity = abs(diag).mean()\n",
    "#     if similarity > best_similarity:\n",
    "#         best_similarity = similarity\n",
    "#         best_perm = perm\n",
    "\n",
    "# print(best_similarity)\n",
    "# print(best_perm)\n",
    "# corr[best_perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize personal differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split participants in half - one for training of common model, one for validation\n",
    "\n",
    "ids = epochs[\"id\"].unique()\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True)\n",
    "first_split = list(kf.split(ids))[0]\n",
    "train_index, test_index = first_split\n",
    "train_ids = ids[train_index]\n",
    "test_ids = ids[test_index]\n",
    "\n",
    "train_epochs = epochs[epochs[\"id\"].isin(train_ids)]\n",
    "test_epochs = epochs[epochs[\"id\"].isin(test_ids)]\n",
    "\n",
    "X_train = np.array(train_epochs[\"epoch\"].to_list())\n",
    "y_train = np.array(train_epochs[\"marker\"].to_list())\n",
    "X_test = np.array(test_epochs[\"epoch\"].to_list())\n",
    "y_test = np.array(test_epochs[\"marker\"].to_list())\n",
    "\n",
    "\n",
    "pipeline = Pipeline(deepcopy(steps), memory=cachedir)\n",
    "pipeline.set_params(**ParameterGrid(regressor_params)[0])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n",
    "if type(steps[-1][1]) == LinearDiscriminantAnalysis:\n",
    "    y_pred = pipeline.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pipeline[\"scaler\"]\n",
    "sc.scale_ * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores on train set\n",
    "if type(steps[-1][1]) == LinearDiscriminantAnalysis:\n",
    "    y_pred = pipeline.predict_proba(X_train)[:, 1]\n",
    "else:\n",
    "    y_pred = pipeline.predict(X_train)\n",
    "\n",
    "auroc = roc_auc_score(y_train, y_pred)\n",
    "corr = np.corrcoef(y_train, y_pred)[0][1]\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pipeline(pipeline, one_pca=True, flip_mask=[-1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_pipeline = Pipeline(pipeline.steps[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/regression.py\n",
    "class l_half_regularization:\n",
    "    \"\"\" Regularization for Ridge Regression \"\"\"\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.alpha * np.sum((np.abs(w) + 0.00001) ** (1 / 2))\n",
    "        # return 0  # to see olny fit error\n",
    "\n",
    "    def grad(self, w):\n",
    "        return self.alpha * 1 / 2 / ((np.abs(w) + 0.00001) ** (1 / 2)) * np.sign(w)\n",
    "\n",
    "\n",
    "class Regression(object):\n",
    "    \"\"\"Base regression model. Models the relationship between a scalar dependent variable y and the independent\n",
    "    variables X.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_iterations: float\n",
    "        The number of training iterations the algorithm will tune the weights for.\n",
    "    learning_rate: float\n",
    "        The step length that will be used when updating the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iterations, learning_rate):\n",
    "        self.n_iterations = n_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def initialize_weights(self, n_features):\n",
    "        \"\"\" Initialize weights randomly [-1/N, 1/N] \"\"\"\n",
    "        limit = 1 / math.sqrt(n_features)\n",
    "        self.w = np.random.uniform(-limit, limit, (n_features,))\n",
    "\n",
    "    def fit(self, X, y, reinit=True):\n",
    "        # Insert constant ones for bias weights\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        if reinit:\n",
    "            self.training_errors = []\n",
    "            self.initialize_weights(n_features=X.shape[1])\n",
    "\n",
    "        # Do gradient descent for n_iterations\n",
    "        for i in range(self.n_iterations):\n",
    "            y_pred = X.dot(self.w)\n",
    "            # Calculate l2 loss\n",
    "            mse = np.mean(0.5 * (y - y_pred) ** 2 + self.regularization(self.w))\n",
    "            self.training_errors.append(mse)\n",
    "            # Gradient of l2 loss w.r.t w\n",
    "            grad_w = -(y - y_pred).dot(X) + self.regularization.grad(self.w)\n",
    "            # Update the weights\n",
    "            self.w -= self.learning_rate * grad_w\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Insert constant ones for bias weights\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        y_pred = X.dot(self.w)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class LHalfRegression(Regression):\n",
    "    def __init__(self, reg_factor, n_iterations=1000, learning_rate=0.001):\n",
    "        self.regularization = l_half_regularization(alpha=reg_factor)\n",
    "        super(LHalfRegression, self).__init__(n_iterations, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "features_train = truncated_pipeline.transform(X_train)\n",
    "features_test = truncated_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize regressor\n",
    "reg = LHalfRegression(0, n_iterations=6000, learning_rate=0.0000001)\n",
    "reg.fit(features_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually retrain with different alphas\n",
    "# suppested consecutive alphas: 0, 3, 10, 30, 10\n",
    "reg.regularization.alpha = 10\n",
    "reg.fit(features_train, y_train, reinit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores\n",
    "y_pred = reg.predict(features_test)\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "corr = np.corrcoef(y_test, y_pred)[0][1]\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores on train set\n",
    "y_pred = reg.predict(features_train)\n",
    "\n",
    "auroc = roc_auc_score(y_train, y_pred)\n",
    "corr = np.corrcoef(y_train, y_pred)[0][1]\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "auroc, corr, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(reg.training_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pipeline(\n",
    "    truncated_pipeline, one_pca=True, flip_mask=[-1, 1, 1, 1], clf_coefs_all=reg.w[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(np.abs(reg.w[1:]) > 0.01)[0]\n",
    "assert len(indices) <= 3\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = features_test[:, indices[0]]\n",
    "ys = features_test[:, indices[1]]\n",
    "if len(indices) >= 3:\n",
    "    zs = features_test[:, indices[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature points for all participants\n",
    "feature_plot_2d = go.FigureWidget()\n",
    "feature_plot_2d.update_layout(**base_layout)\n",
    "max_amp = 4\n",
    "feature_plot_2d.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    xaxis_range=[-max_amp, max_amp],\n",
    "    yaxis_range=[-max_amp, max_amp],\n",
    ")\n",
    "skip = 16\n",
    "feature_plot_2d.add_scatter(\n",
    "    x=xs[::skip],\n",
    "    y=ys[::skip],\n",
    "    marker_color=test_epochs[\"marker\"][::skip],\n",
    "    mode=\"markers\",\n",
    "    marker_size=4,\n",
    "    marker_colorscale=blue_black_red,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature points for one participant, greens are CORRECT, reds are ERROR\n",
    "participant_num = 1\n",
    "test_id = test_ids[participant_num]\n",
    "\n",
    "error_mask = test_epochs[\"marker\"] == ERROR\n",
    "correct_mask = test_epochs[\"marker\"] == CORRECT\n",
    "\n",
    "id_mask = test_epochs[\"id\"] == test_id\n",
    "x_cor = xs[id_mask & correct_mask]\n",
    "x_err = xs[id_mask & error_mask]\n",
    "y_cor = ys[id_mask & correct_mask]\n",
    "y_err = ys[id_mask & error_mask]\n",
    "\n",
    "grouped_plot_2d = go.FigureWidget()\n",
    "grouped_plot_2d.update_layout(**base_layout)\n",
    "max_amp = 4\n",
    "grouped_plot_2d.update_layout(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    xaxis_range=[-max_amp, max_amp],\n",
    "    yaxis_range=[-max_amp, max_amp],\n",
    ")\n",
    "grouped_plot_2d.add_scatter(\n",
    "    x=x_cor,\n",
    "    y=y_cor,\n",
    "    marker_color=\"green\",\n",
    "    mode=\"markers\",\n",
    "    # marker_symbol=test_epochs[\"marker\"][:lim] * 4,\n",
    "    marker_size=4,\n",
    "    # marker_colorscale=blue_black_red,\n",
    ")\n",
    "grouped_plot_2d.add_scatter(\n",
    "    x=x_err,\n",
    "    y=y_err,\n",
    "    marker_color=\"red\",\n",
    "    mode=\"markers\",\n",
    "    # marker_symbol=test_epochs[\"marker\"][:lim] * 4,\n",
    "    marker_size=4,\n",
    "    # marker_colorscale=blue_black_red,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each participant, show arrow in feature space\n",
    "# from their error median to correct median\n",
    "# arrows are colored by the chosen scale - hotter color means higher on the scale\n",
    "\n",
    "\n",
    "@interact(column=Dropdown(value=\"Sex\", options=epochs.columns))\n",
    "def update_plots(column):\n",
    "    error_mask = test_epochs[\"marker\"] == ERROR\n",
    "    correct_mask = test_epochs[\"marker\"] == CORRECT\n",
    "\n",
    "    arrow_plot_2d = go.FigureWidget()\n",
    "    arrow_plot_2d.update_layout(**base_layout)\n",
    "    max_amp = 2.7\n",
    "    arrow_plot_2d.update_layout(\n",
    "        width=600,\n",
    "        height=600,\n",
    "        xaxis_range=[-max_amp, max_amp],\n",
    "        yaxis_range=[-max_amp, max_amp],\n",
    "    )\n",
    "\n",
    "    for test_id in test_ids:\n",
    "        id_mask = test_epochs[\"id\"] == test_id\n",
    "        color_val = test_epochs[test_epochs[\"id\"] == test_id][column].iloc[0] / 5\n",
    "        x_cor = xs[id_mask & correct_mask]\n",
    "        x_err = xs[id_mask & error_mask]\n",
    "        y_cor = ys[id_mask & correct_mask]\n",
    "        y_err = ys[id_mask & error_mask]\n",
    "        arrow_plot_2d.add_annotation(\n",
    "            x=np.median(x_cor),\n",
    "            y=np.median(y_cor),\n",
    "            ax=np.median(x_err),\n",
    "            ay=np.median(y_err),\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            axref=\"x\",\n",
    "            ayref=\"y\",\n",
    "            text=\"\",  # if you want only the arrow\n",
    "            showarrow=True,\n",
    "            arrowhead=3,\n",
    "            arrowsize=1,\n",
    "            arrowwidth=1,\n",
    "            arrowcolor=matplotlib.colors.rgb2hex(cm.hot(color_val)),\n",
    "        )\n",
    "\n",
    "    display(arrow_plot_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each participant, show arrow in feature space\n",
    "# from their error median to correct median\n",
    "# arrows are colored by rumination - hotter color means higher rumination\n",
    "\n",
    "error_mask = test_epochs[\"marker\"] == ERROR\n",
    "correct_mask = test_epochs[\"marker\"] == CORRECT\n",
    "\n",
    "arrow_plot_3d = go.FigureWidget()\n",
    "arrow_plot_3d.update_layout(**base_layout)\n",
    "max_amp = 2.7\n",
    "arrow_plot_3d.update_layout(\n",
    "    #     width=600,\n",
    "    #     height=600,\n",
    "    #     xaxis_range=[-max_amp, max_amp],\n",
    "    #     yaxis_range=[-max_amp, max_amp],\n",
    "    #     zaxis_range=[-max_amp, max_amp],\n",
    ")\n",
    "\n",
    "for test_id in test_ids:\n",
    "    id_mask = test_epochs[\"id\"] == test_id\n",
    "    rumination = test_epochs[test_epochs[\"id\"] == test_id][\n",
    "        \"Rumination Full Scale\"\n",
    "    ].iloc[0]\n",
    "    x_cor = xs[id_mask & correct_mask]\n",
    "    x_err = xs[id_mask & error_mask]\n",
    "    y_cor = ys[id_mask & correct_mask]\n",
    "    y_err = ys[id_mask & error_mask]\n",
    "    z_cor = zs[id_mask & correct_mask]\n",
    "    z_err = zs[id_mask & error_mask]\n",
    "    arrow_plot_3d.add_scatter3d(\n",
    "        x=[np.median(x_err), np.median(x_cor)],\n",
    "        y=[np.median(y_err), np.median(y_cor)],\n",
    "        z=[np.median(z_err), np.median(z_cor)],\n",
    "        line_color=matplotlib.colors.rgb2hex(cm.hot(rumination / 5)),\n",
    "        marker_size=[0, 3],\n",
    "    )\n",
    "\n",
    "arrow_plot_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
