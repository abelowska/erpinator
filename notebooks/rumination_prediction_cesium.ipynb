{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Rumination prediction with cesium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are calculated for each band from cwt separately.\n",
    "Different strategies of ICA and PCA application:\n",
    "\n",
    "- ICA + PCA on all kind of features (mean, std itd) -> do not increase effectivness of the model\n",
    "\n",
    "- ICA + PCA on two the best kind of features (std + abs_diff) -> increase effectivness of the model a lot. Second the best method:  \n",
    "\n",
    "    - 9 * (3 from 2*14) = 27 components\n",
    "    \n",
    "            MAPE: 17.9079258661464\n",
    "            MAE: 0.519917817723914\n",
    "            MSE: 0.478030015754372\n",
    "            R^2: 0.376125673763408\n",
    "\n",
    "    - 9 * (2 from 2*14) = 18 components\n",
    "\n",
    "            MAPE: 16.8443619132606\n",
    "            MAE: 0.502078488977514\n",
    "            MSE: 0.454436875307531\n",
    "            R^2: 0.406916950702055\n",
    "\n",
    "\n",
    "\n",
    "- ICA + PCA separately on std features and abs_diffs features -> ok, but not the best way\n",
    "    \n",
    "    - 6*(5+5) = 60 components\n",
    "\n",
    "            MAPE: 24.018846090150365\n",
    "            MAE: 0.6884866303220416\n",
    "            MSE: 0.7185622829832321\n",
    "            R^2: 0.062208343867806604\n",
    "\n",
    "    - 6*(3+3) = 36 components\n",
    "\n",
    "            MAPE: 21.50178345656211\n",
    "            MAE: 0.6201825754057195\n",
    "            MSE: 0.6069731329548679\n",
    "            R^2: 0.2078427255904629\n",
    "\n",
    "    - 6*(2+2) = 24 components\n",
    "\n",
    "            19.438263116024583\n",
    "            0.5637319078555214\n",
    "            0.5362710537170331\n",
    "            0.3001156176565023\n",
    "\n",
    "    - 6*(1+1) = 12 components\n",
    "\n",
    "            19.85447763226303\n",
    "            0.5760456075230935\n",
    "            0.585240921194229\n",
    "            0.23620531480654705\n",
    "\n",
    "    - 5*(3+3) = 30 components\n",
    "\n",
    "            MAPE: 20.536346943303492\n",
    "            MAE: 0.5910318538690427\n",
    "            MSE: 0.5747574611935253\n",
    "            R^2: 0.24988722039619093\n",
    "\n",
    "    - 5*(2+2) = 20 components\n",
    "\n",
    "            19.35858512090129\n",
    "            0.5635286480139711\n",
    "            0.5433040170537524\n",
    "            0.2909369361542158\n",
    "            \n",
    "    - 5*(1+1) = 10 components\n",
    "    \n",
    "            20.250558073766026\n",
    "            0.5886032629783092\n",
    "            0.5852655385369406\n",
    "            0.23617318684890465\n",
    "\n",
    "    - 4*(4+4) = 32 components\n",
    "\n",
    "            MAPE: 22.031474920258777\n",
    "            MAE: 0.6380830691061724\n",
    "            MSE: 0.6277453555688914\n",
    "            R^2: 0.1807330128932182\n",
    "            \n",
    "   \n",
    "- PCA on flattened ICA channels and PCA separately on std features and abs_diffs features -> more research needed\n",
    "    \n",
    "    - 30 from 6*(5+5) = 30 components\n",
    "    \n",
    "            MAPE: 21.44571192549426\n",
    "            MAE: 0.6261229886064391\n",
    "            MSE: 0.6193306483997083\n",
    "            R^2: 0.19171500061917268\n",
    "       \n",
    "    - 30 from 6*(4+4) = 30 components\n",
    "    \n",
    "            MAPE: 21.24921683786726\n",
    "            MAE: 0.615903392719023\n",
    "            MSE: 0.608353978391557\n",
    "            R^2: 0.20604059185814527\n",
    "    \n",
    "    - 30 from 6*(3+3) = 30 components\n",
    "    \n",
    "            MAPE: 21.11155762577906\n",
    "            MAE: 0.6154703465611149\n",
    "            MSE: 0.604724357788718\n",
    "            R^2: 0.21077758960611548\n",
    "    \n",
    "    - 30 from 5*(5+5) = 30 components\n",
    "    \n",
    "            MAPE: 21.67828659174979\n",
    "            MAE: 0.6323799032109819\n",
    "            MSE: 0.6303037217274262\n",
    "            R^2: 0.17739410338791517\n",
    "    \n",
    "    - 30 from 5*(4+4) = 30 components\n",
    "    \n",
    "            MAPE: 21.317971705648553\n",
    "            MAE: 0.615749388609156\n",
    "            MSE: 0.6219822296997861\n",
    "            R^2: 0.1882544365488662\n",
    "    \n",
    "    - 30 from 5*(3+3) = 30 components\n",
    "    \n",
    "            MAPE: 21.220223056606272\n",
    "            MAE: 0.6138419950553302\n",
    "            MSE: 0.6099786827888294\n",
    "            R^2: 0.20392019914685844\n",
    "    \n",
    "    - 30 from 4*(4+4) = 30 components\n",
    "    \n",
    "            MAPE; 22.469724109237735\n",
    "            MAE: 0.6509080551097475\n",
    "            MSE: 0.6534929953956176\n",
    "            R^2: 0.14712991074547543\n",
    "   \n",
    "   \n",
    "- PCA on flattened ICA channels and (std + abs_diff) feature sets -> **the best results:**\n",
    "    \n",
    "        Example:\n",
    "        \n",
    "        \n",
    "     -  PCA: 18; ICA: 9:\n",
    "     \n",
    "             MAPE: 16.1051630051677\n",
    "             MAE: 0.481721094094576\n",
    "             MSE: 0.412577754295216\n",
    "             R^2: 0.461547057719921\n",
    "             \n",
    "             \n",
    "     - PCA: 18; ICA: 18:\n",
    "     \n",
    "             MAPE: 11.6348912814115\n",
    "             MAE: 0.35025761618236\n",
    "             MSE: 0.237247454583717\n",
    "             R^2: 0.690369660896321\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data load all info from given file but ones can specify it passing list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' | 'annot' | 'peak-to-peak'\n",
    "\n",
    "        Whether the epochs with overlapping bad segments are rejected by default.\n",
    "\n",
    "        'auto' means that bad segments are rejected automatically.\n",
    "        'annot' rejection based on annotations and reject only channels annotated in .vmrk file as\n",
    "        'bad'.\n",
    "        'peak-to-peak' rejection based on peak-to-peak amplitude of channels.\n",
    "\n",
    "        Rejected with 'annot' and 'amplitude' channels are zeroed.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(\"../data/\" + file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(\"../data/\" + annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = True\n",
    "\n",
    "    if reject_bad_segments != \"auto\":\n",
    "        this_reject_by_annotation = False\n",
    "\n",
    "    # Read epochs\n",
    "    temp_epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    if reject_bad_segments == \"annot\":\n",
    "        custom_annotations = get_annotations(annot_file)\n",
    "        bads = get_bads_by_annotation(custom_annotations)\n",
    "    elif reject_bad_segments == \"peak-to-peak\":\n",
    "        bads = get_bads_by_peak_to_peak_amplitude(temp_epochs)\n",
    "    else:\n",
    "        epochs = temp_epochs\n",
    "        return epochs\n",
    "\n",
    "    if mask is None:\n",
    "        epochs = clear_bads(temp_epochs, bads)\n",
    "    elif len(mask) == 64:\n",
    "        epochs = reject_with_mask(temp_epochs, mask, bads)\n",
    "    else:\n",
    "        print(\n",
    "            \"Given mask has wrong shape. Expected len of 64 but got {}\".format(\n",
    "                len(mask)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=[\"Rumination Full Scale\"],\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    header_files = glob.glob(\"../data/responses/*.vhdr\")\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 10 or len(correct) < 10:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, correct, error, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, correct, error, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        rumination_df = pd.read_csv(info_filename, usecols=[\"File\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"File\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "\n",
    "    for epoch in correct:\n",
    "        epoch_df = pd.DataFrame(\n",
    "            {\"id\": [id], \"epoch\": [epoch], \"marker\": [CORRECT]}\n",
    "        ).join(info_df)\n",
    "        participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    for epoch in error:\n",
    "        epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [epoch], \"marker\": [ERROR]}).join(\n",
    "            info_df\n",
    "        )\n",
    "        participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(info_filename=info_filename)\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data is now read into dataframe and each epoch is a single record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sorting participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped_df = epochs_df.groupby(\"id\")\n",
    "epochs_df[\"error_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == ERROR).sum()\n",
    ")\n",
    "epochs_df[\"correct_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs_df = epochs_df.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Training and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Computes ICA and then at each channel computes CWT (ica_n_components = N).\n",
    "- For each band (frequency) from CWT set it computes features given in feature_dict parameter (eg. std or mean).\n",
    "- Then it computes PCA on flattened ICA channels and features (outer_components = N)\n",
    "- Ending feature vector has shape: outer_components from (ica_n_components * len(feature_dict) * frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_signal(t, m, e):\n",
    "    return np.std(m)\n",
    "\n",
    "\n",
    "def abs_diffs_signal(t, m, e):\n",
    "    return np.sum(np.abs(np.diff(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guo_features = {\n",
    "    \"std\": std_signal,\n",
    "    \"abs_diffs\": abs_diffs_signal,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    ## Note: does not handle mix 1d representation\n",
    "    # if _is_1d(y_true):\n",
    "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_adjusted_scorer(y_test, y_pred, p, n):\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r2_adj = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n",
    "    return r2_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressions grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning! It takes a lot of time!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a pipeline which allows manipulation of vectorization's parameters. Base steps dictionary consists all steps of vectorization including standarization of data. \n",
    "\n",
    "In rate_regression function, using GridSearchCV, cross-validation splitting strategy can be specified. Default cv = 5.\n",
    "Results of cross-validated search are in **grid_search.cv_results** and chosen model is in **grid_search.best_estimator_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"epoch\"].to_list())\n",
    "y = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"Rumination Full Scale\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IcaPreprocessingTransformer():\n",
    "    def transform(X):\n",
    "        timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "        return timepoints_per_channel.T\n",
    "\n",
    "    return FunctionTransformer(func=transform)\n",
    "\n",
    "\n",
    "def CwtVectorizer(timepoints_count, feature_dict, mwt=\"morl\", cwt_density=2):\n",
    "    def transform(X):\n",
    "        X_ica_transposed = X.T\n",
    "        ica_n_components = X.shape[1]\n",
    "\n",
    "        epochs_count = int(X_ica_transposed.shape[1] / timepoints_count)\n",
    "        data_per_channel = X_ica_transposed.reshape(\n",
    "            ica_n_components, epochs_count, timepoints_count\n",
    "        )\n",
    "\n",
    "        vectorized_data = []\n",
    "        for data in data_per_channel:\n",
    "            data_cwt = np.array([cwt(epoch, mwt, cwt_density) for epoch in data])\n",
    "\n",
    "            # cesium functions\n",
    "            feature_set_cwt = featurize.featurize_time_series(\n",
    "                times=None,\n",
    "                values=data_cwt,\n",
    "                errors=None,\n",
    "                features_to_use=list(feature_dict.keys()),\n",
    "                custom_functions=feature_dict,\n",
    "            )\n",
    "            features_per_epoch = feature_set_cwt.to_numpy()\n",
    "            vectorized_data.append(features_per_epoch)\n",
    "\n",
    "        vectorized_data = np.array(vectorized_data)\n",
    "        vectorized_data = np.stack(vectorized_data, axis=1)\n",
    "        epochs_per_channel_feature = vectorized_data.reshape(\n",
    "            vectorized_data.shape[0], -1\n",
    "        )\n",
    "        return epochs_per_channel_feature\n",
    "\n",
    "    return FunctionTransformer(func=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_steps = [\n",
    "    (\"ica_preprocessing\", IcaPreprocessingTransformer()),\n",
    "    (\"ica\", FastICA(random_state=5)),\n",
    "    (\n",
    "        \"cwt\",\n",
    "        CwtVectorizer(\n",
    "            timepoints_count=X.shape[-1],\n",
    "            feature_dict=guo_features,\n",
    "        ),\n",
    "    ),\n",
    "    (\"pca\", PCA(random_state=5)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_regression(\n",
    "    X_train, y_train, X_test, y_test, regressor, regressor_params, cv=5\n",
    "):\n",
    "    #     cachedir = mkdtemp()\n",
    "    pipeline = Pipeline(steps=base_steps + [regressor])\n",
    "    param_grid = regressor_params\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv,\n",
    "        scoring={\n",
    "            \"r2\": \"r2\",\n",
    "            \"mae\": \"neg_mean_absolute_error\",\n",
    "        },\n",
    "        refit=\"r2\",\n",
    "        n_jobs=3,\n",
    "        verbose=10,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    predictions = grid_search.predict(X_test)\n",
    "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = grid_search.score(X_test, y_test)\n",
    "    r2_adj = r2_adjusted_scorer(y_test, predictions, len(X_test[0]), len(X_test))\n",
    "    print(\n",
    "        f\"Best result: MAPE: {mape} MAE: {mae} MSE: {mse} R^2: {r2} R^2 adjusted: {r2_adj}\"\n",
    "    )\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction with SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = (\"svr\", SVR(kernel=\"rbf\"))\n",
    "regressor_params = dict(\n",
    "    ica__n_components=np.arange(35, 36, 1),\n",
    "    pca__n_components=np.arange(20, 21, 1),\n",
    "    svr__C=np.arange(1, 2, 1),\n",
    "    svr__gamma=[0.1],\n",
    "    svr__epsilon=[0.1],\n",
    ")\n",
    "svr_grid_search_df = rate_regression(\n",
    "    X_train, y_train, X_test, y_test, svr, regressor_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy Classifier for baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dummy_regr.predict(X_test)\n",
    "print(mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(model.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
