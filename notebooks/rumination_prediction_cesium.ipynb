{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Rumination prediction with cesium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are calculated for each band from cwt separately.\n",
    "Different strategies of ICA and PCA application:\n",
    "\n",
    "- ICA + PCA on all kind of features (mean, std itd) -> do not increase effectivness of the model\n",
    "\n",
    "- ICA + PCA on two the best kind of features (std + abs_diff) -> increase effectivness of the model a lot. Second the best method:  \n",
    "\n",
    "    - 9 * (3 from 2*14) = 27 components\n",
    "    \n",
    "            MAPE: 17.9079258661464\n",
    "            MAE: 0.519917817723914\n",
    "            MSE: 0.478030015754372\n",
    "            R^2: 0.376125673763408\n",
    "\n",
    "    - 9 * (2 from 2*14) = 18 components\n",
    "\n",
    "            MAPE: 16.8443619132606\n",
    "            MAE: 0.502078488977514\n",
    "            MSE: 0.454436875307531\n",
    "            R^2: 0.406916950702055\n",
    "\n",
    "\n",
    "\n",
    "- ICA + PCA separately on std features and abs_diffs features -> ok, but not the best way\n",
    "    \n",
    "    - 6*(5+5) = 60 components\n",
    "\n",
    "            MAPE: 24.018846090150365\n",
    "            MAE: 0.6884866303220416\n",
    "            MSE: 0.7185622829832321\n",
    "            R^2: 0.062208343867806604\n",
    "\n",
    "    - 6*(3+3) = 36 components\n",
    "\n",
    "            MAPE: 21.50178345656211\n",
    "            MAE: 0.6201825754057195\n",
    "            MSE: 0.6069731329548679\n",
    "            R^2: 0.2078427255904629\n",
    "\n",
    "    - 6*(2+2) = 24 components\n",
    "\n",
    "            19.438263116024583\n",
    "            0.5637319078555214\n",
    "            0.5362710537170331\n",
    "            0.3001156176565023\n",
    "\n",
    "    - 6*(1+1) = 12 components\n",
    "\n",
    "            19.85447763226303\n",
    "            0.5760456075230935\n",
    "            0.585240921194229\n",
    "            0.23620531480654705\n",
    "\n",
    "    - 5*(3+3) = 30 components\n",
    "\n",
    "            MAPE: 20.536346943303492\n",
    "            MAE: 0.5910318538690427\n",
    "            MSE: 0.5747574611935253\n",
    "            R^2: 0.24988722039619093\n",
    "\n",
    "    - 5*(2+2) = 20 components\n",
    "\n",
    "            19.35858512090129\n",
    "            0.5635286480139711\n",
    "            0.5433040170537524\n",
    "            0.2909369361542158\n",
    "            \n",
    "    - 5*(1+1) = 10 components\n",
    "    \n",
    "            20.250558073766026\n",
    "            0.5886032629783092\n",
    "            0.5852655385369406\n",
    "            0.23617318684890465\n",
    "\n",
    "    - 4*(4+4) = 32 components\n",
    "\n",
    "            MAPE: 22.031474920258777\n",
    "            MAE: 0.6380830691061724\n",
    "            MSE: 0.6277453555688914\n",
    "            R^2: 0.1807330128932182\n",
    "            \n",
    "   \n",
    "- PCA on flattened ICA channels and PCA separately on std features and abs_diffs features -> more research needed\n",
    "    \n",
    "    - 30 from 6*(5+5) = 30 components\n",
    "    \n",
    "            MAPE: 21.44571192549426\n",
    "            MAE: 0.6261229886064391\n",
    "            MSE: 0.6193306483997083\n",
    "            R^2: 0.19171500061917268\n",
    "       \n",
    "    - 30 from 6*(4+4) = 30 components\n",
    "    \n",
    "            MAPE: 21.24921683786726\n",
    "            MAE: 0.615903392719023\n",
    "            MSE: 0.608353978391557\n",
    "            R^2: 0.20604059185814527\n",
    "    \n",
    "    - 30 from 6*(3+3) = 30 components\n",
    "    \n",
    "            MAPE: 21.11155762577906\n",
    "            MAE: 0.6154703465611149\n",
    "            MSE: 0.604724357788718\n",
    "            R^2: 0.21077758960611548\n",
    "    \n",
    "    - 30 from 5*(5+5) = 30 components\n",
    "    \n",
    "            MAPE: 21.67828659174979\n",
    "            MAE: 0.6323799032109819\n",
    "            MSE: 0.6303037217274262\n",
    "            R^2: 0.17739410338791517\n",
    "    \n",
    "    - 30 from 5*(4+4) = 30 components\n",
    "    \n",
    "            MAPE: 21.317971705648553\n",
    "            MAE: 0.615749388609156\n",
    "            MSE: 0.6219822296997861\n",
    "            R^2: 0.1882544365488662\n",
    "    \n",
    "    - 30 from 5*(3+3) = 30 components\n",
    "    \n",
    "            MAPE: 21.220223056606272\n",
    "            MAE: 0.6138419950553302\n",
    "            MSE: 0.6099786827888294\n",
    "            R^2: 0.20392019914685844\n",
    "    \n",
    "    - 30 from 4*(4+4) = 30 components\n",
    "    \n",
    "            MAPE; 22.469724109237735\n",
    "            MAE: 0.6509080551097475\n",
    "            MSE: 0.6534929953956176\n",
    "            R^2: 0.14712991074547543\n",
    "   \n",
    "   \n",
    "- PCA on flattened ICA channels and (std + abs_diff) feature sets -> **the best results:**\n",
    "    \n",
    "        Example:\n",
    "        \n",
    "        \n",
    "     -  PCA: 18; ICA: 9:\n",
    "     \n",
    "             MAPE: 16.1051630051677\n",
    "             MAE: 0.481721094094576\n",
    "             MSE: 0.412577754295216\n",
    "             R^2: 0.461547057719921\n",
    "             \n",
    "             \n",
    "     - PCA: 18; ICA: 18:\n",
    "     \n",
    "             MAPE: 11.6348912814115\n",
    "             MAE: 0.35025761618236\n",
    "             MSE: 0.237247454583717\n",
    "             R^2: 0.690369660896321\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data load all info from given file but ones can specify it passing list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' | 'annot' | 'peak-to-peak'\n",
    "\n",
    "        Whether the epochs with overlapping bad segments are rejected by default.\n",
    "\n",
    "        'auto' means that bad segments are rejected automatically.\n",
    "        'annot' rejection based on annotations and reject only channels annotated in .vmrk file as\n",
    "        'bad'.\n",
    "        'peak-to-peak' rejection based on peak-to-peak amplitude of channels.\n",
    "\n",
    "        Rejected with 'annot' and 'amplitude' channels are zeroed.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(\"../data/\" + file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(\"../data/\" + annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = True\n",
    "\n",
    "    if reject_bad_segments != \"auto\":\n",
    "        this_reject_by_annotation = False\n",
    "\n",
    "    # Read epochs\n",
    "    temp_epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    if reject_bad_segments == \"annot\":\n",
    "        custom_annotations = get_annotations(annot_file)\n",
    "        bads = get_bads_by_annotation(custom_annotations)\n",
    "    elif reject_bad_segments == \"peak-to-peak\":\n",
    "        bads = get_bads_by_peak_to_peak_amplitude(temp_epochs)\n",
    "    else:\n",
    "        epochs = temp_epochs\n",
    "        return epochs\n",
    "\n",
    "    if mask is None:\n",
    "        epochs = clear_bads(temp_epochs, bads)\n",
    "    elif len(mask) == 64:\n",
    "        epochs = reject_with_mask(temp_epochs, mask, bads)\n",
    "    else:\n",
    "        print(\n",
    "            \"Given mask has wrong shape. Expected len of 64 but got {}\".format(\n",
    "                len(mask)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=[\"Rumination Full Scale\"],\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    header_files = glob.glob(\"../data/responses/*.vhdr\")\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 10 or len(correct) < 10:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, correct, error, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, correct, error, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        rumination_df = pd.read_csv(info_filename, usecols=[\"File\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"File\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "\n",
    "    for epoch in correct:\n",
    "        epoch_df = pd.DataFrame(\n",
    "            {\"id\": [id], \"epoch\": [epoch], \"marker\": [CORRECT]}\n",
    "        ).join(info_df)\n",
    "        participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    for epoch in error:\n",
    "        epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [epoch], \"marker\": [ERROR]}).join(\n",
    "            info_df\n",
    "        )\n",
    "        participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(info_filename=info_filename)\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data is now read into dataframe and each epoch is a single record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sorting participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped_df = epochs_df.groupby(\"id\")\n",
    "epochs_df[\"error_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == ERROR).sum()\n",
    ")\n",
    "epochs_df[\"correct_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs_df = epochs_df.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from cesium import featurize\n",
    "\n",
    "# This function computes ICA and then at each channel computes CWT (ica_n_components = N).\n",
    "# For each band (frequency) from CWT set it computes features given in feature_dict parameter (eg. std or mean).\n",
    "# Then it computes PCA on flattened ICA channels and features (outer_components = N)\n",
    "# Ending feature vector has shape: outer_components from (ica_n_components * len(feature_dict) * frequencies)\n",
    "\n",
    "\n",
    "def vectorize(\n",
    "    X,\n",
    "    feature_dict,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"PCA\",\n",
    "    outer_components=30,\n",
    "):\n",
    "    #     print(\"X shape: {}\".format(X.shape))\n",
    "\n",
    "    # compute ICA for reducing dim from 64-channel to ica-n-components signal.\n",
    "    # for ICA shape must be like  (n_samples, n_features) -> timepoints_per_channel.shape.T == (epochs*timepoints, num_of_channels)\n",
    "    timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    X_ica = ica.fit_transform(timepoints_per_channel.T)\n",
    "\n",
    "    # reshaping X_ica for recover (channel, epoch, timepoints) structure instead (epochs*timepoints, channel)\n",
    "    X_ica_transposed = X_ica.T\n",
    "    data_per_channel = X_ica_transposed.reshape(\n",
    "        ica_n_components, X.shape[0], X.shape[-1]\n",
    "    )\n",
    "\n",
    "    vectorized_data = []\n",
    "\n",
    "    for data in data_per_channel:\n",
    "        data_cwt = np.array([cwt(epoch, mwt, cwt_density) for epoch in data])\n",
    "\n",
    "        # cesium functions\n",
    "        feature_set_cwt = featurize.featurize_time_series(\n",
    "            times=None,\n",
    "            values=data_cwt,\n",
    "            errors=None,\n",
    "            features_to_use=list(feature_dict.keys()),\n",
    "            custom_functions=feature_dict,\n",
    "        )\n",
    "\n",
    "        #         std_features = feature_set_cwt[\"std\"]\n",
    "        #         abs_diffs_features = feature_set_cwt[\"abs_diffs\"]\n",
    "\n",
    "        #         # draw n the best components from cesium-made feature set with PCA\n",
    "        #         pca_std = PCA(n_components=extracted_n_components)\n",
    "        #         pca_std_components_per_epoch = pca_std.fit_transform(std_features)\n",
    "\n",
    "        #         # draw n the best components from cesium-made feature set with PCA\n",
    "        #         pca_diffs = PCA(n_components=extracted_n_components)\n",
    "        #         pca_diffs_components_per_epoch = pca_diffs.fit_transform(abs_diffs_features)\n",
    "\n",
    "        #         selected_features = np.append(\n",
    "        #             pca_std_components_per_epoch, pca_diffs_components_per_epoch, axis=1\n",
    "        #         )\n",
    "\n",
    "        features_per_epoch = feature_set_cwt.to_numpy()\n",
    "        vectorized_data.append(features_per_epoch)\n",
    "\n",
    "    vectorized_data = np.array(vectorized_data)\n",
    "    vectorized_data = np.stack(vectorized_data, axis=1)\n",
    "    epochs_per_channel_feature = vectorized_data.reshape(vectorized_data.shape[0], -1)\n",
    "\n",
    "    pca_outer = PCA(n_components=outer_components)\n",
    "    pca_outer_components_per_epoch = pca_outer.fit_transform(epochs_per_channel_feature)\n",
    "\n",
    "    #     print(\"Vectorized X shape: {}\".format(pca_outer_components_per_epoch.shape))\n",
    "\n",
    "    return pca_outer_components_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from cesium import featurize\n",
    "\n",
    "# This function computes ICA and then at each channel computes CWT (ica_n_components = N).\n",
    "# For each band (frequency) from CWT set it computes features given in feature_dict parameter (eg. std or mean).\n",
    "# Then it computes PCA features set for each ICA channel  (extracted_n_components = N)\n",
    "# Ending feature vector has shape: ica_n_components * extracted_n_components from (len(feature_dict) * frequencies)\n",
    "\n",
    "\n",
    "def vectorize_2(\n",
    "    X,\n",
    "    feature_dict,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"PCA\",\n",
    "    extracted_n_components=3,\n",
    "):\n",
    "    #     print(\"X shape: {}\".format(X.shape))\n",
    "\n",
    "    # compute ICA for reducing dim from 64-channel to ica-n-components signal.\n",
    "    # for ICA shape must be like  (n_samples, n_features) -> timepoints_per_channel.shape.T == (epochs*timepoints, num_of_channels)\n",
    "    timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    X_ica = ica.fit_transform(timepoints_per_channel.T)\n",
    "\n",
    "    # reshaping X_ica for recover (channel, epoch, timepoints) structure instead (epochs*timepoints, channel)\n",
    "    X_ica_transposed = X_ica.T\n",
    "    data_per_channel = X_ica_transposed.reshape(\n",
    "        ica_n_components, X.shape[0], X.shape[-1]\n",
    "    )\n",
    "\n",
    "    vectorized_data = []\n",
    "\n",
    "    for data in data_per_channel:\n",
    "        data_cwt = np.array([cwt(epoch, mwt, cwt_density) for epoch in data])\n",
    "\n",
    "        # cesium functions\n",
    "        feature_set_cwt = featurize.featurize_time_series(\n",
    "            times=None,\n",
    "            values=data_cwt,\n",
    "            errors=None,\n",
    "            features_to_use=list(feature_dict.keys()),\n",
    "            custom_functions=feature_dict,\n",
    "        )\n",
    "\n",
    "        #         std_features = feature_set_cwt[\"std\"]\n",
    "        #         abs_diffs_features = feature_set_cwt[\"abs_diffs\"]\n",
    "\n",
    "        #         # draw n the best components from cesium-made feature set with PCA\n",
    "        #         pca_std = PCA(n_components=extracted_n_components)\n",
    "        #         pca_std_components_per_epoch = pca_std.fit_transform(std_features)\n",
    "\n",
    "        #         # draw n the best components from cesium-made feature set with PCA\n",
    "        #         pca_diffs = PCA(n_components=extracted_n_components)\n",
    "        #         pca_diffs_components_per_epoch = pca_diffs.fit_transform(abs_diffs_features)\n",
    "\n",
    "        #         selected_features = np.append(\n",
    "        #             pca_std_components_per_epoch, pca_diffs_components_per_epoch, axis=1\n",
    "        #         )\n",
    "\n",
    "        pca = PCA(n_components=extracted_n_components)\n",
    "        pca_components_per_epoch = pca.fit_transform(feature_set_cwt)\n",
    "\n",
    "        vectorized_data.append(pca_components_per_epoch)\n",
    "\n",
    "    vectorized_data = np.array(vectorized_data)\n",
    "    vectorized_data = np.stack(vectorized_data, axis=1)\n",
    "    epochs_per_channel_feature = vectorized_data.reshape(vectorized_data.shape[0], -1)\n",
    "\n",
    "    #     print(\"Vectorized X shape: {}\".format(epochs_per_channel_feature.shape))\n",
    "\n",
    "    return epochs_per_channel_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"epoch\"].to_list())\n",
    "y = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"Rumination Full Scale\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized_X = vectorize(\n",
    "#     X,\n",
    "#     feature_dict=guo_features,\n",
    "#     ica_n_components=6,\n",
    "#     mwt=\"morl\",\n",
    "#     outer_components=20,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized_X_df = pd.DataFrame(\n",
    "#     vectorized_X, columns=np.arange(0, vectorized_X.shape[1], 1)\n",
    "# )\n",
    "# vectorized_X_df[\"y\"] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization with different statistical functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guo_features = {\n",
    "    \"std\": std_signal,\n",
    "    \"abs_diffs\": abs_diffs_signal,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "def std_signal(t, m, e):\n",
    "    return np.std(m)\n",
    "\n",
    "\n",
    "def abs_diffs_signal(t, m, e):\n",
    "    return np.sum(np.abs(np.diff(m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    ## Note: does not handle mix 1d representation\n",
    "    # if _is_1d(y_true):\n",
    "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and prediction on SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    vectorized_X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaled_X_train = scaler.transform(X_train)\n",
    "model = SVR(kernel=\"rbf\", C=1, gamma=0.1, epsilon=0.1)\n",
    "model.fit(rescaled_X_train, y_train)\n",
    "\n",
    "# transform the validation dataset\n",
    "rescaled_X_test = scaler.transform(X_test)\n",
    "predictions = model.predict(rescaled_X_test)\n",
    "print(mean_absolute_percentage_error(y_test, predictions))\n",
    "print(mean_absolute_error(y_test, predictions))\n",
    "print(mean_squared_error(y_test, predictions))\n",
    "print(model.score(rescaled_X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching the best ICA and PCA amount of components - manual search best way of vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA on on whole feature set - through all ICA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_outer_df = pd.DataFrame(columns=[\"ICA\", \"PCA\", \"MAPE\", \"MAE\", \"MSE\", \"R^2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(20, 30, 1):\n",
    "    for p in np.arange(10, 26, 1):\n",
    "        vectorized_X = vectorize(\n",
    "            X,\n",
    "            feature_dict=guo_features,\n",
    "            ica_n_components=i,\n",
    "            mwt=\"morl\",\n",
    "            outer_components=p,\n",
    "        )\n",
    "\n",
    "        print(\"ICA N COMPONENTS: {} PCA OUTER N COMPONENTS: {}\".format(i, p))\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            vectorized_X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        rescaled_X_train = scaler.transform(X_train)\n",
    "        model = SVR(kernel=\"rbf\", C=1, gamma=0.1, epsilon=0.1)\n",
    "        model.fit(rescaled_X_train, y_train)\n",
    "\n",
    "        # transform the validation dataset\n",
    "        rescaled_X_test = scaler.transform(X_test)\n",
    "        predictions = model.predict(rescaled_X_test)\n",
    "        print(mean_absolute_percentage_error(y_test, predictions))\n",
    "        print(mean_absolute_error(y_test, predictions))\n",
    "        print(mean_squared_error(y_test, predictions))\n",
    "        print(model.score(rescaled_X_test, y_test))\n",
    "        diff_outer_df = diff_outer_df.append(\n",
    "            {\n",
    "                \"ICA\": i,\n",
    "                \"PCA\": p,\n",
    "                \"MAPE\": mean_absolute_percentage_error(y_test, predictions),\n",
    "                \"MAE\": mean_absolute_error(y_test, predictions),\n",
    "                \"MSE\": mean_squared_error(y_test, predictions),\n",
    "                \"R^2\": model.score(rescaled_X_test, y_test),\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA inside ICA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in np.arange(1, 10, 1):\n",
    "    for p in np.arange(1, 10, 1):\n",
    "        vectorized_X = vectorize_2(\n",
    "            X,\n",
    "            feature_dict=guo_features,\n",
    "            ica_n_components=i,\n",
    "            mwt=\"morl\",\n",
    "            extracted_n_components=p,\n",
    "        )\n",
    "\n",
    "        print(\"ICA N COMPONENTS: {} PCA N COMPONENTS: {}\".format(i, p))\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            vectorized_X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        rescaled_X_train = scaler.transform(X_train)\n",
    "        model = SVR(kernel=\"rbf\", C=1, gamma=0.1, epsilon=0.1)\n",
    "        model.fit(rescaled_X_train, y_train)\n",
    "\n",
    "        # transform the validation dataset\n",
    "        rescaled_X_test = scaler.transform(X_test)\n",
    "        predictions = model.predict(rescaled_X_test)\n",
    "        print(mean_absolute_percentage_error(y_test, predictions))\n",
    "        print(mean_absolute_error(y_test, predictions))\n",
    "        print(mean_squared_error(y_test, predictions))\n",
    "        print(model.score(rescaled_X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressions grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: exhaustive Grid Search with ICA and PCA n_components  as parameters!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy Classifier for baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dummy_regr.predict(X_test)\n",
    "print(mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
