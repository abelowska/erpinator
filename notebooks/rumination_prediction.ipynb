{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Rumination prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data load only info about rumination but ones can specify it passing list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(info_filename=info_filename)\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data is now read into dataframe and each epoch is a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "display(epochs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sorting participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped_df = epochs_df.groupby(\"id\")\n",
    "epochs_df[\"error_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == ERROR).sum()\n",
    ")\n",
    "epochs_df[\"correct_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs_df = epochs_df.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "display(epochs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_df[\"epoch\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\n",
    "    [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]],\n",
    "    [[5, 5, 5], [6, 6, 6], [7, 7, 7], [8, 8, 8]],\n",
    "    [[9, 9, 9], [10, 10, 10], [11, 11, 11], [12, 12, 12]],\n",
    "]\n",
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.concatenate(a, axis=1)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b.T\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_num = 4\n",
    "epoch_num = 3\n",
    "times_len = 3\n",
    "c = x.reshape(channel_num, epoch_num, times_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.stack(c, axis=1)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_2(\n",
    "    X,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"PCA\",\n",
    "    wv_weighting_n_components=3,\n",
    "):\n",
    "    print(\"X shape: {}\".format(X.shape))\n",
    "\n",
    "    # compute ICA for reducing dim from 64-channel to ica-n-components signal.\n",
    "    # for ICA shape must be like  (n_samples, n_features) -> timepoints_per_channel.shape.T == (epochs*timepoints, num_of_channels)\n",
    "    timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    X_ica = ica.fit_transform(timepoints_per_channel.T)\n",
    "    print(\"X_ica transformed shape: {}\".format(X_ica.shape))\n",
    "\n",
    "    # reshaping X_ica for recover (channel, epoch, timepoints) structure instead (epochs*timepoints, channel)\n",
    "    X_ica_transposed = X_ica.T\n",
    "    data_per_channel = X_ica_transposed.reshape(\n",
    "        ica_n_components, X.shape[0], X.shape[-1]\n",
    "    )\n",
    "\n",
    "    for data in data_per_channel:\n",
    "        print(\"Data in one channel shape: {}\".format(data.shape))\n",
    "\n",
    "\n",
    "#         X_cwts = np.array([cwt(epoch, mwt, cwt_density) for epoch in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ICA reduces channles from 64 to given amount of independent components\n",
    "- Continous Wavelet Transform decompose signal of channel from each epoch into set of wavelets functions\n",
    "- PCA reducing dimention of features (wavelets) into computed best ones\n",
    "\n",
    "Each epoch is vectorized as (ica_components*pca_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def vectorize(\n",
    "    X,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"PCA\",\n",
    "    wv_weighting_n_components=3,\n",
    "):\n",
    "\n",
    "    # compute ICA for reducing dim from 64-channel to ica-n-components signal.\n",
    "    # for ICA shape must be like  (n_samples, n_features) -> timepoints_per_channel.shape == (num_of_channels, timepoints)\n",
    "\n",
    "    print(X.shape)\n",
    "    timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    ica.fit(timepoints_per_channel.T)\n",
    "\n",
    "    params = []\n",
    "    X_features = []\n",
    "\n",
    "    for spatial_filter in ica.components_:\n",
    "        # apply ICA for creating independent channel (matrixes mul). Channel has shape EPOCHS x TIMEPOINTS\n",
    "        X_filtered = np.tensordot(X, spatial_filter, axes=([1], [0]))\n",
    "        print(\"Filtered shape: {}\".format(X_filtered.shape))\n",
    "\n",
    "        # compute CWT for channel.\n",
    "        # CWT decompose signal into set of basis functions consisting transformations of mother wavelet function.\n",
    "        # After decomposing it has shape EPOCH x FREQUENCY x TIMEPOINT\n",
    "        X_cwts = np.array([cwt(epoch, mwt, cwt_density) for epoch in X_filtered])\n",
    "\n",
    "        # PCA need 2-dim array (n_samples, n_features)\n",
    "        # prepare X set for PCA -> flatten two last dimenstion.\n",
    "        X_flattened = X_cwts.reshape(X_cwts.shape[0], -1)\n",
    "\n",
    "        # compute which waveltes (factors) are the most significant\n",
    "        if wv_weighting == \"PCA\":\n",
    "            pca = PCA(n_components=wv_weighting_n_components)\n",
    "            pca.fit(X_flattened)\n",
    "            wv_weights = pca.components_\n",
    "        elif wv_weighting == \"ICA\":\n",
    "            ica = FastICA(n_components=wv_weighting_n_components, tol=0.001)\n",
    "            ica.fit(X_flattened)\n",
    "            wv_weights = ica.components_\n",
    "        elif wv_weighting == \"LDA\":\n",
    "            lda = LinearDiscriminantAnalysis(n_components=wv_weighting_n_components)\n",
    "            lda.fit(X_flattened, y)\n",
    "            wv_weights = lda.scalings_\n",
    "        else:\n",
    "            raise ValueError(\"wrong wv_choice argument\")\n",
    "\n",
    "        # unflatten wv_weights\n",
    "        cwt_shape = X_cwts.shape[1:]  # FREQUENCY x TIMEPOINT shape\n",
    "        wv_weights = wv_weights.reshape(wv_weighting_n_components, *cwt_shape)\n",
    "        # X_cwts has a shape EPOCH x FREQUENCY x TIMEPOINT\n",
    "        # wv_weights has a shape  WAVELET_COMPONENT x FREQUENCY x TIMEPOINT\n",
    "        one_channel_X_features = np.tensordot(X_cwts, wv_weights, axes=([1, 2], [1, 2]))\n",
    "        # one_channel_X_features has a shape EPOCH x WAVELET_COMPONENT\n",
    "\n",
    "        params.append((spatial_filter, wv_weights))\n",
    "        X_features.append(one_channel_X_features)\n",
    "\n",
    "    # transform from shape (ICA_COMP x EPOCH x WAVELET_COMP) to (EPOCH x ICA_COMP x WAVELET_COMP)\n",
    "    X_features = np.array(X_features).transpose((1, 0, 2))\n",
    "\n",
    "    return X_features, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    X,\n",
    "    y,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"PCA\",\n",
    "    wv_weighting_n_components=3,\n",
    "):\n",
    "    # X has a shape EPOCHS x CHANNELS x TIMEPOINTS\n",
    "    # y has a shape EPOCHS\n",
    "\n",
    "    features, params = vectorize(\n",
    "        X,\n",
    "        mwt,\n",
    "        cwt_density,\n",
    "        ica_n_components,\n",
    "        wv_weighting,\n",
    "        wv_weighting_n_components,\n",
    "    )\n",
    "\n",
    "    # flatten features into shape EPOCH x (ICA_COMP*WAVELET_COMP)\n",
    "    features = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    clf = None\n",
    "    # create model\n",
    "\n",
    "    return params, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(epochs_df, **hyperparams):\n",
    "    # get only epochs from error responses\n",
    "    X = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"epoch\"].to_list())\n",
    "    # rumination levels for X\n",
    "    y = np.array(\n",
    "        epochs_df[epochs_df[\"marker\"] == ERROR][\"Rumination Full Scale\"].to_list()\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    params, clf = train(X_train, y_train, **hyperparams)\n",
    "    vectorize_2(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(epochs_df, mwt=\"mexh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
