{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Rumination prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data load only info about rumination but ones can specify it passing list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(info_filename=info_filename)\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data is now read into dataframe and each epoch is a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "display(epochs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sorting participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped_df = epochs_df.groupby(\"id\")\n",
    "epochs_df[\"error_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == ERROR).sum()\n",
    ")\n",
    "epochs_df[\"correct_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs_df = epochs_df.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "display(epochs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[[1, 1, 1], [2, 2, 2], [3, 3, 3]], [[4, 4, 4], [5, 5, 5], [6, 6, 6]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.concatenate(a, axis=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b.reshape(3, a.shape[0], a.shape[-1])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.stack(c, axis=1)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ICA reduces channles from 64 to given amount of independent components\n",
    "- Continous Wavelet Transform decompose signal of channel from each epoch into set of wavelets functions\n",
    "- PCA reducing dimention of features (wavelets) into computed best ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_2(\n",
    "    X,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"PCA\",\n",
    "    extracted_n_components=3,\n",
    "):\n",
    "    print(\"X shape: {}\".format(X.shape))\n",
    "\n",
    "    # compute ICA for reducing dim from 64-channel to ica-n-components signal.\n",
    "    # for ICA shape must be like  (n_samples, n_features) -> timepoints_per_channel.shape.T == (epochs*timepoints, num_of_channels)\n",
    "    timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    X_ica = ica.fit_transform(timepoints_per_channel.T)\n",
    "\n",
    "    # reshaping X_ica for recover (channel, epoch, timepoints) structure instead (epochs*timepoints, channel)\n",
    "    X_ica_transposed = X_ica.T\n",
    "    data_per_channel = X_ica_transposed.reshape(\n",
    "        ica_n_components, X.shape[0], X.shape[-1]\n",
    "    )\n",
    "\n",
    "    vectorized_data = []\n",
    "\n",
    "    for data in data_per_channel:\n",
    "        data_cwt = np.array([cwt(epoch, mwt, cwt_density) for epoch in data])\n",
    "\n",
    "        # for PCA shape must be like  (n_samples, n_features) -> wavelets_per_epoch.shape == (epoch, frequencies*timepoints)\n",
    "        wavelets_per_epoch = data_cwt.reshape(data_cwt.shape[0], -1)\n",
    "\n",
    "        pca = PCA(n_components=extracted_n_components)\n",
    "        pca_components_per_epoch = pca.fit_transform(wavelets_per_epoch)\n",
    "        vectorized_data.append(pca_components_per_epoch)\n",
    "\n",
    "    vectorized_data = np.array(vectorized_data)\n",
    "    vectorized_data = np.stack(vectorized_data, axis=1)\n",
    "    print(\"Vectorized X shape: {}\".format(vectorized_data.shape))\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"epoch\"].to_list())\n",
    "y = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"Rumination Full Scale\"].to_list())\n",
    "\n",
    "X_vectorized = vectorize_2(X, extracted_n_components=10)\n",
    "epochs_per_channel_feature = X_vectorized.reshape(X_vectorized.shape[0], -1)\n",
    "print(epochs_per_channel_feature.shape)\n",
    "\n",
    "ica3_pca10_df = pd.DataFrame(data={\"X\": epochs_per_channel_feature.tolist(), \"y\": y})\n",
    "\n",
    "display(ica3_pca10_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    epochs_per_channel_feature, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# neigh = KNeighborsRegressor()\n",
    "neigh = RandomForestRegressor(n_estimators=1000)\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neigh.predict(X_test)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    ## Note: does not handle mix 1d representation\n",
    "    # if _is_1d(y_true):\n",
    "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = X_test[1]\n",
    "print(test_X)\n",
    "test_y = y_test[1]\n",
    "print(test_y)\n",
    "\n",
    "print(neigh.predict([test_X]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
