{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rumination prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data load only info about rumination but ones can specify it passing list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(info_filename=info_filename)\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is now read into dataframe and each epoch is a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(epochs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped_df = epochs_df.groupby(\"id\")\n",
    "epochs_df[\"error_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == ERROR).sum()\n",
    ")\n",
    "epochs_df[\"correct_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs_df = epochs_df.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(epochs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def vectorize(X, y, mwt=\"mexh\", cwt_density=2, ica_n_components=4):\n",
    "\n",
    "    # compute ICA for reducing dim from 64-channel to n-channel signal.\n",
    "    # ICA returns n independent components as n arrays of coeffs each of 64 channles (shape = nx64)\n",
    "\n",
    "    concat = np.concatenate(X, axis=1)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    ica.fit_transform(concat.T)\n",
    "\n",
    "    for spatial_filter in ica.components_:\n",
    "        # apply ICA for creating independent channel. Channel has shape EPOCHS x TIMEPOINTS\n",
    "        X_filtered = filter_(X, spatial_filter)\n",
    "\n",
    "        # compute CWT for channel.\n",
    "        # CWT decompose signal into set of basis functions consisting transformations of mother wavelet function.\n",
    "        # After decomposing it has shape EPOCH x FREQUENCY x TIMEPOINT\n",
    "        X_cwts = np.array([cwt(epoch, mwt, cwt_density) for epoch in X_filtered])\n",
    "        print(X_cwts.shape)\n",
    "\n",
    "        # PCA need 2-dim array (n_samples, n_features)\n",
    "        # prepare X set for PCA -> flatten two last dimenstion.\n",
    "\n",
    "        X_flattened = X_cwts.reshape(X_cwts.shape[0], -1)\n",
    "        print(X_flattened.shape)\n",
    "\n",
    "        #         compute PCA for reducing dim. It chose best principle\n",
    "\n",
    "        pca_n_components = 1\n",
    "        pca = PCA(n_components=pca_n_components)\n",
    "        pca_comps = pca.fit_transform(X_flattened)\n",
    "        print(pca_comps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only epochs from error responses\n",
    "X = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"epoch\"].to_list())\n",
    "# all participant's rumination level\n",
    "y = np.array(epochs_df[\"marker\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = vectorize(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, mwt=\"mexh\", cwt_density=2, ica_n_components=4, wavelet_choice=\"single\"):\n",
    "    # X has a shape EPOCHS x CHANNELS x TIMEPOINTS\n",
    "    # y has a shape EPOCHS\n",
    "\n",
    "    # compute ICA\n",
    "    concat = np.concatenate(X, axis=1)\n",
    "    # concat.shape == (num_of_channels, timepoints)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    ica.fit_transform(concat.T)\n",
    "    # ica.components_.shape == (n_components, num_of_channels)\n",
    "\n",
    "    features_meta = []\n",
    "    feature_values = []\n",
    "    for spatial_filter in ica.components_:\n",
    "        # apply ICA\n",
    "        X_filtered = filter_(X, spatial_filter)\n",
    "        # they have shape EPOCHS x TIMEPOINTS\n",
    "\n",
    "        # apply cwt\n",
    "        X_cwts = np.array([cwt(epoch, mwt, cwt_density) for epoch in X_filtered])\n",
    "        # it has a shape EPOCH x FREQUENCY x TIMEPOINT\n",
    "\n",
    "        if wavelet_choice == \"single\":\n",
    "            # find bets separating wavelet\n",
    "            separations = get_separations(X_cwts[y == ERROR], X_cwts[y == CORRECT])\n",
    "            # separations are shaped FREQUENCY x TIMEPOINT\n",
    "            index = np.unravel_index(separations.argmax(), separations.shape)\n",
    "            wavelet_weights = np.zeros_like(separations)\n",
    "            wavelet_weights[index] = 1\n",
    "        elif wavelet_choice == \"LDA\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"wrong wavelet_choice argument\")\n",
    "\n",
    "        # X_cwts has a shape EPOCH x FREQUENCY x TIMEPOINT\n",
    "        # wavelet_weights has a shape FREQUENCY x TIMEPOINT\n",
    "        X_end = np.tensordot(X_cwts, wavelet_weights, axes=([1, 2], [0, 1]))\n",
    "\n",
    "        features_meta.append((spatial_filter, wavelet_weights))\n",
    "        feature_values.append(X_end)\n",
    "\n",
    "    # create a classifier from end feature values\n",
    "    feature_values = np.array(feature_values)\n",
    "    # TODO maybe balance class sizes or priors somehow?\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(feature_values.T, y)\n",
    "\n",
    "    return features_meta, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(epochs, features, clf, mwt=\"mexh\", cwt_density=2):\n",
    "    end_values = []\n",
    "    for feature in features:\n",
    "        spatial_filter, wavelet_weights = feature\n",
    "\n",
    "        filtered = filter_(epochs, spatial_filter)\n",
    "\n",
    "        cwts = np.array([cwt(epoch, mwt, cwt_density) for epoch in filtered])\n",
    "        # EPOCH x FREQUENCY x TIMEPOINT\n",
    "\n",
    "        end = np.tensordot(cwts, wavelet_weights, axes=([1, 2], [0, 1]))\n",
    "        end_values.append(end)\n",
    "\n",
    "    end_values = np.array(end_values)\n",
    "    probs = clf.predict_proba(end_values.T)\n",
    "    return probs[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "print(\"participant            AUROC   err/corr\")\n",
    "mwt = \"mexh\"\n",
    "aurocs = []\n",
    "auroc_sems = []\n",
    "\n",
    "\n",
    "# group data by participants' ids\n",
    "grouped = epochs_df.groupby([\"id\"])\n",
    "for participant_id in epochs_df[\"id\"].unique():\n",
    "    participant_df = grouped.get_group(participant_id)\n",
    "\n",
    "    X = np.array(participant_df[\"epoch\"].to_list())\n",
    "\n",
    "    # you can change y set in a easy way ---> y=np.array(participant_df[\"column_name\"].to_list())\n",
    "    y = np.array(participant_df[\"marker\"].to_list())\n",
    "\n",
    "    aurocs_personal = []\n",
    "    # KFold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=4)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # train\n",
    "        features, clf = train(X_train, y_train, mwt, wavelet_choice=\"single\")\n",
    "\n",
    "        # test\n",
    "        y_pred = predict(X_test, features, clf, mwt)\n",
    "\n",
    "        auroc = roc_auc_score(y_test, y_pred)\n",
    "        aurocs_personal.append(auroc)\n",
    "\n",
    "    aurocs.append(np.mean(aurocs_personal))\n",
    "    auroc_sems.append(scipy.stats.sem(aurocs_personal))\n",
    "\n",
    "    error_size = epochs_df[epochs_df[\"id\"] == participant_id][\"error_sum\"].iloc[0]\n",
    "    correct_size = epochs_df[epochs_df[\"id\"] == participant_id][\"correct_sum\"].iloc[0]\n",
    "    #     any_statistic_you_need = epochs_df[epochs_df[\"id\"] == participant_id][\"Rumination Full Scale\"].iloc[0]\n",
    "\n",
    "    print(\n",
    "        f\"{participant_id:11}    \"\n",
    "        f\"{aurocs[-1]:.3f} ± {auroc_sems[-1]:.3f}    \"\n",
    "        f\"{error_size:3}/{correct_size:3}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\ntraining time: {(time() - start) / 60:.0f} min\")\n",
    "# TODO is this line legit?\n",
    "total_sem = sum(np.array(auroc_sems) ** 2) ** (1 / 2) / len(auroc_sems)\n",
    "print(f\"mean AUROC: {np.mean(aurocs):.3f} ± {total_sem:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
