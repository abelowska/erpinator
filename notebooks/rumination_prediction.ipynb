{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Rumination prediction on single epochs\n",
    "\n",
    "Not very bad, but compared to using statistical function in vectorization - very poor.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- ICA components: 5   PCA components: 7\n",
    "\n",
    "        MAPE: 22.466791697223616\n",
    "        MAE: 0.6599905225143372\n",
    "        MSE: 0.6545453738857226\n",
    "        STD: 0.87534453886935\n",
    "        R^2: 0.14575645740609866\n",
    "        \n",
    "\n",
    "- ICA components: 6   PCA components: 5\n",
    "\n",
    "        MAPE: 22.081024621164122\n",
    "        MAE: 0.6499267175543688\n",
    "        MSE: 0.6516701147102215\n",
    "        STD: 0.87534453886935\n",
    "        R^2: 0.14950894223289457\n",
    "        \n",
    "- ICA components: 7   PCA components: 5\n",
    "\n",
    "        MAPE: 22.31193061256951\n",
    "        MAE: 0.6556837897338367\n",
    "        MSE: 0.661822957793486\n",
    "        STD: 0.87534453886935\n",
    "        R^2: 0.13625852295124818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Loading EEG data and data from rumination questionnaire. By default create_df_data load all info from given file but ones can specify it passing list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' | 'annot' | 'peak-to-peak'\n",
    "\n",
    "        Whether the epochs with overlapping bad segments are rejected by default.\n",
    "\n",
    "        'auto' means that bad segments are rejected automatically.\n",
    "        'annot' rejection based on annotations and reject only channels annotated in .vmrk file as\n",
    "        'bad'.\n",
    "        'peak-to-peak' rejection based on peak-to-peak amplitude of channels.\n",
    "\n",
    "        Rejected with 'annot' and 'amplitude' channels are zeroed.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(\"../data/\" + file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(\"../data/\" + annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = True\n",
    "\n",
    "    if reject_bad_segments != \"auto\":\n",
    "        this_reject_by_annotation = False\n",
    "\n",
    "    # Read epochs\n",
    "    temp_epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    if reject_bad_segments == \"annot\":\n",
    "        custom_annotations = get_annotations(annot_file)\n",
    "        bads = get_bads_by_annotation(custom_annotations)\n",
    "    elif reject_bad_segments == \"peak-to-peak\":\n",
    "        bads = get_bads_by_peak_to_peak_amplitude(temp_epochs)\n",
    "    else:\n",
    "        epochs = temp_epochs\n",
    "        return epochs\n",
    "\n",
    "    if mask is None:\n",
    "        epochs = clear_bads(temp_epochs, bads)\n",
    "    elif len(mask) == 64:\n",
    "        epochs = reject_with_mask(temp_epochs, mask, bads)\n",
    "    else:\n",
    "        print(\n",
    "            \"Given mask has wrong shape. Expected len of 64 but got {}\".format(\n",
    "                len(mask)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=[\"Rumination Full Scale\"],\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    header_files = glob.glob(\"../data/responses/*.vhdr\")\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 5 or len(correct) < 5:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, correct, error, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, correct, error, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        rumination_df = pd.read_csv(info_filename, usecols=[\"File\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"File\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "\n",
    "    for epoch in correct:\n",
    "        epoch_df = pd.DataFrame(\n",
    "            {\"id\": [id], \"epoch\": [epoch], \"marker\": [CORRECT]}\n",
    "        ).join(info_df)\n",
    "        participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    for epoch in error:\n",
    "        epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [epoch], \"marker\": [ERROR]}).join(\n",
    "            info_df\n",
    "        )\n",
    "        participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(info_filename=info_filename)\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data is now read into dataframe and each epoch is a single record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sorting participants by the number of errors, descending. This way the best participants are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# add new columns with info about error/correct responses amount\n",
    "grouped_df = epochs_df.groupby(\"id\")\n",
    "epochs_df[\"error_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == ERROR).sum()\n",
    ")\n",
    "epochs_df[\"correct_sum\"] = grouped_df[[\"marker\"]].transform(\n",
    "    lambda x: (x.values == CORRECT).sum()\n",
    ")\n",
    "\n",
    "# mergesort for stable sorting\n",
    "epochs_df = epochs_df.sort_values(\"error_sum\", ascending=False, kind=\"mergesort\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ICA reduces channles from 64 to given amount of independent components\n",
    "- Continous Wavelet Transform decompose signal of channel from each epoch into set of wavelets functions\n",
    "- PCA reducing dimention of features (wavelets) into computed best ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"epoch\"].to_list())\n",
    "y = np.array(epochs_df[epochs_df[\"marker\"] == ERROR][\"Rumination Full Scale\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(\n",
    "    X,\n",
    "    mwt=\"mexh\",\n",
    "    cwt_density=2,\n",
    "    ica_n_components=3,\n",
    "    wv_weighting=\"PCA\",\n",
    "    extracted_n_components=3,\n",
    "    window_size=16,\n",
    "):\n",
    "\n",
    "    # compute ICA for reducing dim from 64-channel to ica-n-components signal.\n",
    "    # for ICA shape must be like  (n_samples, n_features) -> timepoints_per_channel.shape.T == (epochs*timepoints, num_of_channels)\n",
    "    timepoints_per_channel = np.concatenate(X, axis=1)\n",
    "    ica = FastICA(n_components=ica_n_components)\n",
    "    X_ica = ica.fit_transform(timepoints_per_channel.T)\n",
    "\n",
    "    # reshaping X_ica for recover (channel, epoch, timepoints) structure instead (epochs*timepoints, channel)\n",
    "    X_ica_transposed = X_ica.T\n",
    "    data_per_channel = X_ica_transposed.reshape(\n",
    "        ica_n_components, X.shape[0], X.shape[-1]\n",
    "    )\n",
    "\n",
    "    vectorized_data = []\n",
    "\n",
    "    for data in data_per_channel:\n",
    "        data_cwt = np.array([cwt(epoch, mwt, cwt_density) for epoch in data])\n",
    "\n",
    "        #         averaged_data_cwt = []\n",
    "        #         for epoch in data_cwt:\n",
    "        #             this_scale = []\n",
    "        #             for scale in epoch:\n",
    "        #                 averaged_data = moving_averge(scale, window_size=window_size)\n",
    "        #                 this_scale.append(averaged_data)\n",
    "        #             averaged_data_cwt.append(this_scale)\n",
    "\n",
    "        #         averaged_data_cwt = np.array(averaged_data_cwt)\n",
    "        #         print(averaged_data_cwt.shape)\n",
    "\n",
    "        # for PCA shape must be like  (n_samples, n_features) -> wavelets_per_epoch.shape == (epoch, frequencies*timepoints)\n",
    "        wavelets_per_epoch = data_cwt.reshape(data_cwt.shape[0], -1)\n",
    "\n",
    "        pca = PCA(n_components=extracted_n_components)\n",
    "        pca_components_per_epoch = pca.fit_transform(wavelets_per_epoch)\n",
    "        #         print(pca.explained_variance_)\n",
    "        vectorized_data.append(pca_components_per_epoch)\n",
    "\n",
    "    vectorized_data = np.array(vectorized_data)\n",
    "    vectorized_data = np.stack(vectorized_data, axis=1)\n",
    "    epochs_per_channel_feature = vectorized_data.reshape(vectorized_data.shape[0], -1)\n",
    "\n",
    "    return epochs_per_channel_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_averge(x, window_size=16):\n",
    "    return (\n",
    "        pd.Series(x).rolling(window=window_size).mean().iloc[window_size - 1 :].values\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_X = vectorize(X, ica_n_components=6, mwt=\"morl\", extracted_n_components=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframe from vectorized X set for future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_X_df = pd.DataFrame(\n",
    "    vectorized_X, columns=np.arange(0, vectorized_X.shape[1], 1)\n",
    ")\n",
    "vectorized_X_df[\"y\"] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and predicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    ## Note: does not handle mix 1d representation\n",
    "    # if _is_1d(y_true):\n",
    "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    vectorized_X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick testing vectorization on SVR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaled_X_train = scaler.transform(X_train)\n",
    "model = SVR(kernel=\"rbf\", C=3, gamma=0.1, epsilon=0.1)\n",
    "model.fit(rescaled_X_train, y_train)\n",
    "\n",
    "# transform the validation dataset\n",
    "rescaled_X_test = scaler.transform(X_test)\n",
    "predictions = model.predict(rescaled_X_test)\n",
    "print(mean_absolute_percentage_error(y_test, predictions))\n",
    "print(mean_absolute_error(y_test, predictions))\n",
    "print(mean_squared_error(y_test, predictions))\n",
    "print(np.std(y_test))\n",
    "print(model.score(rescaled_X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressions grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gbm = dict(n_estimators=np.arange(20, 100, 5))\n",
    "param_grid_knn = dict(n_neighbors=np.arange(5, 100, 5))\n",
    "param_grid_svr = dict(\n",
    "    C=np.arange(1, 5, 1),\n",
    "    gamma=np.arange(0.1, 0.5, 0.1),\n",
    "    epsilon=np.arange(0.1, 0.5, 0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "pipelines.append(\n",
    "    (\"ScaledLR\", Pipeline([(\"Scaler\", StandardScaler()), (\"LR\", LinearRegression())]))\n",
    ")\n",
    "pipelines.append(\n",
    "    (\"ScaledEN\", Pipeline([(\"Scaler\", StandardScaler()), (\"EN\", ElasticNet())]))\n",
    ")\n",
    "pipelines.append(\n",
    "    (\n",
    "        \"ScaledKNN\",\n",
    "        Pipeline([(\"Scaler\", StandardScaler()), (\"KNN\", KNeighborsRegressor())]),\n",
    "    )\n",
    ")\n",
    "pipelines.append(\n",
    "    (\n",
    "        \"ScaledCART\",\n",
    "        Pipeline([(\"Scaler\", StandardScaler()), (\"CART\", DecisionTreeRegressor())]),\n",
    "    )\n",
    ")\n",
    "pipelines.append(\n",
    "    (\n",
    "        \"ScaledGBM\",\n",
    "        Pipeline([(\"Scaler\", StandardScaler()), (\"GBM\", GradientBoostingRegressor())]),\n",
    "    )\n",
    ")\n",
    "pipelines.append(\n",
    "    (\n",
    "        \"ScaledSVR\",\n",
    "        Pipeline([(\"Scaler\", StandardScaler()), (\"SVR\", SVR(kernel=\"rbf\"))]),\n",
    "    )\n",
    ")\n",
    "pipelines.append(\n",
    "    (\n",
    "        \"ScalerDummy\",\n",
    "        Pipeline(\n",
    "            [(\"Scaler\", StandardScaler()), (\"Dummy\", DummyRegressor(strategy=\"mean\"))]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=2)\n",
    "    cv_results = cross_val_score(\n",
    "        model, X_train, y_train, cv=kfold, scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"{}: {:f} ({:f})\".format(name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "model = SVR(kernel=\"rbf\")\n",
    "kfold = KFold(n_splits=10)\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid_svr,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=kfold,\n",
    ")\n",
    "grid_result = grid.fit(rescaledX, y_train)\n",
    "\n",
    "means = grid_result.cv_results_[\"mean_test_score\"]\n",
    "stds = grid_result.cv_results_[\"std_test_score\"]\n",
    "params = grid_result.cv_results_[\"params\"]\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"{:f} ({:f}) with: {!r}\".format(mean, stdev, param))\n",
    "\n",
    "print(\"Best: {:f} using {}\".format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy Classifier for baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dummy_regr.predict(X_test)\n",
    "print(mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA and PCA components test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pre-testing shows that ~30 is the best choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating tuples which give 30-36 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_indices = np.arange(0, 31, 1)\n",
    "pca_indices = np.arange(0, 31, 1)\n",
    "\n",
    "ica_pca_tuples = []\n",
    "\n",
    "for ica_component in ica_indices:\n",
    "    for pca_component in pca_indices:\n",
    "        if (ica_component * pca_component >= 30) and (\n",
    "            ica_component * pca_component <= 36\n",
    "        ):\n",
    "            ica_pca_tuples.append((ica_component, pca_component))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ica_component, pca_component in ica_pca_tuples:\n",
    "    vectorized_X = vectorize(\n",
    "        X,\n",
    "        ica_n_components=ica_component,\n",
    "        mwt=\"morl\",\n",
    "        extracted_n_components=pca_component,\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        vectorized_X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"ICA components: {}   PCA components: {}\".format(ica_component, pca_component)\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    rescaled_X_train = scaler.transform(X_train)\n",
    "    model = SVR(kernel=\"rbf\", C=2, gamma=0.1, epsilon=0.1)\n",
    "    model.fit(rescaled_X_train, y_train)\n",
    "\n",
    "    # transform the validation dataset\n",
    "    rescaled_X_test = scaler.transform(X_test)\n",
    "    predictions = model.predict(rescaled_X_test)\n",
    "    print(mean_absolute_percentage_error(y_test, predictions))\n",
    "    print(mean_absolute_error(y_test, predictions))\n",
    "    print(mean_squared_error(y_test, predictions))\n",
    "    print(np.std(y_test))\n",
    "    print(model.score(rescaled_X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
