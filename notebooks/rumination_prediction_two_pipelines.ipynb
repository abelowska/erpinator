{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a73099-38c1-4bf0-afd3-5481df7c3d19",
   "metadata": {},
   "source": [
    "# Rumination prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff9f56-5483-44c7-b9cc-486a31838279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e84d26-b32c-41ae-b3f4-81e0ee55156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import ast\n",
    "import os.path as op\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium.featurize\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\"..\")\n",
    "# from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c69899-6a32-4418-9079-8bd66aed497f",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading data\n",
    "\n",
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc483a7-410f-4b65-9955-9df42c783bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths TODO\n",
    "dir_path = os.path.dirname(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a25389-d030-415b-982d-b68df981b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6  # Start and end of the segments\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ef4f5-f711-4f8e-8ba5-163dc50bd2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_order_list = [\n",
    "    \"Fp1\",\n",
    "    \"AF7\",\n",
    "    \"AF3\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    \"F5\",\n",
    "    \"F7\",\n",
    "    \"FT7\",\n",
    "    \"FC5\",\n",
    "    \"FC3\",\n",
    "    \"FC1\",\n",
    "    \"C1\",\n",
    "    \"C3\",\n",
    "    \"C5\",\n",
    "    \"T7\",\n",
    "    \"TP7\",\n",
    "    \"CP5\",\n",
    "    \"CP3\",\n",
    "    \"CP1\",\n",
    "    \"P1\",\n",
    "    \"P3\",\n",
    "    \"P5\",\n",
    "    \"P7\",\n",
    "    \"P9\",\n",
    "    \"PO7\",\n",
    "    \"PO3\",\n",
    "    \"O1\",\n",
    "    \"Iz\",\n",
    "    \"Oz\",\n",
    "    \"POz\",\n",
    "    \"Pz\",\n",
    "    \"CPz\",\n",
    "    \"Fpz\",\n",
    "    \"Fp2\",\n",
    "    \"AF8\",\n",
    "    \"AF4\",\n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"F2\",\n",
    "    \"F4\",\n",
    "    \"F6\",\n",
    "    \"F8\",\n",
    "    \"FT8\",\n",
    "    \"FC6\",\n",
    "    \"FC4\",\n",
    "    \"FC2\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"C2\",\n",
    "    \"C4\",\n",
    "    \"C6\",\n",
    "    \"T8\",\n",
    "    \"TP8\",\n",
    "    \"CP6\",\n",
    "    \"CP4\",\n",
    "    \"CP2\",\n",
    "    \"P2\",\n",
    "    \"P4\",\n",
    "    \"P6\",\n",
    "    \"P8\",\n",
    "    \"P10\",\n",
    "    \"PO8\",\n",
    "    \"PO4\",\n",
    "    \"O2\",\n",
    "]\n",
    "\n",
    "channels_dict = dict(zip(channels_order_list, np.arange(1, 64, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c412f36-66a2-4fd8-bfa7-caa98c4a315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=\"all\",\n",
    "    personal=True,\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "\n",
    "    On default, loads a train set: chooses only 80% of participants\n",
    "    and for each of them chooses 80% of epochs.\n",
    "    It will choose them deterministically.\n",
    "\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    If test_participants is set to True, it will load remaining 20% of participants.\n",
    "    If test_epochs is set to True, it will load remaining 20% of epochs.\n",
    "    Test epochs are chronologically after train epochs,\n",
    "    because it reflects real usage (first callibration and then classification).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "    personal: bool\n",
    "        whether a model will be both trained and tested on epochs from one person\n",
    "        if false, person's epochs aren't split into test and train\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    print(os.path.abspath(\"\"))\n",
    "    dir_path = os.path.dirname(os.path.abspath(\"\"))\n",
    "    print(dir_path)\n",
    "    header_files_glob = os.path.join(dir_path, \"data/responses/*.vhdr\")\n",
    "    header_files = glob.glob(header_files_glob)\n",
    "\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    # cut 20% of data for testing\n",
    "    h_train, h_test = train_test_split(header_files, test_size=0.2, random_state=0)\n",
    "\n",
    "    if test_participants:\n",
    "        header_files = h_test\n",
    "    else:\n",
    "        header_files = h_train\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 5 or len(correct) < 5:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        if personal:\n",
    "            # cut 20% of each participant's epochs for testing\n",
    "            # shuffling is disabled to make sure test epochs are after train epochs\n",
    "            # TODO: not sure if this step is necessary\n",
    "            err_train, err_test = train_test_split(error, test_size=0.2, shuffle=False)\n",
    "            cor_train, cor_test = train_test_split(\n",
    "                correct, test_size=0.2, shuffle=False\n",
    "            )\n",
    "            if test_epochs:\n",
    "                error = err_test\n",
    "                correct = cor_test\n",
    "            else:\n",
    "                error = err_train\n",
    "                correct = cor_train\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, correct, error, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991419b-7acd-42bc-b048-e5e9fa3cb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, correct, error, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        if info == \"all\":\n",
    "            rumination_df = pd.read_csv(info_filename)\n",
    "        else:\n",
    "            rumination_df = pd.read_csv(info_filename, usecols=[\"File\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"File\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "\n",
    "    for epoch in correct:\n",
    "        epoch_df = pd.DataFrame(\n",
    "            {\"id\": [id], \"epoch\": [epoch], \"marker\": [CORRECT]}\n",
    "        ).join(info_df)\n",
    "        participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    for epoch in error:\n",
    "        epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [epoch], \"marker\": [ERROR]}).join(\n",
    "            info_df\n",
    "        )\n",
    "        participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88517e-884a-4b5b-9652-1bf2e7787aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' means that bad segments are rejected automatically.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = True\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "    )\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5e07d-9618-47de-a706-687a529fb2fe",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8c673-5477-4126-b09d-870a8fcd9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"go_nogo_df\"\n",
    "pickled_data_filename = \"../data/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/Demographic_Questionnaires_Behavioral_Results_N=163.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16019e-062f-453d-a30e-363fff670393",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Rearrange data:  from: *one row - one epoch* to *one row - one participant* \n",
    "\n",
    "epochs column contain list of epochs from given condition (marker = error or correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df7179-4753-45e0-96f9-6a9e62336f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = (\n",
    "    epochs_df.groupby(\n",
    "        [\"id\", \"marker\"],\n",
    "        sort=False,\n",
    "    )\n",
    "    .apply(\n",
    "        lambda group_df: pd.Series(\n",
    "            {\n",
    "                \"epochs\": np.array(group_df[\"epoch\"].to_list(), dtype=\"float64\"),\n",
    "                # \"ern\": np.array(group_df[\"ern\"].to_list(), dtype=\"float64\"),\n",
    "                # \"pe\": np.array(group_df[\"pe\"].to_list(), dtype=\"float64\"),\n",
    "                \"Rumination\": np.mean(group_df[\"Rumination Full Scale\"]),\n",
    "                \"Anxiety\": np.mean(group_df[\"DASS-21 Anxiety scale\"]),\n",
    "                \"Stress\": np.mean(group_df[\"DASS-21 Stress scale\"]),\n",
    "                \"Depression\": np.mean(group_df[\"DASS-21 Depression scale\"]),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X_df = data_df[data_df['marker'] == ERROR]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7308d796-9764-4562-9377-4f30473a6d68",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a1985f-accc-498e-a33f-17bc26d6304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = (\n",
    "    epochs_df.groupby(\n",
    "        [\"id\", \"marker\"],\n",
    "        sort=False,\n",
    "    )\n",
    "    .size()\n",
    "    .reset_index(name=\"counts\")\n",
    ")\n",
    "\n",
    "participants_data_len = np.array(\n",
    "    summary_df[summary_df[\"marker\"] == 0][\"counts\"].tolist()\n",
    ")\n",
    "\n",
    "# participant data indices for identifying participants data after spatial filtering\n",
    "\n",
    "participants_data_indices = []\n",
    "index = 0\n",
    "\n",
    "for participant_len in participants_data_len:\n",
    "    participant_indices = (index, index + participant_len - 1)\n",
    "    participants_data_indices.append(participant_indices)\n",
    "    index = index + participant_len\n",
    "\n",
    "participants_data_indices = np.array(participants_data_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea2478-01a2-499e-b6bb-b20e580c0676",
   "metadata": {},
   "source": [
    "---\n",
    "## Training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dff3a8-14a7-445c-8940-aba32c28ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a593d8-7928-410c-88b9-0b4b25f1c493",
   "metadata": {},
   "source": [
    "#### Create X train and y train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afda6e-cc8b-492f-bb80-ac3a0b0cde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of the analysed condition: erroneous responses or correct responses\n",
    "\n",
    "dataset = ERROR\n",
    "dataset_name = \"correct\" if dataset == CORRECT else \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f1ac9-2bd1-46c6-b404-a394ac24603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape 4-D: participant x epoch x channel x timepoints\n",
    "# X_train = np.array(data_df[data_df[\"marker\"] == dataset][\"epochs\"].tolist())\n",
    "\n",
    "# dataframe where 1 row = one participant and 'epochs' column consists of 4-array: participant x epoch x channel x timepoints\n",
    "X_train = X_df\n",
    "\n",
    "# shape 1-D: rumination score\n",
    "rumination = np.array(\n",
    "    data_df[data_df[\"marker\"] == dataset][\"Rumination\"].to_list()\n",
    ")\n",
    "\n",
    "anxiety = np.array(data_df[data_df[\"marker\"] == dataset][\"Anxiety\"].to_list())\n",
    "stress = np.array(data_df[data_df[\"marker\"] == dataset][\"Stress\"].to_list())\n",
    "depression = np.array(data_df[data_df[\"marker\"] == dataset][\"Depression\"].to_list())\n",
    "\n",
    "y_train = rumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b37add-c98f-4625-8049-6a2bd087b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366c3c6-dd06-4837-9bd8-a286d9c530f6",
   "metadata": {},
   "source": [
    "---\n",
    "### Experiments \n",
    "\n",
    "Parameters of experiments:\n",
    "- regressors\n",
    "- hyperparameters\n",
    "- preprocessing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962b4be-4bc7-4e61-9bb7-4151a2f59008",
   "metadata": {},
   "source": [
    "#### Prepare experiment estimating \n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fe861-b8ef-4c53-a68c-5241ba5348ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating model with grid search\n",
    "\n",
    "\n",
    "def rate_regressor(\n",
    "    X_train, y_train, X_test, y_test, regressor, regressor_params, base_steps, cv=3\n",
    "):\n",
    "    # define cross-validation method\n",
    "    cv_kf = KFold(n_splits=3)\n",
    "\n",
    "    pipeline = Pipeline([base_steps, regressor])\n",
    "    param_grid = regressor_params\n",
    "    # print(f\"Param grid {param_grid}\")\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv_kf,\n",
    "        scoring={\"r2\", \"neg_mean_absolute_error\", \"neg_mean_squared_error\"},\n",
    "        refit=\"r2\",\n",
    "        return_train_score=True,\n",
    "        n_jobs=10,\n",
    "        verbose=10,\n",
    "        error_score=\"raise\",\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ce1e5-4ec9-4ab1-b3d7-28a80d119ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conducting experiment and saving selected info do result df\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    tested_regressors,\n",
    "    regressor_params,\n",
    "    pipeline_name,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    dataset_name,\n",
    "    base_steps,\n",
    "    results_df,\n",
    "):\n",
    "\n",
    "    for (regressor, params) in tested_regressors:\n",
    "        print(f\"Rating {regressor} \\n\")\n",
    "        tested_params = {**regressor_params, **params}\n",
    "\n",
    "        # enter to grid search\n",
    "        grid_result = rate_regressor(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            regressor,\n",
    "            tested_params,\n",
    "            base_steps,\n",
    "            cv=3,\n",
    "        )\n",
    "\n",
    "        #     predictions = grid_result.predict(X_test)\n",
    "        #     r2 = grid_result.score(X_test, y_test)\n",
    "        #     mae = mean_absolute_error(y_test, predictions)\n",
    "        #     r2_adj = r2_adjusted_scorer(y_test, predictions, len(X_test[0]), len(X_test))\n",
    "\n",
    "        best_estimator_index = grid_result.best_index_\n",
    "        mean_cv_r2 = grid_result.cv_results_[\"mean_test_r2\"][best_estimator_index]\n",
    "        std_cv_r2 = grid_result.cv_results_[\"std_test_r2\"][best_estimator_index]\n",
    "        mean_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        mean_cv_neg_mean_squared_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_squared_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_squared_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_squared_error\"\n",
    "        ][best_estimator_index]\n",
    "        \n",
    "        mean_train_r2 = grid_result.cv_results_[\"mean_train_r2\"][best_estimator_index]\n",
    "        mean_train_mae = grid_result.cv_results_[\"mean_train_neg_mean_absolute_error\"][best_estimator_index]\n",
    "        mean_train_mse = grid_result.cv_results_[\"mean_train_neg_mean_squared_error\"][best_estimator_index]\n",
    "\n",
    "\n",
    "        print(f\"     Best parameters: {grid_result.best_params_}\")\n",
    "        print(f\"     mean r2: {mean_cv_r2}           ± {round(std_cv_r2,3)}\")\n",
    "        print(f\"     mean r2 train: {mean_train_r2}\")\n",
    "\n",
    "        cv_results = grid_result.cv_results_\n",
    "\n",
    "        # calculate p-value\n",
    "        scores_, pvalue_ = calculate_p_permutations(\n",
    "            grid_result.best_estimator_, X_train, y_train\n",
    "        )\n",
    "\n",
    "        # insert selected info to df\n",
    "        data = {\n",
    "            \"data_set\": dataset_name,\n",
    "            \"pipeline_name\": pipeline_name,\n",
    "            \"model\": regressor[0],\n",
    "            \"parameters\": grid_result.best_params_,\n",
    "            \"mean_cv_r2\": mean_cv_r2,\n",
    "            \"std_cv_r2\": std_cv_r2,\n",
    "            \"mean_cv_mae\": mean_cv_neg_mean_absolute_error,\n",
    "            \"std_cv_mae\": std_cv_neg_mean_absolute_error,\n",
    "            \"mean_cv_mse\":mean_cv_neg_mean_squared_error,\n",
    "            \"std_cv_mse\": std_cv_neg_mean_squared_error,\n",
    "            \"cv_results\": cv_results,\n",
    "            \"mean_train_r2\": mean_train_r2,\n",
    "            \"mean_train_mae\":mean_train_mae,\n",
    "            \"mean_train_mse\":mean_train_mse,\n",
    "            \"p-value\": pvalue_,\n",
    "            \"best_estimator\": grid_result.best_estimator_,\n",
    "        }\n",
    "\n",
    "        results_df = results_df.append(data, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfa7df-bda2-4249-97c5-999e73d1abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating p-value with permutation test\n",
    "\n",
    "\n",
    "def calculate_p_permutations(estimator, X, y, cv=3, n_permutations=100, n_jobs=10):\n",
    "\n",
    "    score_, perm_scores_, pvalue_ = permutation_test_score(\n",
    "        estimator, X, y, cv=cv, n_permutations=n_permutations, n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    # summarize\n",
    "    print(f\"     The permutation P-value is = {pvalue_:.3f}\")\n",
    "    print(f\"     The permutation score is = {score_:.3f}\\n\")\n",
    "\n",
    "    return score_, pvalue_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386ed08-cb1a-4cf4-ac0a-9ffa8e324f05",
   "metadata": {},
   "source": [
    "#### Define pipelines\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7ad2a-3ed6-41dd-acdd-e03b8a0aab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rumination_experiment_transformers_averaged import *\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae44fab-a74f-4e47-acce-a324c9b79899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPATIAL FILTER - BINS\n",
    "\n",
    "\n",
    "def spatial_filter_bins_steps(spatial_filter_n_components, timepoints_count):\n",
    "\n",
    "    steps = [\n",
    "        #  (\"extract_epochs\", EEGdata(dataset=dataset)),\n",
    "        # (\n",
    "        #     \"channels_filtering\",\n",
    "        #     ChannelExtraction(significant_channels),\n",
    "        # ),\n",
    "        # (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        # (\n",
    "        #     \"spatial_filter\",\n",
    "        #     PCA(n_components=spatial_filter_n_components, random_state=random_state),\n",
    "        # ),\n",
    "        # (\n",
    "        #     \"spatial_filter_postprocessing\",\n",
    "        #     SpatialFilterPostprocessing(\n",
    "        #         timepoints_count=timepoints_count,\n",
    "        #         participants_data_indices=participants_data_indices,\n",
    "        #     ),\n",
    "        # ),\n",
    "        # (\"lowpass_filter\", LowpassFilter()),\n",
    "        # (\n",
    "        #     \"average_epochs\",\n",
    "        #     AveragePerParticipant(),\n",
    "        # ),\n",
    "        # (\"binning\", BinTransformer(step=step_tp)),\n",
    "        # (\"data_channel_swap\", ChannelDataSwap()),\n",
    "        # (\"postprocessing\", PostprocessingTransformer()),\n",
    "        # (\"scaler\", StandardScaler()),\n",
    "        # (\"feature_selection\", PCA(n_components=2, random_state=random_state)),\n",
    "        (\"extract_epochs\", EEGdata(dataset=dataset)),\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels)\n",
    "        ),\n",
    "        (\n",
    "            \"average_epochs\",\n",
    "            AveragePerParticipant(),\n",
    "        ),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\n",
    "            \"spatial_filter\",\n",
    "            PCA(n_components=spatial_filter_n_components, random_state=random_state),\n",
    "        ),\n",
    "        (\n",
    "            \"spatial_filter_postprocessing\",\n",
    "            SpatialFilterPostprocessing(\n",
    "                timepoints_count=timepoints_count,\n",
    "            ),\n",
    "        ),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\"binning\", BinTransformer(step=step_tp)),\n",
    "        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "        (\"postprocessing\", PostprocessingTransformer()),\n",
    "    ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79fddff-8c71-491b-89fd-b5fb702f5915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPATIAL FILTER - BINS\n",
    "\n",
    "\n",
    "def spatial_filter_bins_metrics_features(spatial_filter_n_components, timepoints_count, feature_name):\n",
    "\n",
    "    ern_features = Pipeline(steps=[\n",
    "                    (\"ern_data_extraction\", ErnTransformer()),\n",
    "                    (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                    (\"postprocessing\", PostprocessingTransformer()),\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"feature_selection\", FastICA(random_state=random_state))])\n",
    "\n",
    "    pe_features = Pipeline(steps = [\n",
    "                    (\"pe_data_extraction\", PeTransformer()),\n",
    "                    (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                    (\"postprocessing\", PostprocessingTransformer()),\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"feature_selection\", FastICA(random_state=random_state))])\n",
    "    \n",
    "    eeg_features = FeatureUnion([(\"ern_features\", ern_features), (\"pe_features\", pe_features)])\n",
    "\n",
    "    eeg_pipeline = Pipeline([\n",
    "        (\"extract_epochs\", EEGdata(dataset=dataset)),\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels)\n",
    "        ),\n",
    "        (\n",
    "            \"average_epochs\",\n",
    "            AveragePerParticipant(),\n",
    "        ),\n",
    "        (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "        (\n",
    "            \"spatial_filter\",\n",
    "            PCA(n_components=spatial_filter_n_components, random_state=random_state),\n",
    "        ),\n",
    "        (\n",
    "            \"spatial_filter_postprocessing\",\n",
    "            SpatialFilterPostprocessing(\n",
    "                timepoints_count=timepoints_count,\n",
    "            ),\n",
    "        ),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\"binning\", BinTransformer(step=step_tp)),\n",
    "        ('ern_pe_features', eeg_features)\n",
    "    \n",
    "    ])\n",
    "        \n",
    "    \n",
    "    metric = Pipeline(steps = [\n",
    "            (\"anxiety\", GetFeature(feature_name=feature_name, dataset=dataset)),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "    \n",
    "    features = FeatureUnion([(\"eeg_features\", eeg_pipeline),(\"metric_features\", metric)])\n",
    "    steps = ('features', features)\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14be436-829d-4819-bb18-b6cce5c7b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINS\n",
    "def erp_bins_steps():\n",
    "    steps = [\n",
    "        (\n",
    "            \"channels_filtering\",\n",
    "            ChannelExtraction(significant_channels),\n",
    "        ),\n",
    "        (\"lowpass_filter\", LowpassFilter()),\n",
    "        (\n",
    "            \"average_epochs\",\n",
    "            AveragePerParticipant(),\n",
    "        ),\n",
    "        (\"binning\", BinTransformer(step=step_tp)),\n",
    "        (\"data_channel_swap\", ChannelDataSwap()),\n",
    "        (\"postprocessing\", PostprocessingTransformer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        # (\"feature_selection\", PCA(random_state=random_state)),\n",
    "    ]\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05322ac1-c31d-412b-828a-e39e78305751",
   "metadata": {},
   "source": [
    "Generate estimator HTML representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b556630-2263-4a3b-b4e9-be6d4f29a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import estimator_html_repr\n",
    "\n",
    "# with open(\"my_estimator.html\", \"w\") as f:\n",
    "#     f.write(estimator_html_repr(Pipeline(this_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fea081-c0ff-47c9-ba47-6323d0fa33bd",
   "metadata": {},
   "source": [
    "### Perform Experiments\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385045e-557e-44ab-a650-439c38ff8ab6",
   "metadata": {},
   "source": [
    "#### Global parameters common for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8dc3b-60fb-4bb0-848a-876ea8c1bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels that will be included in the experiment\n",
    "\n",
    "red_box = [\n",
    "    \"F1\",\n",
    "    \"Fz\",\n",
    "    \"F2\",\n",
    "    \"FC1\",\n",
    "    \"FCz\",\n",
    "    \"FC2\",\n",
    "    \"C1\",\n",
    "    \"Cz\",\n",
    "    \"C2\",\n",
    "    \"CP1\",\n",
    "    \"CPz\",\n",
    "    \"CP2\",\n",
    "    \"P1\",\n",
    "    \"Pz\",\n",
    "    \"P2\",\n",
    "]\n",
    "significant_channels = [channels_dict[channel] for channel in red_box]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0044c-fdee-4048-8982-19caa4ea0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial filters\n",
    "\n",
    "spatial_filters_dict = {\n",
    "    \"ICA\": FastICA(random_state=random_state),\n",
    "    \"PCA\": PCA(random_state=random_state),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f0948-12b8-42e9-9d9e-c9c7fb579939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins width\n",
    "\n",
    "step_in_ms = 50  # in miliseconds (?)\n",
    "step_tp = int(signal_frequency * step_in_ms / 1000)  # in timepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff9c2c2-558f-4616-9b88-63631942e80d",
   "metadata": {},
   "source": [
    "---\n",
    "#### Experiment 1\n",
    "\n",
    "- spatial filter\n",
    "- bins\n",
    "- feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51cf19-bf5e-4c67-aaf6-13be50b4457d",
   "metadata": {},
   "source": [
    "##### Spatial filter & binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5d755-b1d9-4a03-9e88-95fe9a5a1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters of pipeline\n",
    "\n",
    "spatial_filter = \"PCA\"\n",
    "\n",
    "min_spatial_filter = 2\n",
    "max_spatial_filter = 5\n",
    "step_spatial_filter = 1\n",
    "\n",
    "min_feature_selection = 2\n",
    "max_feature_selection = 9\n",
    "step_feature_selection = 1\n",
    "\n",
    "\n",
    "# define proper parameters for training. In this case define range of number of feature extraction to search\n",
    "regressor_params = dict(\n",
    "        features__eeg_features__ern_pe_features__ern_features__feature_selection__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    "    features__eeg_features__ern_pe_features__pe_features__feature_selection__n_components=np.arange(\n",
    "        min_feature_selection, max_feature_selection, step_feature_selection\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19073a0-ee3d-4ace-bbfd-177c753cefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define estimators and their hyperparameters\n",
    "\n",
    "en = (\"en\", ElasticNet(random_state=random_state))\n",
    "en_params = dict(\n",
    "    en__alpha=np.logspace(-7, 3, num=20, base=10),\n",
    "    en__l1_ratio=np.logspace(-8, 0, num=17, base=10),\n",
    ")\n",
    "\n",
    "kr = (\"kr\", KernelRidge(kernel=\"rbf\"))\n",
    "kr_params = dict(\n",
    "    kr__alpha=np.logspace(-5, 3, num=20, base=10),\n",
    "    kr__gamma=np.logspace(-5, 3, num=20, base=10),\n",
    ")\n",
    "\n",
    "\n",
    "svr = (\"svr\", SVR())\n",
    "svr_params = dict(\n",
    "    svr__kernel=[\"linear\", \"rbf\"],\n",
    "    svr__C=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    svr__gamma=[\"scale\"],\n",
    "    svr__epsilon=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    ")\n",
    "\n",
    "tested_regressors = [\n",
    "    # (svr, svr_params), \n",
    "    # (kr, kr_params), \n",
    "    (en, en_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de81c0ce-bc07-42e2-9dd4-a16eef799b2e",
   "metadata": {},
   "source": [
    "#### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec869ac-4509-4ce0-91d6-726a334f62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d726e3-0844-4360-86a8-5b09b8fba6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_static_ICA_anxiety_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ee8c7-d8dd-4223-a935-9e86a0f17ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually test different numbers of spatial filter components\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "\n",
    "for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter):\n",
    "\n",
    "    pipeline_name = f\"{spatial_filter}_{n_components}_union_anxiety_bins\"\n",
    "    \n",
    "    this_steps = spatial_filter_bins_metrics_features(spatial_filter_n_components=n_components, timepoints_count=181, feature_name=\"Anxiety\")\n",
    "    \n",
    "    # this_steps = spatial_filter_bins_steps(spatial_filter_n_components=n_components, timepoints_count=181)\n",
    "    # pre_processing_pipeline = Pipeline(steps=this_steps)\n",
    "    # pre_processed_X = pre_processing_pipeline.fit_transform(X_train)\n",
    "    \n",
    "   \n",
    "    # rate different models\n",
    "    results_static_ICA_anxiety_df = run_experiment(\n",
    "        tested_regressors,\n",
    "        regressor_params,\n",
    "        pipeline_name,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        dataset_name,\n",
    "        this_steps,\n",
    "        results_static_ICA_anxiety_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e62242-bf6c-41b6-8c3a-f701db5b954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_static_ICA_anxiety_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a3bb78-fcf3-4ed4-b18b-44faeef165fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_pickle(\"../data/regression_ICA_union_error.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e1c8f-2fb5-4612-9017-f9a629ace9f0",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cbb6b-8259-44a6-8ff6-949fe9d7e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = results_df \n",
    "data_df.name = \"union\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdc3c7-5692-47ba-8626-fe94ec398b0c",
   "metadata": {},
   "source": [
    "#### Extract coefficients of ERN and PE features extraction (ICA) and coefficient od estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42575b1-e957-4341-94bd-b2efb0604898",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_features = data_df.best_estimator[1][\"features\"].transformer_list[0][1][\"feature_selection\"].components_\n",
    "pe_features = data_df.best_estimator[1][\"features\"].transformer_list[1][1][\"feature_selection\"].components_\n",
    "coeffs = data_df.best_estimator[1][\"en\"].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26fe94-2a8e-4acb-8cda-9b6e0fc5e8c6",
   "metadata": {},
   "source": [
    "#### Weigh components with coeffs from estimator and sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591889e2-aa90-447e-afde-2385b2bee260",
   "metadata": {},
   "outputs": [],
   "source": [
    "ern_components_weighed = np.array([ern_features[i] * coeffs[i] for i in range(0,ern_features.shape[0])])\n",
    "pe_components_weighed = np.array([pe_features[i-ern_features.shape[0]] * coeffs[i] for i in range(ern_features.shape[0], ern_features.shape[0] + pe_features.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aefe62-2e54-405f-9b33-74523fd6fbe7",
   "metadata": {},
   "source": [
    "#### Sum all feature extraction components to extract direct weigh of given bin at given spatial filter component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279af91-4d5b-49ec-b4bf-44cf202bdca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_weighed_ern_sum = sum(ern_components_weighed)\n",
    "components_weighed_pe_sum = sum(pe_components_weighed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe975747-5514-4b6d-a7d0-5b7c9249c822",
   "metadata": {},
   "source": [
    "#### Extract components of spatial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb0ee3-7b85-4ee4-9afb-65aefd48565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_filter_n_components = 3\n",
    "\n",
    "this_steps = spatial_filter_bins_steps(spatial_filter_n_components=spatial_filter_n_components, timepoints_count=181)\n",
    "pre_processed_X = Pipeline(steps=this_steps).fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6757d7-cbc7-42a1-abec-a3cbde22dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaged signal within components through all participants\n",
    "mean_X = np.mean(pre_processed_X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7bd29-2396-4c70-9f76-4d170512f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(0,3):\n",
    "    plt.plot(-mean_X[i*14:(i+1)*15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6573c8-792b-4eba-a2ea-474177d202e7",
   "metadata": {},
   "source": [
    "-----\n",
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88488c65-4b2a-4c04-a963-506505712f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0, 1 or 2\n",
    "this_component = 0\n",
    "\n",
    "pe_step = int(pe_features.shape[1]/ spatial_filter_n_components)\n",
    "ern_step = int(ern_features.shape[1]/ spatial_filter_n_components)\n",
    "spatial_filter_step = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938b9f5-af49-4100-93f6-e15de0582164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# ax1 = plt.twinx()\n",
    "ax1.set(ylim=(min(components_weighed_ern_sum)-0.02, max(components_weighed_ern_sum)+0.02))\n",
    "ax1.tick_params(axis='y', color=\"magenta\", width=3, length=10)\n",
    "\n",
    "plt.axhline(y=0, color=\"grey\", linewidth = 2, linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "for i in range(0,ern_features.shape[0]):\n",
    "    sns.lineplot(np.arange(2,5), ern_components_weighed[i][this_component*ern_step:(this_component+1)*ern_step], ax=ax1)\n",
    "\n",
    "for i in range(0,pe_features.shape[0]):\n",
    "    sns.lineplot(np.arange(5,9), pe_components_weighed[i][this_component*pe_step:(this_component+1)*pe_step], ax=ax1)\n",
    "    \n",
    "\n",
    "ax2 = plt.twinx()\n",
    "ax2.set(ylim=(-3e-5,3e-5))\n",
    "ax2.tick_params(axis='y', color=\"black\")\n",
    "\n",
    "# ax3 = plt.twinx()\n",
    "# ax3.set(ylim=(min(components_weighed_ern_sum), max(components_weighed_ern_sum)))\n",
    "# ax3.tick_params(axis='y', color=\"magenta\")\n",
    "\n",
    "\n",
    "sns.lineplot(np.arange(2,5), components_weighed_ern_sum[this_component*ern_step:(this_component+1)*ern_step], ax=ax1, color=\"magenta\", linewidth = 3)\n",
    "sns.lineplot(np.arange(5,9), components_weighed_pe_sum[this_component*pe_step:(this_component+1)*pe_step], ax=ax1, color=\"magenta\", linewidth = 3)\n",
    "# plt.axhline(y=0, color=\"magenta\", linewidth = 2)\n",
    "\n",
    "sns_plot = sns.lineplot(np.arange(0,spatial_filter_step), -mean_X[this_component*spatial_filter_step:(this_component+1)*spatial_filter_step], ax=ax2, color=\"black\", linewidth = 3)\n",
    "\n",
    "sns_plot.figure.savefig(f\"{data_df.name}_output_{this_component}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547dfae-7072-4983-9462-d97eacf968bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
